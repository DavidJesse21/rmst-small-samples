@book{2002,
  title = {The {{Statistical Analysis}} of {{Failure Time Data}}},
  date = {2002},
  edition = {1},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118032985},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/9781118032985},
  urldate = {2024-02-24},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\2002.pdf;C\:\\Users\\david\\Zotero\\storage\\JPIA5QWC\\9781118032985.html}
}

@article{aalen2015,
  title = {Does {{Cox}} Analysis of a Randomized Survival Study Yield a Causal Treatment Effect?},
  author = {Aalen, Odd O. and Cook, Richard J. and Røysland, Kjetil},
  date = {2015-10-01},
  journaltitle = {Lifetime Data Analysis},
  shortjournal = {Lifetime Data Anal},
  volume = {21},
  number = {4},
  pages = {579--593},
  issn = {1572-9249},
  doi = {10.1007/s10985-015-9335-y},
  url = {https://doi.org/10.1007/s10985-015-9335-y},
  urldate = {2023-08-30},
  abstract = {Statistical methods for survival analysis play a central role in the assessment of treatment effects in randomized clinical trials in cardiovascular disease, cancer, and many other fields. The most common approach to analysis involves fitting a Cox regression model including a treatment indicator, and basing inference on the large sample properties of the regression coefficient estimator. Despite the fact that treatment assignment is randomized, the hazard ratio is not a quantity which admits a causal interpretation in the case of unmodelled heterogeneity. This problem arises because the risk sets beyond the first event time are comprised of the subset of individuals who have not previously failed. The balance in the distribution of potential confounders between treatment arms is lost by this implicit conditioning, whether or not censoring is present. Thus while the Cox model may be used as a basis for valid tests of the null hypotheses of no treatment effect if robust variance estimates are used, modeling frameworks more compatible with causal reasoning may be preferrable in general for estimation.},
  langid = {english},
  keywords = {Causation,Collapsible model,Confounding,Hazard function,notion,Survival data},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\aalen2015.pdf}
}

@article{ambrogi2022,
  title = {Analyzing Differences between Restricted Mean Survival Time Curves Using Pseudo-Values},
  author = {Ambrogi, Federico and Iacobelli, Simona and Andersen, Per Kragh},
  date = {2022-03-18},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  volume = {22},
  number = {1},
  pages = {71},
  issn = {1471-2288},
  doi = {10.1186/s12874-022-01559-z},
  url = {https://doi.org/10.1186/s12874-022-01559-z},
  urldate = {2023-10-06},
  abstract = {Hazard ratios are ubiquitously used in time to event analysis to quantify treatment effects. Although hazard ratios are invaluable for hypothesis testing, other measures of association, both relative and absolute, may be used to fully elucidate study results. Restricted mean survival time (RMST) differences between groups have been advocated as useful measures of association. Recent work focused on model-free estimates of the difference in restricted mean survival through follow-up times, instead of focusing on a single time horizon. The resulting curve can be used to quantify the association in time units with a simultaneous confidence band. In this work a model-based estimate of the curve is proposed using pseudo-values allowing for possible covariate adjustment. The method is easily implementable with available software and makes possible to compute a simultaneous confidence region for the curve. The pseudo-values regression using multiple restriction times is in good agreement with the estimates obtained by standard direct regression models fixing a single restriction time. Moreover, the proposed method is flexible enough to reproduce the results of the non-parametric approach when no covariates are considered. Examples where it is important to adjust for baseline covariates will be used to illustrate the different methods together with some simulations.},
  keywords = {Crossing survival curves,notion,Pseudo-values,RMST curve difference},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\ambrogi2022.pdf;C\:\\Users\\david\\Zotero\\storage\\5GSG9CA7\\s12874-022-01559-z.html}
}

@article{ananthakrishnan2021,
  title = {Critical Review of Oncology Clinical Trial Design under Non-Proportional Hazards},
  author = {Ananthakrishnan, Revathi and Green, Stephanie and Previtali, Alessandro and Liu, Rong and Li, Daniel and LaValley, Michael},
  date = {2021-06},
  journaltitle = {Critical Reviews in Oncology/Hematology},
  shortjournal = {Critical Reviews in Oncology/Hematology},
  volume = {162},
  pages = {103350},
  issn = {10408428},
  doi = {10.1016/j.critrevonc.2021.103350},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1040842821001384},
  urldate = {2023-08-22},
  abstract = {In trials of novel immuno-oncology drugs, the proportional hazards (PH) assumption often does not hold for the primary time-to-event (TTE) efficacy endpoint, likely due to the unique mechanism of action of these drugs. In practice, when it is anticipated that PH may not hold for the TTE endpoint with respect to treatment, the sample size is often still calculated under the PH assumption, and the hazard ratio (HR) from the Cox model is still reported as the primary measure of the treatment effect. Sensitivity analyses of the TTE data using methods that are suitable under non-proportional hazards (non-PH) are commonly pre-planned. In cases where a substantial deviation from the PH assumption is likely, we suggest designing the trial, calculating the sample size and analyzing the data, using a suitable method that accounts for non-PH, after gaining alignment with regulatory authorities.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\ananthakrishnan2021.pdf}
}

@article{andersen2003,
  title = {Generalised Linear Models for Correlated Pseudo-Observations, with Applications to Multi-State Models},
  author = {Andersen, P. K.},
  date = {2003-03-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {90},
  number = {1},
  pages = {15--27},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/90.1.15},
  url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/90.1.15},
  urldate = {2024-02-14},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\andersen2003.pdf}
}

@article{andersen2004,
  title = {Regression {{Analysis}} of {{Restricted Mean Survival Time Based}} on {{Pseudo-Observations}}},
  author = {Andersen, Per Kragh and Hansen, Mette Gerster and Klein, John P.},
  date = {2004-12-01},
  journaltitle = {Lifetime Data Analysis},
  shortjournal = {Lifetime Data Anal},
  volume = {10},
  number = {4},
  pages = {335--350},
  issn = {1572-9249},
  doi = {10.1007/s10985-004-4771-0},
  url = {https://doi.org/10.1007/s10985-004-4771-0},
  urldate = {2023-10-31},
  abstract = {Regression models for survival data are often specified from the hazard function while classical regression analysis of quantitative outcomes focuses on the mean value (possibly after suitable transformations). Methods for regression analysis of mean survival time and the related quantity, the restricted mean survival time, are reviewed and compared to a method based on pseudo-observations. Both Monte Carlo simulations and two real data sets are studied. It is concluded that while existing methods may be superior for analysis of the mean, pseudo-observations seem well suited when the restricted mean is studied.},
  langid = {english},
  keywords = {censoring,hazard function,health economics,mean survival time,notion,pseudo-observations,regression model,restricted mean survival time,survival analysis},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\andersen2004.pdf}
}

@article{andersen2010,
  title = {Pseudo-Observations in Survival Analysis},
  author = {Andersen, Per Kragh and Pohar Perme, Maja},
  date = {2010-02-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {19},
  number = {1},
  pages = {71--99},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280209105020},
  url = {https://doi.org/10.1177/0962280209105020},
  urldate = {2023-08-25},
  abstract = {We review recent work on the application of pseudo-observations in survival and event history analysis. This includes regression models for parameters like the survival function in a single point, the restricted mean survival time and transition or state occupation probabilities in multi-state models, e.g. the competing risks cumulative incidence function. Graphical and numerical methods for assessing goodness-of-fit for hazard regression models and for the Fine—Gray model in competing risks studies based on pseudo-observations are also reviewed. Sensitivity to covariate-dependent censoring is studied. The methods are illustrated using a data set from bone marrow transplantation.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\andersen2010.pdf}
}

@article{andersen2010a,
  title = {Pseudo-Observations in Survival Analysis},
  author = {Andersen, Per Kragh and Pohar Perme, Maja},
  date = {2010-02-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {19},
  number = {1},
  pages = {71--99},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280209105020},
  url = {https://doi.org/10.1177/0962280209105020},
  urldate = {2023-11-01},
  abstract = {We review recent work on the application of pseudo-observations in survival and event history analysis. This includes regression models for parameters like the survival function in a single point, the restricted mean survival time and transition or state occupation probabilities in multi-state models, e.g. the competing risks cumulative incidence function. Graphical and numerical methods for assessing goodness-of-fit for hazard regression models and for the Fine—Gray model in competing risks studies based on pseudo-observations are also reviewed. Sensitivity to covariate-dependent censoring is studied. The methods are illustrated using a data set from bone marrow transplantation.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\andersen2010a.pdf}
}

@article{andersen2017,
  title = {Causal Inference in Survival Analysis Using Pseudo-Observations},
  author = {Andersen, Per K. and Syriopoulou, Elisavet and Parner, Erik T.},
  date = {2017},
  journaltitle = {Statistics in Medicine},
  volume = {36},
  number = {17},
  pages = {2669--2681},
  issn = {1097-0258},
  doi = {10.1002/sim.7297},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7297},
  urldate = {2023-09-06},
  abstract = {Causal inference for non-censored response variables, such as binary or quantitative outcomes, is often based on either (1) direct standardization (‘G-formula’) or (2) inverse probability of treatment assignment weights (‘propensity score’). To do causal inference in survival analysis, one needs to address right-censoring, and often, special techniques are required for that purpose. We will show how censoring can be dealt with ‘once and for all’ by means of so-called pseudo-observations when doing causal inference in survival analysis. The pseudo-observations can be used as a replacement of the outcomes without censoring when applying ‘standard’ causal inference methods, such as (1) or (2) earlier. We study this idea for estimating the average causal effect of a binary treatment on the survival probability, the restricted mean lifetime, and the cumulative incidence in a competing risks situation. The methods will be illustrated in a small simulation study and via a study of patients with acute myeloid leukemia who received either myeloablative or non-myeloablative conditioning before allogeneic hematopoetic cell transplantation. We will estimate the average causal effect of the conditioning regime on outcomes such as the 3-year overall survival probability and the 3-year risk of chronic graft-versus-host disease. Copyright © 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {causal inference,G-formula,notion,propensity score,pseudo-observations,right-censoring,survival data},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\andersen2017.pdf}
}

@article{andersen2017a,
  title = {Causal Inference in Survival Analysis Using Pseudo-Observations},
  author = {Andersen, Per K. and Syriopoulou, Elisavet and Parner, Erik T.},
  date = {2017},
  journaltitle = {Statistics in Medicine},
  volume = {36},
  number = {17},
  pages = {2669--2681},
  issn = {1097-0258},
  doi = {10.1002/sim.7297},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7297},
  urldate = {2023-11-01},
  abstract = {Causal inference for non-censored response variables, such as binary or quantitative outcomes, is often based on either (1) direct standardization (‘G-formula’) or (2) inverse probability of treatment assignment weights (‘propensity score’). To do causal inference in survival analysis, one needs to address right-censoring, and often, special techniques are required for that purpose. We will show how censoring can be dealt with ‘once and for all’ by means of so-called pseudo-observations when doing causal inference in survival analysis. The pseudo-observations can be used as a replacement of the outcomes without censoring when applying ‘standard’ causal inference methods, such as (1) or (2) earlier. We study this idea for estimating the average causal effect of a binary treatment on the survival probability, the restricted mean lifetime, and the cumulative incidence in a competing risks situation. The methods will be illustrated in a small simulation study and via a study of patients with acute myeloid leukemia who received either myeloablative or non-myeloablative conditioning before allogeneic hematopoetic cell transplantation. We will estimate the average causal effect of the conditioning regime on outcomes such as the 3-year overall survival probability and the 3-year risk of chronic graft-versus-host disease. Copyright © 2017 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {causal inference,G-formula,notion,propensity score,pseudo-observations,right-censoring,survival data},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\andersen2017a.pdf;C\:\\Users\\david\\Zotero\\storage\\4TSFDZUI\\sim.html}
}

@article{austin2022,
  title = {Using Fractional Polynomials and Restricted Cubic Splines to Model Non-Proportional Hazards or Time-Varying Covariate Effects in the {{Cox}} Regression Model},
  author = {Austin, Peter C. and Fang, Jiming and Lee, Douglas S.},
  date = {2022},
  journaltitle = {Statistics in Medicine},
  volume = {41},
  number = {3},
  pages = {612--624},
  issn = {1097-0258},
  doi = {10.1002/sim.9259},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9259},
  urldate = {2023-09-06},
  abstract = {The Cox proportional hazards model is used extensively in clinical and epidemiological research. A key assumption of this model is that of proportional hazards. A variable satisfies the proportional hazards assumption if the effect of that variable on the hazard function is constant over time. When the proportional hazards assumption is violated for a given variable, a common approach is to modify the model so that the regression coefficient associated with the given variable is assumed to be a linear function of time (or of log-time), rather than being constant or fixed. However, this is an unnecessarily restrictive assumption. We describe two different methods to allow a regression coefficient, and thus the hazard ratio, in a Cox model to vary as a flexible function of time. These methods use either fractional polynomials or restricted cubic splines to model the log-hazard ratio as a function of time. We illustrate the utility of these methods using data on 12 705 patients who presented to a hospital emergency department with a primary diagnosis of heart failure. We used a Cox model to assess the association between elevated cardiac troponin at presentation and the hazard of death after adjustment for an extensive set of covariates. SAS code for implementing the restricted cubic spline approach is provided, while an existing Stata function allows for the use of fractional polynomials.},
  langid = {english},
  keywords = {Cox proportional hazards model,fractional polynomials,notion,restricted cubic splines,survival analysis,time-dependent effect},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\austin2022.pdf;C\:\\Users\\david\\Zotero\\storage\\P7WRFWIC\\sim.html}
}

@online{bardo2023,
  title = {Methods for Non-Proportional Hazards in Clinical Trials: {{A}} Systematic Review},
  shorttitle = {Methods for Non-Proportional Hazards in Clinical Trials},
  author = {Bardo, Maximilian and Huber, Cynthia and Benda, Norbert and Brugger, Jonas and Fellinger, Tobias and Galaune, Vaidotas and Heinz, Judith and Heinzl, Harald and Hooker, Andrew C. and Klinglmüller, Florian and König, Franz and Mathes, Tim and Mittlböck, Martina and Posch, Martin and Ristl, Robin and Friede, Tim},
  date = {2023-06-29},
  eprint = {2306.16858},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2306.16858},
  urldate = {2023-08-21},
  abstract = {For the analysis of time-to-event data, frequently used methods such as the log-rank test or the Cox proportional hazards model are based on the proportional hazards assumption, which is often debatable. Although a wide range of parametric and non-parametric methods for non-proportional hazards (NPH) has been proposed, there is no consensus on the best approaches. To close this gap, we conducted a systematic literature search to identify statistical methods and software appropriate under NPH. Our literature search identified 907 abstracts, out of which we included 211 articles, mostly methodological ones. Review articles and applications were less frequently identified. The articles discuss effect measures, effect estimation and regression approaches, hypothesis tests, and sample size calculation approaches, which are often tailored to specific NPH situations. Using a unified notation, we provide an overview of methods available. Furthermore, we derive some guidance from the identified articles. We summarized the contents from the literature review in a concise way in the main text and provide more detailed explanations in the supplement (page 29).},
  langid = {english},
  pubstate = {preprint},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\bardo2023.pdf}
}

@article{barthel2006,
  title = {Evaluation of Sample Size and Power for Multi-Arm Survival Trials Allowing for Non-Uniform Accrual, Non-Proportional Hazards, Loss to Follow-up and Cross-Over},
  author = {Barthel, F. M.-S. and Babiker, A. and Royston, P. and Parmar, M. K. B.},
  date = {2006-08-15},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Statist. Med.},
  volume = {25},
  number = {15},
  pages = {2521--2542},
  issn = {0277-6715, 1097-0258},
  doi = {10.1002/sim.2517},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.2517},
  urldate = {2023-08-21},
  abstract = {We present a general framework for sample size calculation in survival studies based on comparing two or more survival distributions using any one of a class of tests including the logrank test. Incorporated within this framework are the possible presence of non-uniform staggered patient entry, non-proportional hazards, loss to follow-up and treatment changes including cross-over between treatment arms. The framework is very general in nature and is based on using piecewise exponential distributions to model the survival distributions. We illustrate the use of the approach and explore its validity using simulation studies. These studies have shown that not adjusting for loss to follow-up, non-proportional hazards or cross-over can lead to signiÿcant alterations in power or equivalently, a marked e ect on sample size. The approach has been implemented in the freely available program ART (for Stata). Our investigations suggest that ART is the ÿrst software to allow incorporation of all these elements. Further extensions to the methodology such as non-local alternatives for the logrank test are also considered. Copyright ? 2006 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\barthel2006.pdf}
}

@article{bartlett,
  title = {The {{Hazards}} of {{Period Specific}} and {{Weighted Hazard Ratios}}},
  author = {Bartlett, Jonathan W. and Morris, Tim P. and Stensrud, Mats J. and Daniel, Rhian M. and Vansteelandt, Stijn K. and Burman, Carl-Fredrik},
  journaltitle = {Statistics in Biopharmaceutical Research},
  shortjournal = {Stat Biopharm Res},
  volume = {12},
  number = {4},
  eprint = {34191984},
  eprinttype = {pmid},
  pages = {518--519},
  issn = {1946-6315},
  doi = {10.1080/19466315.2020.1755722},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8011481/},
  urldate = {2023-08-30},
  pmcid = {PMC8011481},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\bartlett.pdf}
}

@article{bebu2023,
  title = {Generalized Fiducial Inference for the Restricted Mean Survival Time},
  author = {Bebu, Ionut and Diao, Guoqing and Hamasaki, Toshimitsu},
  date = {2023-05-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {32},
  number = {5},
  pages = {1010--1020},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/09622802231163333},
  url = {https://doi.org/10.1177/09622802231163333},
  urldate = {2023-10-04},
  abstract = {The standard modeling approach for time-to-event outcomes subject to censoring is based on the hazard function, with hazard ratios capturing the effect of exposures on the risk of outcome. The restricted mean survival time, defined as the expected time to event up to a pre-specified time horizon, provides an alternative useful summary of time-to-event outcomes. Restricted mean survival time can be estimated nonparametrically and can be used to compare groups or interventions when the proportional hazards (PHs) assumption does not hold. Moreover, even when the proportional hazards assumption holds, the restricted mean survival time, an additive measure of risk, provides additional information to the hazard ratio, which is a measure of relative risk that can be difficult to interpret in absence of an estimate of the reference risk. Herein, a generalized fiducial approach is proposed for restricted mean survival time, and its asymptotic properties are investigated. Numerical simulations show the proposed approach provides one- and two-sided confidence intervals with coverage probabilities close to nominal values and controls the type-I error for two-group comparisons even for small sample sizes with a low number of events. Data from a type 1 diabetes study is used for illustration.},
  langid = {english},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\bebu2023.pdf}
}

@article{bender2018,
  title = {A Generalized Additive Model Approach to Time-to-Event                     Analysis},
  author = {Bender, Andreas and Groll, Andreas and Scheipl, Fabian},
  date = {2018-06-01},
  journaltitle = {Statistical Modelling},
  volume = {18},
  number = {3-4},
  pages = {299--321},
  publisher = {SAGE Publications India},
  issn = {1471-082X},
  doi = {10.1177/1471082X17748083},
  url = {https://doi.org/10.1177/1471082X17748083},
  urldate = {2023-11-02},
  abstract = {: This tutorial article demonstrates how time-to-event data can be modelled in a very flexible way by taking advantage of advanced inference methods that have recently been developed for generalized additive mixed models. In particular, we describe the necessary pre-processing steps for transforming such data into a suitable format and show how a variety of effects, including a smooth nonlinear baseline hazard, and potentially nonlinear and nonlinearly time-varying effects, can be estimated and interpreted. We also present useful graphical tools for model evaluation and interpretation of the estimated effects. Throughout, we demonstrate this approach using various application examples. The article is accompanied by a new R-package called pammtools implementing all of the tools described here.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\bender2018.pdf}
}

@article{binder2014,
  title = {Pseudo-Observations for Competing Risks with Covariate Dependent Censoring},
  author = {Binder, Nadine and Gerds, Thomas A. and Andersen, Per Kragh},
  date = {2014-04-01},
  journaltitle = {Lifetime Data Analysis},
  shortjournal = {Lifetime Data Anal},
  volume = {20},
  number = {2},
  pages = {303--315},
  issn = {1572-9249},
  doi = {10.1007/s10985-013-9247-7},
  url = {https://doi.org/10.1007/s10985-013-9247-7},
  urldate = {2024-02-20},
  abstract = {Regression analysis for competing risks data can be based on generalized estimating equations. For the case with right censored data, pseudo-values were proposed to solve the estimating equations. In this article we investigate robustness of the pseudo-values against violation of the assumption that the probability of not being lost to follow-up (un-censored) is independent of the covariates. Modified pseudo-values are proposed which rely on a correctly specified regression model for the censoring times. Bias and efficiency of these methods are compared in a simulation study. Further illustration of the differences is obtained in an application to bone marrow transplantation data and a corresponding sensitivity analysis.},
  langid = {english},
  keywords = {Competing risks,Covariate-dependent censoring,Cumulative incidence,notion,Pseudo-observations},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\binder2014.pdf}
}

@article{bouaziz2023,
  title = {Fast Approximations of Pseudo-Observations in the Context of Right Censoring and Interval Censoring},
  author = {Bouaziz, Olivier},
  date = {2023},
  journaltitle = {Biometrical Journal},
  volume = {65},
  number = {4},
  pages = {2200071},
  issn = {1521-4036},
  doi = {10.1002/bimj.202200071},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.202200071},
  urldate = {2023-11-01},
  abstract = {In the context of right-censored and interval-censored data, we develop asymptotic formulas to compute pseudo-observations for the survival function and the restricted mean survival time (RMST). These formulas are based on the original estimators and do not involve computation of the jackknife estimators. For right-censored data, Von Mises expansions of the Kaplan–Meier estimator are used to derive the pseudo-observations. For interval-censored data, a general class of parametric models for the survival function is studied. An asymptotic representation of the pseudo-observations is derived involving the Hessian matrix and the score vector. Theoretical results that justify the use of pseudo-observations in regression are also derived. The formula is illustrated on the piecewise-constant-hazard model for the RMST. The proposed approximations are extremely accurate, even for small sample sizes, as illustrated by Monte Carlo simulations and real data. We also study the gain in terms of computation time, as compared to the original jackknife method, which can be substantial for a large dataset.},
  langid = {english},
  keywords = {interval censoring,jackknife,notion,pseudo-observations,restricted mean survival time,Von Mises expansions},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\bouaziz2023.pdf}
}

@article{boulesteix2020,
  title = {Introduction to Statistical Simulations in Health Research},
  author = {Boulesteix, Anne-Laure and Groenwold, Rolf HH and Abrahamowicz, Michal and Binder, Harald and Briel, Matthias and Hornung, Roman and Morris, Tim P. and Rahnenführer, Jörg and Sauerbrei, Willi},
  date = {2020-12-01},
  journaltitle = {BMJ Open},
  volume = {10},
  number = {12},
  eprint = {33318113},
  eprinttype = {pmid},
  pages = {e039921},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2020-039921},
  url = {https://bmjopen.bmj.com/content/10/12/e039921},
  urldate = {2023-09-14},
  abstract = {In health research, statistical methods are frequently used to address a wide variety of research questions. For almost every analytical challenge, different methods are available. But how do we choose between different methods and how do we judge whether the chosen method is appropriate for our specific study? Like in any science, in statistics, experiments can be run to find out which methods should be used under which circumstances. The main objective of this paper is to demonstrate that simulation studies, that is, experiments investigating synthetic data with known properties, are an invaluable tool for addressing these questions. We aim to provide a first introduction to simulation studies for data analysts or, more generally, for researchers involved at different levels in the analyses of health data, who (1) may rely on simulation studies published in statistical literature to choose their statistical methods and who, thus, need to understand the criteria of assessing the validity and relevance of simulation results and their interpretation; and/or (2) need to understand the basic principles of designing statistical simulations in order to efficiently collaborate with more experienced colleagues or start learning to conduct their own simulations. We illustrate the implementation of a simulation study and the interpretation of its results through a simple example inspired by recent literature, which is completely reproducible using the R-script available from online supplemental file 1.},
  langid = {english},
  keywords = {epidemiology,notion,protocols & guidelines,statistics & research methods},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\boulesteix2020.pdf}
}

@article{boulesteix2020a,
  title = {Introduction to Statistical Simulations in Health Research},
  author = {Boulesteix, Anne-Laure and Groenwold, Rolf HH and Abrahamowicz, Michal and Binder, Harald and Briel, Matthias and Hornung, Roman and Morris, Tim P. and Rahnenführer, Jörg and Sauerbrei, Willi},
  date = {2020-12-01},
  journaltitle = {BMJ Open},
  volume = {10},
  number = {12},
  eprint = {33318113},
  eprinttype = {pmid},
  pages = {e039921},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2020-039921},
  url = {https://bmjopen.bmj.com/content/10/12/e039921},
  urldate = {2023-11-13},
  abstract = {In health research, statistical methods are frequently used to address a wide variety of research questions. For almost every analytical challenge, different methods are available. But how do we choose between different methods and how do we judge whether the chosen method is appropriate for our specific study? Like in any science, in statistics, experiments can be run to find out which methods should be used under which circumstances. The main objective of this paper is to demonstrate that simulation studies, that is, experiments investigating synthetic data with known properties, are an invaluable tool for addressing these questions. We aim to provide a first introduction to simulation studies for data analysts or, more generally, for researchers involved at different levels in the analyses of health data, who (1) may rely on simulation studies published in statistical literature to choose their statistical methods and who, thus, need to understand the criteria of assessing the validity and relevance of simulation results and their interpretation; and/or (2) need to understand the basic principles of designing statistical simulations in order to efficiently collaborate with more experienced colleagues or start learning to conduct their own simulations. We illustrate the implementation of a simulation study and the interpretation of its results through a simple example inspired by recent literature, which is completely reproducible using the R-script available from online supplemental file 1.},
  langid = {english},
  keywords = {epidemiology,notion,protocols & guidelines,statistics & research methods},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\boulesteix2020a.pdf}
}

@article{boyd2012,
  title = {Estimation of Treatment Effect under Non-Proportional Hazards and Conditionally Independent Censoring},
  author = {Boyd, Adam P. and Kittelson, John M. and Gillen, Daniel L.},
  date = {2012-12-10},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  volume = {31},
  number = {28},
  eprint = {22763957},
  eprinttype = {pmid},
  pages = {3504--3515},
  issn = {1097-0258},
  doi = {10.1002/sim.5440},
  abstract = {In clinical trials with time-to-event outcomes, it is common to estimate the marginal hazard ratio from the proportional hazards model, even when the proportional hazards assumption is not valid. This is unavoidable from the perspective that the estimator must be specified a priori if probability statements about treatment effect estimates are desired. Marginal hazard ratio estimates under non-proportional hazards are still useful, as they can be considered to be average treatment effect estimates over the support of the data. However, as many have shown, under non-proportional hazard, the 'usual' unweighted marginal hazard ratio estimate is a function of the censoring distribution, which is not normally considered to be scientifically relevant when describing the treatment effect. In addition, in many practical settings, the censoring distribution is only conditionally independent (e.g., differing across treatment arms), which further complicates the interpretation. In this paper, we investigate an estimator of the hazard ratio that removes the influence of censoring and propose a consistent robust variance estimator. We compare the coverage probability of the estimator to both the usual Cox model estimator and an estimator proposed by Xu and O'Quigley (2000) when censoring is independent of the covariate. The new estimator should be used for inference that does not depend on the censoring distribution. It is particularly relevant to adaptive clinical trials where, by design, censoring distributions differ across treatment arms.},
  langid = {english},
  pmcid = {PMC3876422},
  keywords = {Analysis of Variance,Bias,Brain Neoplasms,Computer Simulation,Humans,Neoplasm Metastasis,notion,Proportional Hazards Models,Randomized Controlled Trials as Topic,Sample Size,Time Factors,Treatment Outcome},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\boyd2012.pdf}
}

@article{brunner2021,
  title = {Win Odds: {{An}} Adaptation of the Win Ratio to Include Ties},
  shorttitle = {Win Odds},
  author = {Brunner, Edgar and Vandemeulebroecke, Marc and Mütze, Tobias},
  date = {2021},
  journaltitle = {Statistics in Medicine},
  volume = {40},
  number = {14},
  pages = {3367--3384},
  issn = {1097-0258},
  doi = {10.1002/sim.8967},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8967},
  urldate = {2023-10-01},
  abstract = {The win ratio, a recently proposed measure for comparing the benefit of two treatment groups, allows ties in the data but ignores ties in the inference. In this article, we highlight some difficulties that this can lead to, and we propose to focus on the win odds instead, a modification of the win ratio which takes ties into account. We construct hypothesis tests and confidence intervals for the win odds, and we investigate their properties through simulations and in a case study. We conclude that the win odds should be preferred over the win ratio.},
  langid = {english},
  keywords = {multiple sclerosis,nonparametrics,notion,win odds,win ratio},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\brunner2021.pdf;C\:\\Users\\david\\Zotero\\storage\\MC5FESNG\\sim.html}
}

@article{buyse2020,
  title = {Assessing {{Treatment Benefit}} in {{Immuno-oncology}}},
  author = {Buyse, Marc and Saad, Everardo D. and Burzykowski, Tomasz and Péron, Julien},
  date = {2020-07-01},
  journaltitle = {Statistics in Biosciences},
  shortjournal = {Stat Biosci},
  volume = {12},
  number = {2},
  pages = {83--103},
  issn = {1867-1772},
  doi = {10.1007/s12561-020-09268-1},
  url = {https://doi.org/10.1007/s12561-020-09268-1},
  urldate = {2023-10-01},
  abstract = {Immuno-oncology is a buoyant field of research, with recently developed drugs showing unprecedented response rates and/or a hope for a meaningful prolongation of the overall survival of some patients. These promising clinical developments have also pointed to the need of adapting statistical methods to best describe and test for treatment effects in randomized clinical trials. We review adaptations to tumor response and progression criteria for immune therapies. Survival may be the endpoint of choice for clinical trials in some tumor types, and the search for surrogate endpoints is likely to continue to try and reduce the duration and size of clinical trials. In situations for which hazards are likely to be non-proportional, weighted logrank tests may be preferred as they have substantially more power to detect late separation of survival curves. Alternatively, there is currently much interest in accelerated failure time models, and in capturing treatment effect by the difference in restricted mean survival times between randomized groups. Finally, generalized pairwise comparisons offer much promise in the field of immuno-oncology, both to detect late emerging treatment effects and as a general approach to personalize treatment choices through a benefit/risk approach.},
  langid = {english},
  keywords = {Accelerated failure time model,Generalized pairwise comparisons,Immune RECIST,Immuno-oncology,notion,Restricted mean survival time,Weighted logrank test},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\buyse2020.pdf}
}

@article{castanon2020,
  title = {Critical Reappraisal of Phase {{III}} Trials with Immune Checkpoint Inhibitors in Non-Proportional Hazards Settings},
  author = {Castañon, Eduardo and Sanchez-Arraez, Alvaro and Alvarez-Manceñido, Felipe and Jimenez-Fonseca, Paula and Carmona-Bayonas, Alberto},
  date = {2020-09-01},
  journaltitle = {European Journal of Cancer},
  shortjournal = {European Journal of Cancer},
  volume = {136},
  pages = {159--168},
  issn = {0959-8049},
  doi = {10.1016/j.ejca.2020.06.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0959804920303300},
  urldate = {2023-09-06},
  abstract = {Background The dynamic effects of immune checkpoint inhibitors (ICIs) are a challenge when designing and analysing data in non-proportional hazards (PH) scenarios. Herein, we present the risk of making type II errors, affecting pharmacotherapeutic development when methods that assume constant effects are applied. Patients and methods Individual patient data from six clinical trials (KEYNOTE-062/061, IMvigor211, CA184-143~y CheckMate-057/037) were extracted. The most relevant time-varying effects were examined using the Royston-Parmar spline model (RPSM), time-driven analyses and weighted log-rank~and Renyi tests. Results The RPSM yields an appropriate fit in non-PH contexts, enabling dynamic descriptions of the hazard rate, and time-varying differences of overall survival (OS)/progression-free survival. In the KEYNOTE-061, CheckMate-057~and 037 trials, 12-, 18-, and 24-month OS rates were higher with immunotherapy (differences of some 10\%) (P-value {$<$}0.05). In KEYNOTE-062, CA184-043~and IMvigor-211 trials, OS rate differences were significant for past 20 months. Flemming-Harrington and Renyi tests with late weighting (e.g. with ρ-value~=~0 and γ-value~=~1) captured the existence of significant differences on all curves. The Cox models and log-rank tests were inefficient at detecting the effect. Conclusion This analysis highlights the risk of declaring studies with ICIs negative, despite associating substantial OS benefits. Effort and consensus are needed with respect to methodology to design and evaluate trials with ICIs in non-PH settings.},
  keywords = {Immune checkpoint inhibitors,Immunotherapy,Milestone survival,Non-proportional hazards,notion,Royston-Parmar spline model,Weighted log-rank},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\castanon2020.pdf;C\:\\Users\\david\\Zotero\\storage\\6YXE2P77\\S0959804920303300.html}
}

@article{castanon2020a,
  title = {Critical Reappraisal of Phase {{III}} Trials with Immune Checkpoint Inhibitors in Non-Proportional Hazards Settings},
  author = {Castañon, Eduardo and Sanchez-Arraez, Alvaro and Alvarez-Manceñido, Felipe and Jimenez-Fonseca, Paula and Carmona-Bayonas, Alberto},
  date = {2020-09},
  journaltitle = {European Journal of Cancer},
  shortjournal = {European Journal of Cancer},
  volume = {136},
  pages = {159--168},
  issn = {09598049},
  doi = {10.1016/j.ejca.2020.06.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0959804920303300},
  urldate = {2023-09-06},
  abstract = {Background: The dynamic effects of immune checkpoint inhibitors (ICIs) are a challenge when designing and analysing data in non-proportional hazards (PH) scenarios. Herein, we present the risk of making type II errors, affecting pharmacotherapeutic development when methods that assume constant effects are applied.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\Zotero\storage\V9543IUE\Castañon et al. - 2020 - Critical reappraisal of phase III trials with immu.pdf}
}

@article{charkhi2018,
  title = {Asymptotic Post-Selection Inference for the {{Akaike}} Information Criterion},
  author = {Charkhi, Ali and Claeskens, Gerda},
  date = {2018-09-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {105},
  number = {3},
  pages = {645--664},
  issn = {0006-3444},
  doi = {10.1093/biomet/asy018},
  url = {https://doi.org/10.1093/biomet/asy018},
  urldate = {2023-10-22},
  abstract = {Ignoring the model selection step in inference after selection is harmful. In this paper we study the asymptotic distribution of estimators after model selection using the Akaike information criterion. First, we consider the classical setting in which a true model exists and is included in the candidate set of models. We exploit the overselection property of this criterion in constructing a selection region, and we obtain the asymptotic distribution of estimators and linear combinations thereof conditional on the selected model. The limiting distribution depends on the set of competitive models and on the smallest overparameterized model. Second, we relax the assumption on the existence of a true model and obtain uniform asymptotic results. We use simulation to study the resulting post-selection distributions and to calculate confidence regions for the model parameters, and we also apply the method to a diabetes dataset.},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\charkhi2018.pdf;C\:\\Users\\david\\Zotero\\storage\\6VRQWAQT\\5032574.html}
}

@article{chen2001,
  title = {Causal {{Inference}} on the {{Difference}} of the {{Restricted Mean Lifetime Between Two Groups}}},
  author = {Chen, Pei-Yun and Tsiatis, Anastasios A.},
  date = {2001},
  journaltitle = {Biometrics},
  volume = {57},
  number = {4},
  pages = {1030--1038},
  issn = {1541-0420},
  doi = {10.1111/j.0006-341X.2001.01030.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0006-341X.2001.01030.x},
  urldate = {2023-09-06},
  abstract = {Summary. When comparing survival times between two treatment groups, it may be more appropriate to compare the restricted mean lifetime, i.e., the expectation of lifetime restricted to a time L, rather than mean lifetime in order to accommodate censoring. When the treatments are not assigned to patients randomly, as in observational studies, we also need to account for treatment imbalances in confounding factors. In this article, we propose estimators for the difference of the restricted mean lifetime between two groups that account for treatment imbalances in prognostic factors assuming a proportional hazards relationship. Large-sample properties of our estimators based on martingale theory for counting processes are also derived. Simulation studies were conducted to compare these estimators and to assess the adequacy of the large-sample approximations. Our methods are also applied to an observational database of acute coronary syndrome patients from Duke University Medical Center to estimate the treatment effect on the restricted mean lifetime over 5 years.},
  langid = {english},
  keywords = {Causal inference,Cox's proportional hazard model,Martingale process,notion,Observational study,Restricted lifetime,Stochastic integral,Survival analysis},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\chen2001.pdf;C\:\\Users\\david\\Zotero\\storage\\LVLHTGIN\\j.0006-341X.2001.01030.html}
}

@article{chen2015,
  title = {Quantifying the Average of the Time-Varying Hazard Ratio via a Class of Transformations},
  author = {Chen, Qingxia and Zeng, Donglin and Ibrahim, Joseph G. and Chen, Ming-Hui and Pan, Zhiying and Xue, Xiaodong},
  date = {2015-04},
  journaltitle = {Lifetime Data Analysis},
  shortjournal = {Lifetime Data Anal},
  volume = {21},
  number = {2},
  pages = {259--279},
  issn = {1380-7870, 1572-9249},
  doi = {10.1007/s10985-014-9301-0},
  url = {http://link.springer.com/10.1007/s10985-014-9301-0},
  urldate = {2023-08-21},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\chen2015.pdf}
}

@article{chen2020,
  title = {Comparison of Survival Distributions in Clinical Trials: {{A}} Practical Guidance},
  shorttitle = {Comparison of Survival Distributions in Clinical Trials},
  author = {Chen, Xiaotian and Wang, Xin and Chen, Kun and Zheng, Yeya and Chappell, Richard J and Dey, Jyotirmoy},
  date = {2020-10-01},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {17},
  number = {5},
  pages = {507--521},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774520928614},
  url = {https://doi.org/10.1177/1740774520928614},
  urldate = {2023-08-30},
  abstract = {Background In randomized clinical trials with censored time-to-event outcomes, the logrank test is known to have substantial statistical power under the proportional hazards assumption and is widely adopted as a tool to compare two survival distributions. However, the proportional hazards assumption is impossible to validate in practice until the data are unblinded. However, the statistical analysis plan of a randomized clinical trial and in particular its primary analysis method must be pre-specified before any unblinded information may be reviewed. Purpose The purpose of this article is to guide applied biostatisticians in the prespecification of a desired primary analysis method when a treatment effect with nonproportional hazards is anticipated. While articles proposing alternate statistical tests are aplenty, to the best of our knowledge, there is no article available that attempts to simplify the choice and prespecification of a primary statistical test under specific expected patterns on nonproportional hazards. We provide such guidance by reviewing various tests proposed as more powerful alternatives to the standard logrank test under nonproportional hazards and simultaneously comparing their performance under a wide variety of nonproportional hazards scenarios to elucidate their advantages and disadvantages. Method In order to select the most preferable test for detecting specific differences between survival distributions of interest while controlling false positive rates, we review and assess the performance of weighted and adaptively weighted logrank tests, weighted and adaptively weighted Kaplan–Meier tests and versatile tests under various patterns of nonproportional hazards treatment effects through simulation. Conclusion We validate some of the claimed properties of the proposed extensions and identify tests that may be more preferable under specific expected pattern of nonproportional hazards when such knowledge is available. We show that versatile tests, while achieving robustness to departures from proportional hazards, may lose interpretation of directionality (superiority or inferiority) and can only be seen to test departures from equality. Detailed summary and discussion of the performance of each test in terms of type I error rate and power are provided to formulate specific guidance about their applicability and use.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\chen2020.pdf}
}

@article{chen2022,
  title = {Conversion of Non-Inferiority Margin from Hazard Ratio to Restricted Mean Survival Time Difference Using Data from Multiple Historical Trials},
  author = {Chen, Ruizhe and Basu, Sanjib and Meyers, Jeffrey P and Shi, Qian},
  date = {2022-10-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {31},
  number = {10},
  pages = {1819--1844},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/09622802221102621},
  url = {https://doi.org/10.1177/09622802221102621},
  urldate = {2023-09-08},
  abstract = {The restricted mean survival time measure has gained a lot of interests for designing and analyzing oncology trials with time-to-event endpoints due to its intuitive clinical interpretation and potentially high statistical power. In the non-inferiority trial literature, restricted mean survival time has been used as an alternative measure for reanalyzing a completed trial, which was originally designed and analyzed based on traditional proportional hazard model. However, the reanalysis procedure requires a conversion from the non-inferiority margin measured in hazard ratio to a non-inferiority margin measured by restricted mean survival time difference. An existing conversion method assumes a Weibull distribution for the population survival time of the historical active control group under the proportional hazard assumption using data from a single trial. In this article, we develop a methodology for non-inferiority margin conversion when data from multiple historical active control studies are available, and introduce a Kaplan-Meier estimator-based method for the non-inferiority margin conversion to relax the parametric assumption. We report extensive simulation studies to examine the performances of proposed methods under the Weibull data generative models and a piecewise-exponential data generative model that mimic the tumor recurrence and survival characteristics of advanced colon cancer. This work is motivated to achieve non-inferiority margin conversion, using historical patient-level data from a large colon cancer clinical database, to reanalyze an internationally collaborated non-inferiority study that evaluates 6-month versus 3-month duration of adjuvant chemotherapy in stage III colon cancer patients.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\chen2022.pdf}
}

@article{collett,
  title = {Modelling {{Survival Data}} in {{Medical Research Third Edition}}},
  author = {Collett, David},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\Zotero\storage\TYSMZXRT\Collett - Modelling Survival Data in Medical Research Third .pdf}
}

@book{collett2015,
  title = {Modelling {{Survival Data}} in {{Medical Research}}},
  author = {Collett, David},
  date = {2015-02-02},
  edition = {3},
  publisher = {{Chapman and Hall/CRC}},
  location = {New York},
  doi = {10.1201/b18041},
  abstract = {Modelling Survival Data in Medical Research describes the modelling approach to the analysis of survival data using a wide range of examples from biomedical research.Well known for its nontechnical style, this third edition contains new chapters on frailty models and their applications, competing risks, non-proportional hazards, and dependent censo},
  isbn = {978-0-429-19629-4},
  pagetotal = {548},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\collett2015.pdf}
}

@article{cox1972,
  title = {Regression {{Models}} and {{Life-Tables}}},
  author = {Cox, D. R.},
  date = {1972},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Methodological)},
  volume = {34},
  number = {2},
  eprint = {2985181},
  eprinttype = {jstor},
  pages = {187--220},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0035-9246},
  url = {https://www.jstor.org/stable/2985181},
  urldate = {2024-02-12},
  abstract = {The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\cox1972.pdf}
}

@article{crowther2023,
  title = {A Flexible Parametric Accelerated Failure Time Model and the Extension to Time-Dependent Acceleration Factors},
  author = {Crowther, Michael J and Royston, Patrick and Clements, Mark},
  date = {2023-07-01},
  journaltitle = {Biostatistics},
  shortjournal = {Biostatistics},
  volume = {24},
  number = {3},
  pages = {811--831},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxac009},
  url = {https://doi.org/10.1093/biostatistics/kxac009},
  urldate = {2023-09-14},
  abstract = {Accelerated failure time (AFT) models are used widely in medical research, though to a much lesser extent than proportional hazards models. In an AFT model, the effect of covariates act to accelerate or decelerate the time to event of interest, that is, shorten or extend the time to event. Commonly used parametric AFT models are limited in the underlying shapes that they can capture. In this article, we propose a general parametric AFT model, and in particular concentrate on using restricted cubic splines to model the baseline to provide substantial flexibility. We then extend the model to accommodate time-dependent acceleration factors. Delayed entry is also allowed, and hence, time-dependent covariates. We evaluate the proposed model through simulation, showing substantial improvements compared to standard parametric AFT models. We also show analytically and through simulations that the AFT models are collapsible, suggesting that this model class will be well suited to causal inference. We illustrate the methods with a data set of patients with breast cancer. Finally, we provide highly efficient, user-friendly Stata, and R software packages.},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\crowther2023.pdf}
}

@article{davidson2008,
  title = {The Wild Bootstrap, Tamed at Last},
  author = {Davidson, Russell and Flachaire, Emmanuel},
  date = {2008-09-01},
  journaltitle = {Journal of Econometrics},
  shortjournal = {Journal of Econometrics},
  volume = {146},
  number = {1},
  pages = {162--169},
  issn = {0304-4076},
  doi = {10.1016/j.jeconom.2008.08.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0304407608000833},
  urldate = {2024-02-07},
  abstract = {The wild bootstrap is studied in the context of regression models with heteroskedastic disturbances. We show that, in one very specific case, perfect bootstrap inference is possible, and a substantial reduction in the error in the rejection probability of a bootstrap test is available much more generally. However, the version of the wild bootstrap with this desirable property is without the skewness correction afforded by the currently most popular version of the wild bootstrap. Simulation experiments show that this does not prevent the preferred version from having the smallest error in rejection probability in small and medium-sized samples.},
  keywords = {Bootstrap inference,Heteroskedasticity,notion,Wild bootstrap},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\davidson2008.pdf;C\:\\Users\\david\\Zotero\\storage\\3PXCNHAF\\S0304407608000833.html}
}

@article{dehbi2017,
  title = {Life Expectancy Difference and Life Expectancy Ratio: Two Measures of Treatment Effects in Randomised Trials with Non-Proportional Hazards},
  shorttitle = {Life Expectancy Difference and Life Expectancy Ratio},
  author = {Dehbi, Hakim-Moulay and Royston, Patrick and Hackshaw, Allan},
  date = {2017-05-25},
  journaltitle = {BMJ},
  shortjournal = {BMJ},
  pages = {j2250},
  issn = {0959-8138, 1756-1833},
  doi = {10.1136/bmj.j2250},
  url = {https://www.bmj.com/lookup/doi/10.1136/bmj.j2250},
  urldate = {2023-08-21},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\dehbi2017.pdf}
}

@article{deltuvaite-thomas2023,
  title = {Generalized Pairwise Comparisons for Censored Data: {{An}} Overview},
  shorttitle = {Generalized Pairwise Comparisons for Censored Data},
  author = {Deltuvaite-Thomas, Vaiva and Verbeeck, Johan and Burzykowski, Tomasz and Buyse, Marc and Tournigand, Christophe and Molenberghs, Geert and Thas, Olivier},
  date = {2023},
  journaltitle = {Biometrical Journal},
  volume = {65},
  number = {2},
  pages = {2100354},
  issn = {1521-4036},
  doi = {10.1002/bimj.202100354},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.202100354},
  urldate = {2023-10-01},
  abstract = {The method of generalized pairwise comparisons (GPC) is an extension of the well-known nonparametric Wilcoxon–Mann–Whitney test for comparing two groups of observations. Multiple generalizations of Wilcoxon–Mann–Whitney test and other GPC methods have been proposed over the years to handle censored data. These methods apply different approaches to handling loss of information due to censoring: ignoring noninformative pairwise comparisons due to censoring (Gehan, Harrell, and Buyse); imputation using estimates of the survival distribution (Efron, Péron, and Latta); or inverse probability of censoring weighting (IPCW, Datta and Dong). Based on the GPC statistic, a measure of treatment effect, the “net benefit,” can be defined. It quantifies the difference between the probabilities that a randomly selected individual from one group is doing better than an individual from the other group. This paper aims at evaluating GPC methods for censored data, both in the context of hypothesis testing and estimation, and providing recommendations related to their choice in various situations. The methods that ignore uninformative pairs have comparable power to more complex and computationally demanding methods in situations of low censoring, and are slightly superior for high proportions ({$>$}40\%) of censoring. If one is interested in estimation of the net benefit, Harrell's c index is an unbiased estimator if the proportional hazards assumption holds. Otherwise, the imputation (Efron or Peron) or IPCW (Datta, Dong) methods provide unbiased estimators in case of proportions of drop-out censoring up to 60\%.},
  langid = {english},
  keywords = {bias,censored outcome,generalized pairwise comparisons,net benefit,notion,statistical power},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\deltuvaite-thomas2023.pdf;C\:\\Users\\david\\Zotero\\storage\\LNYMKK2B\\bimj.html}
}

@article{ditzhaus2020,
  title = {More Powerful Logrank Permutation Tests for Two-Sample Survival Data},
  author = {Ditzhaus, Marc and Friedrich, Sarah},
  date = {2020-08-12},
  journaltitle = {Journal of Statistical Computation and Simulation},
  volume = {90},
  number = {12},
  pages = {2209--2227},
  publisher = {Taylor \& Francis},
  issn = {0094-9655},
  doi = {10.1080/00949655.2020.1773463},
  url = {https://doi.org/10.1080/00949655.2020.1773463},
  urldate = {2023-08-30},
  abstract = {Weighted logrank tests are a popular tool for analysing right-censored survival data from two independent samples. Each of these tests is optimal against a certain hazard alternative, for example, the classical logrank test for proportional hazards. But which weight function should be used in practical applications? We address this question by a flexible combination idea leading to a testing procedure with broader power. Besides the test's asymptotic exactness and consistency, its power behaviour under local alternatives is derived. All theoretical properties can be transferred to a permutation version of the test, which is even finitely exact under exchangeability and showed a better finite sample performance in our simulation study. The procedure is illustrated in a real data example.},
  keywords = {Local alternatives,notion,right censoring,two-sample survival model,weighted logrank test},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\ditzhaus2020.pdf}
}

@article{ditzhaus2023,
  title = {Studentized Permutation Method for Comparing Two Restricted Mean Survival Times with Small Sample from Randomized Trials},
  author = {Ditzhaus, Marc and Yu, Menggang and Xu, Jin},
  date = {2023},
  journaltitle = {Statistics in Medicine},
  volume = {42},
  number = {13},
  pages = {2226--2240},
  issn = {1097-0258},
  doi = {10.1002/sim.9720},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9720},
  urldate = {2023-09-29},
  abstract = {Recent observations, especially in cancer immunotherapy clinical trials with time-to-event outcomes, show that the commonly used proportional hazard assumption is often not justifiable, hampering an appropriate analysis of the data by hazard ratios. An attractive alternative advocated is given by the restricted mean survival time (RMST), which does not rely on any model assumption and can always be interpreted intuitively. Since methods for the RMST based on asymptotic theory suffer from inflated type-I error under small sample sizes, a permutation test was proposed recently leading to more convincing results in simulations. However, classical permutation strategies require an exchangeable data setup between comparison groups which may be limiting in practice. Besides, it is not possible to invert related testing procedures to obtain valid confidence intervals, which can provide more in-depth information. In this paper, we address these limitations by proposing a studentized permutation test as well as respective permutation-based confidence intervals. In an extensive simulation study, we demonstrate the advantage of our new method, especially in situations with relatively small sample sizes and unbalanced groups. Finally, we illustrate the application of the proposed method by re-analyzing data from a recent lung cancer clinical trial.},
  langid = {english},
  keywords = {hazard ratio,notion,permutation methods,restricted mean survival time,survival analysis,time-to-event outcomes},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\ditzhaus2023.pdf;C\:\\Users\\david\\Zotero\\storage\\726F9UMN\\sim.html}
}

@article{dixit2021,
  title = {Multi-Arm Multi-Stage Clinical Trials for Time-to-Event Outcomes},
  author = {Dixit, Vaidehi and Mitra, Priyam and Simonsen, Katy},
  date = {2021-11-02},
  journaltitle = {Journal of Biopharmaceutical Statistics},
  shortjournal = {Journal of Biopharmaceutical Statistics},
  volume = {31},
  number = {6},
  pages = {838--851},
  issn = {1054-3406, 1520-5711},
  doi = {10.1080/10543406.2021.1979575},
  url = {https://www.tandfonline.com/doi/full/10.1080/10543406.2021.1979575},
  urldate = {2023-08-21},
  abstract = {This paper investigates the use of a general multi-arm multi-stage (MAMS) approach for time-to-event outcomes that would streamline simultaneous comparison of a large number of promising therapies in clinical trials, thus significantly reducing the time and the number of patients needed to evaluate the treatment. Controlling type I error in this setting is different than regular clinical trials as this approach incorporates both multiple com­ parison between arms and multiple stages. Historically, pairwise (PWER) and familywise (FWER) type I error rates have been primarily used to regulate the type I error in such designs. This paper will focus on constructing the efficacy and futility boundaries for a MAMS clinical trial in two different scenarios. In the first, it is assumed that the same outcome is used throughout the clinical trial for both intermediate and final assessments. In this scenario, we propose using the generalized Dunnett procedure that controls FWER. In the latter scenario, where intermediate and final outcomes are different in nature, we propose modifications to the existing method that originally concentrated on controlling PWER and extend the method to include FWER in the design. We also explore the performance of the proposed MAMS design in a setting where the proportional hazard assumption is violated in the presence of a delayed treatment effect and demonstrate the loss of power because of that. An alternative test statistic that can help circumvent this problem to main­ tain the desired power is also suggested.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\dixit2021.pdf}
}

@article{dobler2018,
  title = {Bootstrap- and Permutation-Based Inference for the {{Mann}}–{{Whitney}} Effect for Right-Censored and Tied Data},
  author = {Dobler, Dennis and Pauly, Markus},
  date = {2018-09-01},
  journaltitle = {TEST},
  shortjournal = {TEST},
  volume = {27},
  number = {3},
  pages = {639--658},
  issn = {1863-8260},
  doi = {10.1007/s11749-017-0565-z},
  url = {https://doi.org/10.1007/s11749-017-0565-z},
  urldate = {2023-08-29},
  abstract = {The Mann–Whitney effect is an intuitive measure for discriminating two survival distributions. Here we analyse various inference techniques for this parameter in a two-sample survival setting with independent right-censoring, where the survival times are even allowed to be discretely distributed. This allows for ties in the data and requires the introduction of normalized versions of Kaplan–Meier estimators from which adequate point estimates are deduced. Asymptotically exact inference procedures based on standard normal, bootstrap, and permutation quantiles are developed and compared in simulations. Here, the asymptotically robust and—under exchangeable data—even finitely exact permutation procedure turned out to be the best. Finally, all procedures are illustrated using a real data set.},
  langid = {english},
  keywords = {62G09,62G20,62N02,62N03,Counting process,Efron’s bootstrap,Heteroscedasticity,Kaplan–Meier estimator,notion,Permutation technique},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\dobler2018.pdf}
}

@article{dong2020,
  title = {The Win Ratio: {{Impact}} of Censoring and Follow-up Time and Use with Nonproportional Hazards},
  shorttitle = {The Win Ratio},
  author = {Dong, Gaohong and Huang, Bo and Chang, Yu-Wei and Seifu, Yodit and Song, James and Hoaglin, David C.},
  date = {2020},
  journaltitle = {Pharmaceutical Statistics},
  volume = {19},
  number = {3},
  pages = {168--177},
  issn = {1539-1612},
  doi = {10.1002/pst.1977},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1977},
  urldate = {2023-10-01},
  abstract = {The win ratio has been studied methodologically and applied in data analysis and in designing clinical trials. Researchers have pointed out that the results depend on follow-up time and censoring time, which are sometimes used interchangeably. In this article, we distinguish between follow-up time and censoring time, show theoretically the impact of censoring on the win ratio, and illustrate the impact of follow-up time. We then point out that, if the treatment has long-term benefit from a more important but less frequent endpoint (eg, death), the win ratio can show that benefit by following patients longer, avoiding masking by more frequent but less important outcomes, which occurs in conventional time-to-first-event analyses. For the situation of nonproportional hazards, we demonstrate that the win ratio can be a good alternative to methods such as landmark survival rate, restricted mean survival time, and weighted log-rank tests.},
  langid = {english},
  keywords = {hazard ratio,landmark survival rate,log-rank test,notion,prioritized pairwise comparisons,restricted mean survival time},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\dong2020.pdf;C\:\\Users\\david\\Zotero\\storage\\3AH3HMQY\\pst.html}
}

@article{dong2023,
  title = {Win Statistics (Win Ratio, Win Odds, and Net Benefit) Can Complement One Another to Show the Strength of the Treatment Effect on Time-to-Event Outcomes},
  author = {Dong, Gaohong and Huang, Bo and Verbeeck, Johan and Cui, Ying and Song, James and Gamalo-Siebers, Margaret and Wang, Duolao and Hoaglin, David C. and Seifu, Yodit and Mütze, Tobias and Kolassa, John},
  date = {2023},
  journaltitle = {Pharmaceutical Statistics},
  volume = {22},
  number = {1},
  pages = {20--33},
  issn = {1539-1612},
  doi = {10.1002/pst.2251},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.2251},
  urldate = {2023-10-01},
  abstract = {Conventional analyses of a composite of multiple time-to-event outcomes use the time to the first event. However, the first event may not be the most important outcome. To address this limitation, generalized pairwise comparisons and win statistics (win ratio, win odds, and net benefit) have become popular and have been applied to clinical trial practice. However, win ratio, win odds, and net benefit have typically been used separately. In this article, we examine the use of these three win statistics jointly for time-to-event outcomes. First, we explain the relation of point estimates and variances among the three win statistics, and the relation between the net benefit and the Mann–Whitney U statistic. Then we explain that the three win statistics are based on the same win proportions, and they test the same null hypothesis of equal win probabilities in two groups. We show theoretically that the Z-values of the corresponding statistical tests are approximately equal; therefore, the three win statistics provide very similar p-values and statistical powers. Finally, using simulation studies and data from a clinical trial, we demonstrate that, when there is no (or little) censoring, the three win statistics can complement one another to show the strength of the treatment effect. However, when the amount of censoring is not small, and without adjustment for censoring, the win odds and the net benefit may have an advantage for interpreting the treatment effect; with adjustment (e.g., IPCW adjustment) for censoring, the three win statistics can complement one another to show the strength of the treatment effect. For calculations we use the R package WINS, available on the CRAN (Comprehensive R Archive Network).},
  langid = {english},
  keywords = {generalized pairwise comparisons,inverse-probability-of-censoring weighting,IPCW,IPCW-adjusted win statistics,Mann–Whitney U statistic,notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\dong2023.pdf;C\:\\Users\\david\\Zotero\\storage\\MVY8DNNP\\pst.html}
}

@article{dormuth2022,
  title = {Which Test for Crossing Survival Curves? {{A}} User’s Guideline},
  shorttitle = {Which Test for Crossing Survival Curves?},
  author = {Dormuth, Ina and Liu, Tiantian and Xu, Jin and Yu, Menggang and Pauly, Markus and Ditzhaus, Marc},
  date = {2022-01-30},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  volume = {22},
  number = {1},
  pages = {34},
  issn = {1471-2288},
  doi = {10.1186/s12874-022-01520-0},
  url = {https://doi.org/10.1186/s12874-022-01520-0},
  urldate = {2023-08-30},
  abstract = {The exchange of knowledge between statisticians developing new methodology and clinicians, reviewers or authors applying them is fundamental. This is specifically true for clinical trials with time-to-event endpoints. Thereby, one of the most commonly arising questions is that of equal survival distributions in two-armed trial. The log-rank test is still the gold-standard to infer this question. However, in case of non-proportional hazards, its power can become poor and multiple extensions have been developed to overcome this issue. We aim to facilitate the choice of a test for the detection of survival differences in the case of crossing hazards.},
  keywords = {Crossing,Log-rank test,Non-proportional hazards,notion,Oncology,Restricted-mean survival,Survival analysis,Time-to-event outcome},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\dormuth2022.pdf}
}

@article{dormuth2023,
  title = {A Comparative Study to Alternatives to the Log-Rank Test},
  author = {Dormuth, Ina and Liu, Tiantian and Xu, Jin and Pauly, Markus and Ditzhaus, Marc},
  date = {2023-05-01},
  journaltitle = {Contemporary Clinical Trials},
  shortjournal = {Contemporary Clinical Trials},
  volume = {128},
  pages = {107165},
  issn = {1551-7144},
  doi = {10.1016/j.cct.2023.107165},
  url = {https://www.sciencedirect.com/science/article/pii/S1551714423000885},
  urldate = {2023-08-30},
  abstract = {Background Studies to compare the survival of two or more groups using time-to-event data are of high importance in medical research. The gold standard is the log-rank test, which is optimal under proportional hazards. As the latter is no simple regularity assumption, we are interested in evaluating the power of various statistical tests under different settings including proportional and non-proportional hazards with a special emphasis on crossing hazards. This challenge has been going on for many years now and multiple methods have already been investigated in extensive simulation studies. However, in recent years new omnibus tests and methods based on the restricted mean survival time appeared that have been strongly recommended in biometric literature. Methods Thus, to give updated recommendations, we perform a vast simulation study to compare tests that showed high power in previous studies with these more recent approaches. We thereby analyze various simulation settings with varying survival and censoring distributions, unequal censoring between groups, small sample sizes and unbalanced group sizes. Results Overall, omnibus tests are more robust in terms of power against deviations from the proportional hazards assumption. Conclusion We recommend considering the more robust omnibus approaches for group comparison in case of uncertainty about the underlying survival time distributions.},
  keywords = {Crossing hazards,Log-rank,Non-proportional hazards,notion,Simulation study,Survival analysis},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\dormuth2023.pdf;C\:\\Users\\david\\Zotero\\storage\\TZSWVCMH\\S1551714423000885.html}
}

@article{eaton2020,
  title = {Designing Clinical Trials with (Restricted) Mean Survival Time Endpoint: {{Practical}} Considerations},
  shorttitle = {Designing Clinical Trials with (Restricted) Mean Survival Time Endpoint},
  author = {Eaton, Anne and Therneau, Terry and Le-Rademacher, Jennifer},
  date = {2020-06-01},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {17},
  number = {3},
  pages = {285--294},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774520905563},
  url = {https://doi.org/10.1177/1740774520905563},
  urldate = {2023-08-29},
  abstract = {Background/aims: The difference in mean survival time, which quantifies the treatment effect in terms most meaningful to patients and retains its interpretability regardless of the shape of the survival distribution or the proportionality of the treatment effect, is an alternative endpoint that could be used more often as the primary endpoint to design clinical trials. The underuse of this endpoint is due to investigators’ lack of familiarity with the test comparing the mean survival times and the lack of tools to facilitate trial design with this endpoint. The aim of this article is to provide investigators with insights and software to design trials with restricted mean survival time as the primary endpoint. Methods: A closed-form formula for the asymptotic power of the test of restricted mean survival time difference is presented. The effects of design parameters on power were evaluated for the mean survival time test and log-rank test. An R package which calculates the power or the sample size for user-specified parameter values and provides power plots for each design parameter is provided. The R package also calculates the probability that the restricted mean survival time is estimable for user-defined trial designs. Results: Under proportional hazards and late differences in survival, the power of the mean survival time test can approach that of the log-rank test if the restriction time is late. Under early differences, the power of the restricted mean survival time test is higher than that of the log-rank test. Duration of accrual and follow-up have little influence on the power of the restricted mean survival time test. The choice of restriction time, on the other hand, has a large impact on power. Because the power depends on the interplay among the design factors, plotting the relationship between each design parameter and power allows the users to select the designs most appropriate for their trial. When modification is necessary to ensure the difference in restricted mean survival time is estimable, the three available modifications all perform adequately in the scenarios studied. Conclusion: The restricted mean survival time is a survival endpoint that is meaningful to investigators and to patients and at the same time requires less restrictive assumptions. The biggest challenge with this endpoint is selection of the restriction time. We recommend selecting a restriction time that is clinically relevant to the disease and the clinical setting of the trial of interest. The practical considerations and the R package provided in this work are readily available tools that researchers can use to design trials with restricted mean survival time as the primary endpoint.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\eaton2020.pdf}
}

@article{eaton2020a,
  title = {Designing Clinical Trials with (Restricted) Mean Survival Time Endpoint: {{Practical}} Considerations},
  shorttitle = {Designing Clinical Trials with (Restricted) Mean Survival Time Endpoint},
  author = {Eaton, Anne and Therneau, Terry and Le-Rademacher, Jennifer},
  date = {2020-06-01},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {17},
  number = {3},
  pages = {285--294},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774520905563},
  url = {https://doi.org/10.1177/1740774520905563},
  urldate = {2023-09-08},
  abstract = {Background/aims: The difference in mean survival time, which quantifies the treatment effect in terms most meaningful to patients and retains its interpretability regardless of the shape of the survival distribution or the proportionality of the treatment effect, is an alternative endpoint that could be used more often as the primary endpoint to design clinical trials. The underuse of this endpoint is due to investigators’ lack of familiarity with the test comparing the mean survival times and the lack of tools to facilitate trial design with this endpoint. The aim of this article is to provide investigators with insights and software to design trials with restricted mean survival time as the primary endpoint. Methods: A closed-form formula for the asymptotic power of the test of restricted mean survival time difference is presented. The effects of design parameters on power were evaluated for the mean survival time test and log-rank test. An R package which calculates the power or the sample size for user-specified parameter values and provides power plots for each design parameter is provided. The R package also calculates the probability that the restricted mean survival time is estimable for user-defined trial designs. Results: Under proportional hazards and late differences in survival, the power of the mean survival time test can approach that of the log-rank test if the restriction time is late. Under early differences, the power of the restricted mean survival time test is higher than that of the log-rank test. Duration of accrual and follow-up have little influence on the power of the restricted mean survival time test. The choice of restriction time, on the other hand, has a large impact on power. Because the power depends on the interplay among the design factors, plotting the relationship between each design parameter and power allows the users to select the designs most appropriate for their trial. When modification is necessary to ensure the difference in restricted mean survival time is estimable, the three available modifications all perform adequately in the scenarios studied. Conclusion: The restricted mean survival time is a survival endpoint that is meaningful to investigators and to patients and at the same time requires less restrictive assumptions. The biggest challenge with this endpoint is selection of the restriction time. We recommend selecting a restriction time that is clinically relevant to the disease and the clinical setting of the trial of interest. The practical considerations and the R package provided in this work are readily available tools that researchers can use to design trials with restricted mean survival time as the primary endpoint.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\eaton2020a.pdf}
}

@article{edmonson1979,
  title = {Different Chemotherapeutic Sensitivities and Host Factors Affecting Prognosis in Advanced Ovarian Carcinoma versus Minimal Residual Disease},
  author = {Edmonson, J. H. and Fleming, T. R. and Decker, D. G. and Malkasian, G. D. and Jorgensen, E. O. and Jefferies, J. A. and Webb, M. J. and Kvols, L. K.},
  date = {1979-02},
  journaltitle = {Cancer Treatment Reports},
  shortjournal = {Cancer Treat Rep},
  volume = {63},
  number = {2},
  eprint = {445503},
  eprinttype = {pmid},
  pages = {241--247},
  issn = {0361-5960},
  abstract = {Treatment of patients with advanced ovarian carcinoma (stages IIIB and IV) using either cyclophosphamide alone (1 g/m2) or cyclophosphamide (500 mg/m2) plus adriamycin (40 mg/m2) by iv injection every 3 weeks each produced partial regression in approximately one third of the patients. Survival curves and time-to-progression curves for the two regimens were nearly identical in these patients with advanced disease. These same regimens produced different results when used monthly in patients who had minimal residual disease (stages II and IIIA). In patients with minimal residual disease the therapeutic index of the combination regimen was superior to that of cyclophosphamide alone. Prognosis was better overall among patients with minimal residual disease than among patients with advanced disease. Within the minimal-disease group grossly complete excision of tumor prior to chemotherapy was associated with still better prognosis. Among patients with advanced disease, prognosis was significantly better for older patients despite their generally less favorable performance scores. Much of this prognostic superiority appeared to be related to menopausal status and presumably to the depletion of endogenous estrogens in the older patients.},
  langid = {english},
  keywords = {Cyclophosphamide,Doxorubicin,Drug Therapy Combination,Female,Humans,Menopause,Middle Aged,Neoplasm Staging,notion,Ovarian Neoplasms,Prognosis,Remission Spontaneous,Time Factors}
}

@book{efron1993,
  title = {An {{Introduction}} to the {{Bootstrap}}},
  author = {Efron, Bradley and Tibshirani, Robert J.},
  date = {1993},
  publisher = {Springer US},
  location = {Boston, MA},
  doi = {10.1007/978-1-4899-4541-9},
  url = {http://link.springer.com/10.1007/978-1-4899-4541-9},
  urldate = {2023-11-13},
  isbn = {978-0-412-04231-7 978-1-4899-4541-9},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\efron1993.pdf}
}

@book{fahrmeir2013,
  title = {Regression: {{Models}}, {{Methods}} and {{Applications}}},
  shorttitle = {Regression},
  author = {Fahrmeir, Ludwig and Kneib, Thomas and Lang, Stefan and Marx, Brian},
  date = {2013},
  publisher = {Springer},
  location = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-34333-9},
  url = {https://link.springer.com/10.1007/978-3-642-34333-9},
  urldate = {2024-02-19},
  isbn = {978-3-642-34332-2 978-3-642-34333-9},
  langid = {english},
  keywords = {generalized linear models,linear regression,mixed models,notion,semiparametric regression,spatial regression},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\fahrmeir2013.pdf}
}

@article{fine2007,
  title = {Consequences of {{Delayed Treatment Effects}} on {{Analysis}} of {{Time-to-Event Endpoints}}},
  author = {Fine, Gil D.},
  date = {2007-07-01},
  journaltitle = {Drug Information Journal},
  volume = {41},
  number = {4},
  pages = {535--539},
  publisher = {SAGE Publications},
  issn = {0092-8615},
  doi = {10.1177/009286150704100412},
  url = {https://doi.org/10.1177/009286150704100412},
  urldate = {2023-08-29},
  abstract = {The assumption of proportional hazard ratios is implicit in certain analyses of time-to-event endpoints such as Cox regression. Other statistical analyses, such as the nonparametric logrank test, possess some desirable properties only under the proportional hazards model. Data models for delayed effects of treatment on time-to-event endpoints such as survival violate the proportional hazards assumption. Fleming and Harrington's Gρ,γ class of weighted log-rank tests, a new option in SAS 9.1, is appropriate to test against a broad range of alternative hypotheses, including delayed treatment effects. A model for delayed treatment effects is proposed, and it is demonstrated that weighted log-rank tests are more powerful under this model than the standard unweighted log-rank test.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\fine2007.pdf}
}

@article{freidlin2019,
  title = {Methods for {{Accommodating Nonproportional Hazards}} in {{Clinical Trials}}: {{Ready}} for the {{Primary Analysis}}?},
  shorttitle = {Methods for {{Accommodating Nonproportional Hazards}} in {{Clinical Trials}}},
  author = {Freidlin, Boris and Korn, Edward L.},
  date = {2019-12-10},
  journaltitle = {Journal of Clinical Oncology},
  shortjournal = {JCO},
  volume = {37},
  number = {35},
  pages = {3455--3459},
  issn = {0732-183X, 1527-7755},
  doi = {10.1200/JCO.19.01681},
  url = {https://ascopubs.org/doi/10.1200/JCO.19.01681},
  urldate = {2023-08-21},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\freidlin2019.pdf}
}

@article{freidlin2021,
  title = {Are Restricted Mean Survival Time Methods Especially Useful for Noninferiority Trials?},
  author = {Freidlin, Boris and Hu, Chen and Korn, Edward L},
  date = {2021-04},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {18},
  number = {2},
  pages = {188--196},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774520976576},
  url = {http://journals.sagepub.com/doi/10.1177/1740774520976576},
  urldate = {2023-08-23},
  abstract = {Background: Restricted mean survival time methods compare the areas under the Kaplan–Meier curves up to a time t for the control and experimental treatments. Extraordinary claims have been made about the benefits (in terms of dramatically smaller required sample sizes) when using restricted mean survival time methods as compared to proportional hazards methods for analyzing noninferiority trials, even when the true survival distributions satisfy proportional hazardss. Methods: Through some limited simulations and asymptotic power calculations, the authors compare the operating characteristics of restricted mean survival time and proportional hazards methods for analyzing both noninferiority and superiority trials under proportional hazardss to understand what relative power benefits there are when using restricted mean survival time methods for noninferiority testing. Results: In the setting of low-event rates, very large targeted noninferiority margins, and limited follow-up past t, restricted mean survival time methods have more power than proportional hazards methods. For superiority testing, proportional hazards methods have more power. This is not a small-sample phenomenon but requires a low-event rate and a large noninferiority margin. Conclusion: Although there are special settings where restricted mean survival time methods have a power advantage over proportional hazards methods for testing noninferiority, the larger issue in these settings is defining appropriate noninferiority margins. We find the restricted mean survival time methods lacking in these regards.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\freidlin2021.pdf}
}

@article{friedrich2024,
  title = {On the Role of Benchmarking Data Sets and Simulations in Method Comparison Studies},
  author = {Friedrich, Sarah and Friede, Tim},
  date = {2024},
  journaltitle = {Biometrical Journal},
  volume = {66},
  number = {1},
  pages = {2200212},
  issn = {1521-4036},
  doi = {10.1002/bimj.202200212},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.202200212},
  urldate = {2024-02-22},
  abstract = {Method comparisons are essential to provide recommendations and guidance for applied researchers, who often have to choose from a plethora of available approaches. While many comparisons exist in the literature, these are often not neutral but favor a novel method. Apart from the choice of design and a proper reporting of the findings, there are different approaches concerning the underlying data for such method comparison studies. Most manuscripts on statistical methodology rely on simulation studies and provide a single real-world data set as an example to motivate and illustrate the methodology investigated. In the context of supervised learning, in contrast, methods are often evaluated using so-called benchmarking data sets, that is, real-world data that serve as gold standard in the community. Simulation studies, on the other hand, are much less common in this context. The aim of this paper is to investigate differences and similarities between these approaches, to discuss their advantages and disadvantages, and ultimately to develop new approaches to the evaluation of methods picking the best of both worlds. To this aim, we borrow ideas from different contexts such as mixed methods research and Clinical Scenario Evaluation.},
  langid = {english},
  keywords = {benchmarking,machine learning,neutral comparison studies,notion,simulation studies},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\friedrich2024.pdf;C\:\\Users\\david\\Zotero\\storage\\ZUVGMQTB\\bimj.html}
}

@article{fukuda2023,
  title = {The Net Benefit for Time-to-Event Outcome in Oncology Clinical Trials with Treatment Switching},
  author = {Fukuda, Musashi and Sakamaki, Kentaro and Oba, Koji},
  date = {2023-07-16},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  pages = {17407745231186081},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/17407745231186081},
  url = {https://doi.org/10.1177/17407745231186081},
  urldate = {2023-10-01},
  abstract = {Background The net benefit is an effect measure for any type of endpoint, including the time-to-event outcome, and can provide intuitive and clinically meaningful interpretation. It is defined as the probability of a randomly selected subject from the experimental arm surviving by at least a clinically relevant time longer than a randomly selected subject from the control arm. In oncology clinical trials, an intercurrent event such as treatment switching is common, which potentially causes informative censoring; nevertheless, conventional methods for the net benefit are not able to deal with it. In this study, we proposed a new estimator using the inverse probability of censoring weighting (IPCW) method and illustrated an oncology clinical trial with treatment switching (the SHIVA study) to apply the proposed method under the estimand framework. Methods The net benefit can be estimated using the survival functions of each treatment group. The proposed estimator was based on the survival functions estimated by the inverse probability of the censoring weighting method that can handle covariate-dependent censoring. The simulation study was undertaken to evaluate the operating characteristics of the proposed estimator under several scenarios; we varied the shapes of the survival curves, treatment effect, covariates effect on censoring, proportion of the censoring, threshold of the net benefit, and sample size. We also applied conventional methods (the scoring rules by Péron or Gehan) and the proposed method to the SHIVA study. Results Our simulation study showed that the proposed estimator provided less biased results under the covariate-dependent censoring than existing estimators. When applying the proposed method to the SHIVA study, we were able to estimate the net benefit by incorporating the information of the covariates with different estimand strategies to address the intercurrent event of the treatment switching. However, the estimates of the proposed method and those of the aforementioned conventional methods were similar under the hypothetical strategy. Conclusions We proposed a new estimator of the net benefit that can include covariates to account for the possibly informative censoring. We also provided an illustrative analysis of the proposed method for the oncology clinical trial with treatment switching using the estimand framework. Our proposed new estimator is suitable for handling the intercurrent events that can potentially cause covariate-dependent censoring.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\fukuda2023.pdf}
}

@article{giai2021,
  title = {Net Benefit in the Presence of Correlated Prioritized Outcomes Using Generalized Pairwise Comparisons: {{A}} Simulation Study},
  shorttitle = {Net Benefit in the Presence of Correlated Prioritized Outcomes Using Generalized Pairwise Comparisons},
  author = {Giai, Joris and Maucort-Boulch, Delphine and Ozenne, Brice and Chiêm, Jean-Christophe and Buyse, Marc and Péron, Julien},
  date = {2021},
  journaltitle = {Statistics in Medicine},
  volume = {40},
  number = {3},
  pages = {553--565},
  issn = {1097-0258},
  doi = {10.1002/sim.8788},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8788},
  urldate = {2023-10-01},
  abstract = {Background The prioritized net benefit (Δ) is a measure of the benefit-risk balance in clinical trials, based on generalized pairwise comparisons (GPC) using several prioritized outcomes. Its estimation requires the classification as Wins or Losses of all possible pairs of patients, one from the experimental treatment (E) group and one from the control treatment (C) group. In this simulation study, we assessed the impact of the correlation between prioritized outcomes on Δ, its estimate, bias, size, and power. Methods The theoretical Δ value was derived for the specific case of two correlated binary outcomes when a normal copula is used. Focusing on one efficacy and one toxicity outcome, two situations frequently met in practice were simulated: binary efficacy outcome with binary toxicity outcome, or time to event efficacy outcome with categorical toxicity outcome. Several scenarios of efficacy and toxicity were generated, with various levels of correlation. Results When E was more effective than C, positive correlations were mainly associated with a decrease in the proportion of Losses, while negative correlations were associated with a decrease in the proportion of Wins on the toxicity outcome. This resulted in an increase of Δ\^{} with the intensity of the positive correlation without adding any bias. Results were similar whatever the type of outcomes generated but led to power alteration. Conclusion Correlations between outcomes analyzed with GPC led to substantial but predictable modifications of Δ and its estimate. Correlations should be taken into consideration when performing sample size estimations in clinical trials.},
  langid = {english},
  keywords = {clinical trial,correlation,generalized pairwise comparisons,multivariate analysis,net benefit,notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\giai2021.pdf}
}

@inproceedings{giordano2019,
  title = {A {{Swiss Army Infinitesimal Jackknife}}},
  booktitle = {Proceedings of the {{Twenty-Second International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Giordano, Ryan and Stephenson, William and Liu, Runjing and Jordan, Michael and Broderick, Tamara},
  date = {2019-04-11},
  pages = {1139--1147},
  publisher = {PMLR},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v89/giordano19a.html},
  urldate = {2024-02-21},
  abstract = {The error or variability of machine learning algorithms is often assessed by repeatedly refitting a model with different weighted versions of the observed data. The ubiquitous tools of cross-validation (CV) and the bootstrap are examples of this technique. These methods are powerful in large part due to their model agnosticism but can be slow to run on modern, large data sets due to the need to repeatedly re-fit the model. In this work, we use a linear approximation to the dependence of the fitting procedure on the weights, producing results that can be faster than repeated re-fitting by an order of magnitude. This linear approximation is sometimes known as the "infinitesimal jackknife" in the statistics literature, where it is mostly used as a theoretical tool to prove asymptotic results. We provide explicit finite-sample error bounds for the infinitesimal jackknife in terms of a small number of simple, verifiable assumptions. Our results apply whether the weights and data are stochastic or deterministic, and so can be used as a tool for proving the accuracy of the infinitesimal jackknife on a wide variety of problems. As a corollary, we state mild regularity conditions under which our approximation consistently estimates true leave k-out cross-validation for any fixed k. These theoretical results, together with modern automatic differentiation software, support the application of the infinitesimal jackknife to a wide variety of practical problems in machine learning, providing a "Swiss Army infinitesimal jackknife." We demonstrate the accuracy of our methods on a range of simulated and real datasets.},
  eventtitle = {The 22nd {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\giordano2019.pdf;C\:\\Users\\david\\Zotero\\storage\\RKKI4XXA\\Giordano et al. - 2019 - A Swiss Army Infinitesimal Jackknife.pdf}
}

@article{gorfine2020,
  title = {K-Sample Omnibus Non-Proportional Hazards Tests Based on Right-Censored Data},
  author = {Gorfine, Malka and Schlesinger, Matan and Hsu, Li},
  date = {2020-10-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {29},
  number = {10},
  pages = {2830--2850},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280220907355},
  url = {https://doi.org/10.1177/0962280220907355},
  urldate = {2023-08-30},
  abstract = {This work presents novel and powerful tests for comparing non-proportional hazard functions, based on sample–space partitions. Right censoring introduces two major difficulties, which make the existing sample–space partition tests for uncensored data non-applicable: (i) the actual event times of censored observations are unknown and (ii) the standard permutation procedure is invalid in case the censoring distributions of the groups are unequal. We overcome these two obstacles, introduce invariant tests, and prove their consistency. Extensive simulations reveal that under non-proportional alternatives, the proposed tests are often of higher power compared with existing popular tests for non-proportional hazards. Efficient implementation of our tests is available in the R package KONPsurv, which can be freely downloaded from CRAN.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\gorfine2020.pdf}
}

@article{grambsch1994,
  title = {Proportional Hazards Tests and Diagnostics Based on Weighted Residuals},
  author = {Grambsch, Patricia M. and Therneau, Terry M.},
  date = {1994-09-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {81},
  number = {3},
  pages = {515--526},
  issn = {0006-3444},
  doi = {10.1093/biomet/81.3.515},
  url = {https://doi.org/10.1093/biomet/81.3.515},
  urldate = {2024-03-01},
  abstract = {Nonproportional hazards can often be expressed by extending the Cox model to include time varying coefficients; e.g., for a single covariate, the hazard function for subject i is modelled as exp \{β(t)Zi(t)\}. A common example is a treatment effect that decreases with time. We show that the function βi(t) can be directly visualized by smoothing an appropriate residual plot. Also, many tests of proportional hazards, including those of Cox (1972), Gill \&amp; Schumacher (1987), Harrell (1986), Lin (1991), Moreau, O'Quigley \&amp; Mesbah (1985), Nagelkerke, Oosting \&amp; Hart (1984), O'Quigley \&amp; Pessione (1989), Schoenfeld (1980) and Wei (1984) are related to time-weighted score tests of the proportional hazards hypothesis, and can be visualized as a weighted least-squares line fitted to the residual plot.},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\grambsch1994.pdf;C\:\\Users\\david\\Zotero\\storage\\82AVU5KX\\257037.html}
}

@article{grana2002,
  title = {Pretargeted Adjuvant Radioimmunotherapy with {{Yttrium-90-biotin}} in Malignant Glioma Patients: {{A}} Pilot Study},
  shorttitle = {Pretargeted Adjuvant Radioimmunotherapy with {{Yttrium-90-biotin}} in Malignant Glioma Patients},
  author = {Grana, C. and Chinol, M. and Robertson, C. and Mazzetta, C. and Bartolomei, M. and De Cicco, C. and Fiorenza, M. and Gatti, M. and Caliceti, P. and Paganelli, G.},
  date = {2002-01},
  journaltitle = {British Journal of Cancer},
  shortjournal = {Br J Cancer},
  volume = {86},
  number = {2},
  pages = {207--212},
  publisher = {Nature Publishing Group},
  issn = {1532-1827},
  doi = {10.1038/sj.bjc.6600047},
  url = {https://www.nature.com/articles/6600047},
  urldate = {2024-03-19},
  abstract = {In a previous study we applied a three-step avidin–biotin pretargeting approach to target 90Y-biotin to the tumour in patients with recurrent high grade glioma. The encouraging results obtained in this phase I–II study prompted us to apply the same approach in an adjuvant setting, to evaluate (i) time to relapse and (ii) overall survival. We enrolled 37 high grade glioma patients, 17 with grade III glioma and 20 with glioblastoma, in a controlled open non-randomized study. All patients received surgery and radiotherapy and were disease-free by neuroradiological examinations. Nineteen patients (treated) received adjuvant treatment with radioimmunotherapy. In the treated glioblastoma patients, median disease-free interval was 28 months (range=9–59); median survival was 33.5 months and one patient is still without evidence of disease. All 12 control glioblastoma patients died after a median survival from diagnosis of 8 months. In the treated grade III glioma patients median disease-free interval was 56 months (range=15–60) and survival cannot be calculated as only two, within this group, died. Three-step radioimmunotherapy promises to have an important role as adjuvant treatment in high grade gliomas, particularly in glioblastoma where it impedes progression, prolonging time to relapse and overall survival. A further randomized trial is justified.},
  langid = {english},
  keywords = {Biomedicine,Cancer Research,Drug Resistance,Epidemiology,general,Molecular Medicine,notion,Oncology},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\grana2002.pdf}
}

@article{gu2023,
  title = {Omnibus Test for Restricted Mean Survival Time Based on Influence Function},
  author = {Gu, Jiaqi and Fan, Yiwei and Yin, Guosheng},
  date = {2023-06-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {32},
  number = {6},
  pages = {1082--1099},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/09622802231158735},
  url = {https://doi.org/10.1177/09622802231158735},
  urldate = {2023-09-16},
  abstract = {The restricted mean survival time (RMST), which evaluates the expected survival time up to a pre-specified time point τ, has been widely used to summarize the survival distribution due to its robustness and straightforward interpretation. In comparative studies with time-to-event data, the RMST-based test has been utilized as an alternative to the classic log-rank test because the power of the log-rank test deteriorates when the proportional hazards assumption is violated. To overcome the challenge of selecting an appropriate time point τ, we develop an RMST-based omnibus Wald test to detect the survival difference between two groups throughout the study follow-up period. Treating a vector of RMSTs at multiple quantile-based time points as a statistical functional, we construct a Wald χ2 test statistic and derive its asymptotic distribution using the influence function. We further propose a new procedure based on the influence function to estimate the asymptotic covariance matrix in contrast to the usual bootstrap method. Simulations under different scenarios validate the size of our RMST-based omnibus test and demonstrate its advantage over the existing tests in power, especially when the true survival functions cross within the study follow-up period. For illustration, the proposed test is applied to two real datasets, which demonstrate its power and applicability in various situations.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\gu2023.pdf}
}

@article{guyot2012,
  title = {Enhanced Secondary Analysis of Survival Data: Reconstructing the Data from Published {{Kaplan-Meier}} Survival Curves},
  shorttitle = {Enhanced Secondary Analysis of Survival Data},
  author = {Guyot, Patricia and family=Ades, given=AE, given-i=AE and Ouwens, Mario JNM and Welton, Nicky J.},
  date = {2012-02-01},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  volume = {12},
  number = {1},
  pages = {9},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-12-9},
  url = {https://doi.org/10.1186/1471-2288-12-9},
  urldate = {2024-03-01},
  abstract = {The results of Randomized Controlled Trials (RCTs) on time-to-event outcomes that are usually reported are median time to events and Cox Hazard Ratio. These do not constitute the sufficient statistics required for meta-analysis or cost-effectiveness analysis, and their use in secondary analyses requires strong assumptions that may not have been adequately tested. In order to enhance the quality of secondary data analyses, we propose a method which derives from the published Kaplan Meier survival curves a close approximation to the original individual patient time-to-event data from which they were generated.},
  keywords = {algorithm,Cost-Effectiveness Analysis,Health Technology Assessment,Individual Patient Data,Kaplan-Meier,life-table,notion,Survival analysis},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\guyot2012.pdf;C\:\\Users\\david\\Zotero\\storage\\2NLK4UD9\\1471-2288-12-9.html}
}

@article{hall1991,
  title = {Two {{Guidelines}} for {{Bootstrap Hypothesis Testing}}},
  author = {Hall, Peter and Wilson, Susan R.},
  date = {1991},
  journaltitle = {Biometrics},
  volume = {47},
  number = {2},
  eprint = {2532163},
  eprinttype = {jstor},
  pages = {757--762},
  publisher = {[Wiley, International Biometric Society]},
  issn = {0006-341X},
  doi = {10.2307/2532163},
  url = {https://www.jstor.org/stable/2532163},
  urldate = {2024-01-26},
  abstract = {Two guidelines for nonparametric bootstrap hypothesis testing are highlighted. The first recommends that resampling be done in a way that reflects the null hypothesis, even when the true hypothesis is distant from the null. The second guideline argues that bootstrap hypothesis tests should employ methods that are already recognized as having good features in the closely related problem of confidence interval construction. Violation of the first guideline can seriously reduce the power of a test. Sometimes this reduction is spectacular, since it is most serious when the null hypothesis is grossly in error. The second guideline is of some importance when the conclusion of a test is equivocal. It has no direct bearing on power, but improves the level accuracy of a test.},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\hall1991.pdf}
}

@article{hasegawa2020,
  title = {Restricted Mean Survival Time as a Summary Measure of Time-to-Event Outcome},
  author = {Hasegawa, Takahiro and Misawa, Saori and Nakagawa, Shintaro and Tanaka, Shinichi and Tanase, Takanori and Ugai, Hiroyuki and Wakana, Akira and Yodo, Yasuhide and Tsuchiya, Satoru and Suganami, Hideki and Members, for the JPMA Task Force},
  date = {2020},
  journaltitle = {Pharmaceutical Statistics},
  volume = {19},
  number = {4},
  pages = {436--453},
  issn = {1539-1612},
  doi = {10.1002/pst.2004},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.2004},
  urldate = {2023-09-06},
  abstract = {Many clinical research studies evaluate a time-to-event outcome, illustrate survival functions, and conventionally report estimated hazard ratios to express the magnitude of the treatment effect when comparing between groups. However, it may not be straightforward to interpret the hazard ratio clinically and statistically when the proportional hazards assumption is invalid. In some recent papers published in clinical journals, the use of restricted mean survival time (RMST) or τ-year mean survival time is discussed as one of the alternative summary measures for the time-to-event outcome. The RMST is defined as the expected value of time to event limited to a specific time point corresponding to the area under the survival curve up to the specific time point. This article summarizes the necessary information to conduct statistical analysis using the RMST, including the definition and statistical properties of the RMST, adjusted analysis methods, sample size calculation, information fraction for the RMST difference, and clinical and statistical meaning and interpretation. Additionally, we discuss how to set the specific time point to define the RMST from two main points of view. We also provide developed SAS codes to determine the sample size required to detect an expected RMST difference with appropriate power and reconstruct individual survival data to estimate an RMST reference value from a reported survival curve.},
  langid = {english},
  keywords = {notion,proportional hazards assumption,restricted mean survival time,summary measure,survival analysis,time-to-event data},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\hasegawa2020.pdf}
}

@article{hashimoto2022,
  title = {A Note on Confidence Intervals for the Restricted Mean Survival Time Based on Transformations in Small Sample Size},
  author = {Hashimoto, Hiroya and Kada, Akiko},
  date = {2022},
  journaltitle = {Pharmaceutical Statistics},
  volume = {21},
  number = {2},
  pages = {309--316},
  issn = {1539-1612},
  doi = {10.1002/pst.2170},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.2170},
  urldate = {2023-09-29},
  abstract = {Restricted mean survival time (RMST) is one measure now used to summarize time-to-event type data, but it has been pointed out that the distribution of differences in RMST deviates markedly from a normal distribution for controlled clinical trials with small sample sizes. Therefore, we conducted a numerical simulation of the RMST in which the one-sample survival time follows a Weibull distribution, comparing eight different confidence intervals combining two types of variance with four types of variable transformations, including no transformation. The evaluation items were the coverage probability and the above and below error probabilities for the true value. The variance types were based on Greenwood's formula and its Kaplan–Meier correction. The arcsine square root transformation, logit transformation, and complementary log–log transformation were used as the variable transformations. When the sample size was small and the event rate was low, the confidence interval of the untransformed RMST tended to have a small coverage probability and to be overestimated. Variance by Kaplan–Meier correction improved the coverage. The problems of coverage and overestimation were also improved by variable transformations, and in particular, applying the logit transformation and the complementary log–log transformation both resulted in substantial improvements. Our study suggested that it is preferable to construct the confidence intervals of RMST using the logit transformation for variances based on Greenwood's formula in small sample size trials. The SAS code to replicate the analyses is available at https://github.com/HiroyaHashimoto/SAS-Programs.},
  langid = {english},
  keywords = {arcsine square root transform,complementary log–log transform,logit transform,notion,restricted mean survival time,survival analysis},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\hashimoto2022.pdf;C\:\\Users\\david\\Zotero\\storage\\X3FV4IA4\\pst.html}
}

@article{hellmann2018,
  title = {Nivolumab plus {{Ipilimumab}} in {{Lung Cancer}} with a {{High Tumor Mutational Burden}}},
  author = {Hellmann, Matthew D. and Ciuleanu, Tudor-Eliade and Pluzanski, Adam and Lee, Jong Seok and Otterson, Gregory A. and Audigier-Valette, Clarisse and Minenza, Elisa and Linardou, Helena and Burgers, Sjaak and Salman, Pamela and Borghaei, Hossein and Ramalingam, Suresh S. and Brahmer, Julie and Reck, Martin and O’Byrne, Kenneth J. and Geese, William J. and Green, George and Chang, Han and Szustakowski, Joseph and Bhagavatheeswaran, Prabhu and Healey, Diane and Fu, Yali and Nathan, Faith and Paz-Ares, Luis},
  date = {2018-05-31},
  journaltitle = {New England Journal of Medicine},
  volume = {378},
  number = {22},
  eprint = {29658845},
  eprinttype = {pmid},
  pages = {2093--2104},
  publisher = {Massachusetts Medical Society},
  issn = {0028-4793},
  doi = {10.1056/NEJMoa1801946},
  url = {https://doi.org/10.1056/NEJMoa1801946},
  urldate = {2024-03-01},
  abstract = {Tumor Mutational Burden and Immunotherapy in Lung Cancer Among patients with non–small-cell lung cancer and tumors containing at least 10 mutations per megabase, the 1-year progression-free survival rate was 43\% with nivolumab plus ipilimumab versus 13\% with chemotherapy. This result was independent of PD-L1 expression level.},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\hellmann2018.pdf}
}

@article{hemerik2018,
  title = {Exact Testing with Random Permutations},
  author = {Hemerik, Jesse and Goeman, Jelle},
  date = {2018-12-01},
  journaltitle = {TEST},
  shortjournal = {TEST},
  volume = {27},
  number = {4},
  pages = {811--825},
  issn = {1863-8260},
  doi = {10.1007/s11749-017-0571-1},
  url = {https://doi.org/10.1007/s11749-017-0571-1},
  urldate = {2023-10-17},
  abstract = {When permutation methods are used in practice, often a limited number of random permutations are used to decrease the computational burden. However, most theoretical literature assumes that the whole permutation group is used, and methods based on random permutations tend to be seen as approximate. There exists a very limited amount of literature on exact testing with random permutations, and only recently a thorough proof of exactness was given. In this paper, we provide an alternative proof, viewing the test as a “conditional Monte Carlo test” as it has been called in the literature. We also provide extensions of the result. Importantly, our results can be used to prove properties of various multiple testing procedures based on random permutations.},
  langid = {english},
  keywords = {62G09,62G10,Nonparametric test,notion,Permutation test,Resampling},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\hemerik2018.pdf}
}

@article{hernan2010,
  title = {The {{Hazards}} of {{Hazard Ratios}}},
  author = {Hernán, Miguel A.},
  date = {2010-01},
  journaltitle = {Epidemiology (Cambridge, Mass.)},
  shortjournal = {Epidemiology},
  volume = {21},
  number = {1},
  eprint = {20010207},
  eprinttype = {pmid},
  pages = {13--15},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e3181c1ea43},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3653612/},
  urldate = {2023-08-30},
  pmcid = {PMC3653612},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\hernan2010.pdf}
}

@article{hida2018,
  title = {Design and Analysis of a 3-Arm Noninferiority Trial with a Prespecified Margin for the Hazard Ratio},
  author = {Hida, Eisuke and Tango, Toshiro},
  date = {2018-07-09},
  journaltitle = {Pharmaceutical Statistics},
  shortjournal = {Pharmaceutical Statistics},
  issn = {15391604},
  doi = {10.1002/pst.1875},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/pst.1875},
  urldate = {2023-08-21},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\hida2018.pdf}
}

@article{horiguchi2018,
  title = {A Flexible and Coherent Test/Estimation Procedure Based on Restricted Mean Survival Times for Censored Time-to-Event Data in Randomized Clinical Trials},
  author = {Horiguchi, Miki and Cronin, Angel M. and Takeuchi, Masahiro and Uno, Hajime},
  date = {2018},
  journaltitle = {Statistics in Medicine},
  volume = {37},
  number = {15},
  pages = {2307--2320},
  issn = {1097-0258},
  doi = {10.1002/sim.7661},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7661},
  urldate = {2023-09-13},
  abstract = {In randomized clinical trials where time-to-event is the primary outcome, almost routinely, the logrank test is prespecified as the primary test and the hazard ratio is used to quantify treatment effect. If the ratio of 2 hazard functions is not constant, the logrank test is not optimal and the interpretation of hazard ratio is not obvious. When such a nonproportional hazards case is expected at the design stage, the conventional practice is to prespecify another member of weighted logrank tests, eg, Peto-Prentice-Wilcoxon test. Alternatively, one may specify a robust test as the primary test, which can capture various patterns of difference between 2 event time distributions. However, most of those tests do not have companion procedures to quantify the treatment difference, and investigators have fallen back on reporting treatment effect estimates not associated with the primary test. Such incoherence in the “test/estimation” procedure may potentially mislead clinicians/patients who have to balance risk-benefit for treatment decision. To address this, we propose a flexible and coherent test/estimation procedure based on restricted mean survival time, where the truncation time τ is selected data dependently. The proposed procedure is composed of a prespecified test and an estimation of corresponding robust and interpretable quantitative treatment effect. The utility of the new procedure is demonstrated by numerical studies based on 2 randomized cancer clinical trials; the test is dramatically more powerful than the logrank, Wilcoxon tests, and the restricted mean survival time–based test with a fixed τ, for the patterns of difference seen in these cancer clinical trials.},
  langid = {english},
  keywords = {logrank test,notion,perturbation resampling method,proportional hazards,robust tests,test/estimation coherency},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\horiguchi2018.pdf;C\:\\Users\\david\\Zotero\\storage\\TK9ERIB2\\sim.html}
}

@article{horiguchi2019,
  title = {How {{Do}} the {{Accrual Pattern}} and {{Follow}}‐{{Up Duration Affect}} the {{Hazard Ratio Estimate When}} the {{Proportional Hazards Assumption Is Violated}}?},
  author = {Horiguchi, Miki and Hassett, Michael J. and Uno, Hajime},
  date = {2019-07},
  journaltitle = {The Oncologist},
  shortjournal = {Oncologist},
  volume = {24},
  number = {7},
  eprint = {30201741},
  eprinttype = {pmid},
  pages = {867--871},
  issn = {1083-7159},
  doi = {10.1634/theoncologist.2018-0141},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6656438/},
  urldate = {2023-09-14},
  abstract = {In randomized clinical trials, the magnitude of the treatment effect is often reported using the hazard ratio (HR) even when the proportional hazards (PH) assumption is not met. Conducting numerical studies, this commentary illustrates how/why the HR estimate via the standard Cox's procedure is difficult to interpret even as an “average” treatment effect for non‐PH cases.},
  pmcid = {PMC6656438},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\horiguchi22.pdf}
}

@article{horiguchi2020,
  title = {Empirical Power Comparison of Statistical Tests in Contemporary Phase {{III}} Randomized Controlled Trials with Time-to-Event Outcomes in Oncology},
  author = {Horiguchi, Miki and Hassett, Michael J and Uno, Hajime},
  date = {2020-12-01},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {17},
  number = {6},
  pages = {597--606},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774520940256},
  url = {https://doi.org/10.1177/1740774520940256},
  urldate = {2023-09-08},
  abstract = {Background: More than 95\% of recent cancer randomized controlled trials used the log-rank test to detect a treatment difference making it the predominant tool for comparing two survival functions. As with other tests, the log-rank test has both advantages and disadvantages. One advantage is that it offers the highest power against proportional hazards differences, which may be a major reason why alternative methods have rarely been employed in practice. The performance of statistical tests has traditionally been investigated both theoretically and numerically for several patterns of difference between two survival functions. However, to the best of our knowledge, there has been no attempt to compare the performance of various statistical tests using empirical data from past oncology randomized controlled trials. So, it is unknown whether the log-rank test offers a meaningful power advantage over alternative testing methods in contemporary cancer randomized controlled trials. Focusing on recently reported phase III cancer randomized controlled trials, we assessed whether the log-rank test gave meaningfully greater power when compared with five alternative testing methods: generalized Wilcoxon, test based on maximum of test statistics from multiple weighted log-rank tests, difference in t-year event rate, and difference in restricted mean survival time with fixed and adaptive  τ τ . Methods: Using manuscripts from cancer randomized controlled trials recently published in high-tier clinical journals, we reconstructed patient-level data for overall survival (69 trials) and progression-free survival (54 trials). For each trial endpoint, we estimated the empirical power of each test. Empirical power was measured as the proportion of trials for which a test would have identified a significant result (p value\,{$<$}\,.05). Results: For overall survival, t-year event rate offered the lowest (30.4\%) empirical power and restricted mean survival time with fixed  τ τ  offered the highest (43.5\%). The empirical power of the other types of tests was almost identical (36.2\%–37.7\%). For progression-free survival, the tests we investigated offered numerically equivalent empirical power (55.6\%–61.1\%). No single test consistently outperformed any other test. Conclusion: The empirical power assessment with the past cancer randomized controlled trials provided new insights on the performance of statistical tests. Although the log-rank test has been used in almost all trials, our study suggests that the log-rank test is not the only option from an empirical power perspective. Near universal use of the log-rank test is not supported by a meaningful difference in empirical power. Clinical trial investigators could consider alternative methods, beyond the log-rank test, for their primary analysis when designing a cancer randomized controlled trial. Factors other than power (e.g. interpretability of the estimated treatment effect) should garner greater consideration when selecting statistical tests for cancer randomized controlled trials.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\horiguchi2020.pdf}
}

@article{horiguchi2020a,
  title = {On Permutation Tests for Comparing Restricted Mean Survival Time with Small Sample from Randomized Trials},
  author = {Horiguchi, Miki and Uno, Hajime},
  date = {2020},
  journaltitle = {Statistics in Medicine},
  volume = {39},
  number = {20},
  pages = {2655--2670},
  issn = {1097-0258},
  doi = {10.1002/sim.8565},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8565},
  urldate = {2023-09-13},
  abstract = {Between-group comparison based on the restricted mean survival time (RMST) is getting attention as an alternative to the conventional logrank/hazard ratio approach for time-to-event outcomes in randomized controlled trials (RCTs). The validity of the commonly used nonparametric inference procedure for RMST has been well supported by large sample theories. However, we sometimes encounter cases with a small sample size in practice, where we cannot rely on the large sample properties. Generally, the permutation approach can be useful to handle these situations in RCTs. However, a numerical issue arises when implementing permutation tests for difference or ratio of RMST from two groups. In this article, we discuss the numerical issue and consider six permutation methods for comparing survival time distributions between two groups using RMST in RCTs setting. We conducted extensive numerical studies and assessed type I error rates of these methods. Our numerical studies demonstrated that the inflation of the type I error rate of the asymptotic methods is not negligible when sample size is small, and that all of the six permutation methods are workable solutions. Although some permutation methods became a little conservative, no remarkable inflation of the type I error rates were observed. We recommend using permutation tests instead of the asymptotic tests, especially when the sample size is less than 50 per arm.},
  langid = {english},
  keywords = {notion,randomization test,randomized clinical trials,survival analysis,time-to-event outcomes,type I error rate},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\horiguchi2020a.pdf;C\:\\Users\\david\\Zotero\\storage\\XAERQXAH\\sim.html}
}

@article{horiguchi2023,
  title = {On Assessing Survival Benefit of Immunotherapy Using Long-Term Restricted Mean Survival Time},
  author = {Horiguchi, Miki and Tian, Lu and Uno, Hajime},
  date = {2023},
  journaltitle = {Statistics in Medicine},
  volume = {42},
  number = {8},
  pages = {1139--1155},
  issn = {1097-0258},
  doi = {10.1002/sim.9662},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9662},
  urldate = {2023-09-13},
  abstract = {The pattern of the difference between two survival curves we often observe in randomized clinical trials for evaluating immunotherapy is not proportional hazards; the treatment effect typically appears several months after the initiation of the treatment (ie, delayed difference pattern). The commonly used logrank test and hazard ratio estimation approach will be suboptimal concerning testing and estimation for those trials. The long-term restricted mean survival time (LT-RMST) approach is a promising alternative for detecting the treatment effect that potentially appears later in the study. A challenge in employing the LT-RMST approach is that it must specify a lower end of the time window in addition to a truncation time point that the RMST requires. There are several investigations and suggestions regarding the choice of the truncation time point for the RMST. However, little has been investigated to address the choice of the lower end of the time window. In this paper, we propose a flexible LT-RMST-based test/estimation approach that does not require users to specify a lower end of the time window. Numerical studies demonstrated that the potential power loss by adopting this flexibility was minimal, compared to the standard LT-RMST approach using a prespecified lower end of the time window. The proposed method is flexible and can offer higher power than the RMST-based approach when the delayed treatment effect is expected. Also, it provides a robust estimate of the magnitude of the treatment effect and its confidence interval that corresponds to the test result.},
  langid = {english},
  keywords = {delayed difference,hazard ratio,non-proportional hazards,notion,versatile test,weighted logrank test},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\horiguchi2023.pdf;C\:\\Users\\david\\Zotero\\storage\\5HEP7UAU\\sim.html}
}

@article{hothorn2020,
  title = {Most {{Likely Transformations}}: {{The}} Mlt {{Package}}},
  shorttitle = {Most {{Likely Transformations}}},
  author = {Hothorn, Torsten},
  date = {2020-02-18},
  journaltitle = {Journal of Statistical Software},
  volume = {92},
  pages = {1--68},
  issn = {1548-7660},
  doi = {10.18637/jss.v092.i01},
  url = {https://doi.org/10.18637/jss.v092.i01},
  urldate = {2023-11-02},
  abstract = {The mlt package implements maximum likelihood estimation in the class of conditional transformation models. Based on a suitable explicit parameterization of the unconditional or conditional transformation function using infrastructure from package basefun, we show how one can define, estimate, and compare a cascade of increasingly complex transformation models in the maximum likelihood framework. Models for the unconditional or conditional distribution function of any univariate response variable are set-up and estimated in the same computational framework simply by choosing an appropriate transformation function and parameterization thereof. As it is computationally cheap to evaluate the distribution function, models can be estimated by maximization of the exact likelihood, especially in the presence of random censoring or truncation. The relatively dense high-level implementation in the R system for statistical computing allows generalization of many established implementations of linear transformation models, such as the Cox model or other parametric models for the analysis of survival or ordered categorical data, to the more complex situations illustrated in this paper.},
  langid = {english},
  keywords = {censoring,conditional distribution function,conditional quantile function,distribution regression,notion,transformation analysis,transformation model,truncation},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\hothorn2020.pdf}
}

@article{huang2018,
  title = {Comparison of the Restricted Mean Survival Time with the Hazard Ratio in Superiority Trials with a Time-to-Event End Point},
  author = {Huang, Bo and Kuan, Pei-Fen},
  date = {2018-05},
  journaltitle = {Pharmaceutical Statistics},
  shortjournal = {Pharm Stat},
  volume = {17},
  number = {3},
  eprint = {29282880},
  eprinttype = {pmid},
  pages = {202--213},
  issn = {1539-1612},
  doi = {10.1002/pst.1846},
  abstract = {With the emergence of novel therapies exhibiting distinct mechanisms of action compared to traditional treatments, departure from the proportional hazard (PH) assumption in clinical trials with a time-to-event end point is increasingly common. In these situations, the hazard ratio may not be a valid statistical measurement of treatment effect, and the log-rank test may no longer be the most powerful statistical test. The restricted mean survival time (RMST) is an alternative robust and clinically interpretable summary measure that does not rely on the PH assumption. We conduct extensive simulations to evaluate the performance and operating characteristics of the RMST-based inference and against the hazard ratio-based inference, under various scenarios and design parameter setups. The log-rank test is generally a powerful test when there is evident separation favoring 1 treatment arm at most of the time points across the Kaplan-Meier survival curves, but the performance of the RMST test is similar. Under non-PH scenarios where late separation of survival curves is observed, the RMST-based test has better performance than the log-rank test when the truncation time is reasonably close to the tail of the observed curves. Furthermore, when flat survival tail (or low event rate) in the experimental arm is expected, selecting the minimum of the maximum observed event time as the truncation timepoint for the RMST is not recommended. In addition, we recommend the inclusion of analysis based on the RMST curve over the truncation time in clinical settings where there is suspicion of substantial departure from the PH assumption.},
  langid = {english},
  keywords = {Humans,Kaplan-Meier Estimate,log-rank test,Neoplasms,notion,proportional hazard,Proportional Hazards Models,Randomized Controlled Trials as Topic,restricted mean survival time,Survival Rate,time to event},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\huang2018.pdf}
}

@article{huang2023,
  title = {Relative Likelihood Ratios for Neutral Comparisons of Statistical Tests in Simulation Studies},
  author = {Huang, Qiuxi and Trinquart, Ludovic},
  date = {2023},
  journaltitle = {Biometrical Journal},
  volume = {n/a},
  number = {n/a},
  pages = {2200102},
  issn = {1521-4036},
  doi = {10.1002/bimj.202200102},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.202200102},
  urldate = {2023-09-20},
  abstract = {When comparing the performance of two or more competing tests, simulation studies commonly focus on statistical power. However, if the size of the tests being compared are either different from one another or from the nominal size, comparing tests based on power alone may be misleading. By analogy with diagnostic accuracy studies, we introduce relative positive and negative likelihood ratios to factor in both power and size in the comparison of multiple tests. We derive sample size formulas for a comparative simulation study. As an example, we compared the performance of six statistical tests for small-study effects in meta-analyses of randomized controlled trials: Begg's rank correlation, Egger's regression, Schwarzer's method for sparse data, the trim-and-fill method, the arcsine-Thompson test, and Lin and Chu's combined test. We illustrate that comparing power alone, or power adjusted or penalized for size, can be misleading, and how the proposed likelihood ratio approach enables accurate comparison of the trade-off between power and size between competing tests.},
  langid = {english},
  keywords = {meta-analysis,notion,publication bias,simulation studies,small-study effects,statistical power,type I error},
  file = {C:\Users\david\Zotero\storage\NXF3EL46\bimj.html}
}

@article{janssen2003,
  title = {How {{Do Bootstrap}} and {{Permutation Tests Work}}?},
  author = {Janssen, Arnold and Pauls, Thorsten},
  date = {2003},
  journaltitle = {The Annals of Statistics},
  volume = {31},
  number = {3},
  eprint = {3448420},
  eprinttype = {jstor},
  pages = {768--806},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364},
  url = {https://www.jstor.org/stable/3448420},
  urldate = {2023-10-09},
  abstract = {Resampling methods are frequently used in practice to adjust critical values of nonparametric tests. In the present paper a comprehensive and unified approach for the conditional and unconditional analysis of linear resampling statistics is presented. Under fairly mild assumptions we prove tightness and an asymptotic series representation for their weak accumulation points. From this series it becomes clear which part of the resampling statistic is responsible for asymptotic normality. The results leads to a discussion of the asymptotic correctness of resampling methods as well as their applications in testing hypotheses. They are conditionally correct iff a central limit theorem holds for the original test statistic. We prove unconditional correctness iff the central limit theorem holds or when symmetric random variables are resampled by a scheme of asymptotically random signs. Special cases are the m(n) out of k(n) bootstrap, the weighted bootstrap, the wild bootstrap and all kinds of permutation statistics. The program is carried out for convergent partial sums of rowwise independent infinitesimal triangular arrays in detail. These results are used to compare power functions of conditional resampling tests and their unconditional counterparts. The proof uses the method of random scores for permutation type statistics.},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\janssen2003.pdf}
}

@article{jimenez,
  title = {Quantifying Treatment Differences in Confirmatory Trials under Non-Proportional Hazards},
  author = {Jiménez, José L.},
  journaltitle = {Journal of Applied Statistics},
  shortjournal = {J Appl Stat},
  volume = {49},
  number = {2},
  eprint = {35707213},
  eprinttype = {pmid},
  pages = {466--484},
  issn = {0266-4763},
  doi = {10.1080/02664763.2020.1815673},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9196085/},
  urldate = {2023-09-06},
  abstract = {Proportional hazards are a common assumption when designing confirmatory clinical trials in oncology. With the emergence of immunotherapy and novel targeted therapies, departure from the proportional hazard assumption is not rare in nowadays clinical research. Under non-proportional hazards, the hazard ratio does not have a straightforward clinical interpretation, and the log-rank test is no longer the most powerful statistical test even though it is still valid. Nevertheless, the log-rank test and the hazard ratio are still the primary analysis tools, and traditional approaches such as sample size increase are still proposed to account for the impact of non-proportional hazards. The weighed log-rank test and the test based on the restricted mean survival time (RMST) are receiving a lot of attention as a potential alternative to the log-rank test. We conduct a simulation study comparing the performance and operating characteristics of the log-rank test, the weighted log-rank test and the test based on the RMST, including a treatment effect estimation, under different non-proportional hazards patterns. Results show that, under non-proportional hazards, the hazard ratio and weighted hazard ratio have no straightforward clinical interpretation whereas the RMST ratio can be interpreted regardless of the proportional hazards assumption. In terms of power, the RMST achieves a similar performance when compared to the log-rank test.},
  pmcid = {PMC9196085},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\jimenez.pdf}
}

@article{kahan2014,
  title = {The Risks and Rewards of Covariate Adjustment in Randomized Trials: An Assessment of 12 Outcomes from 8 Studies},
  shorttitle = {The Risks and Rewards of Covariate Adjustment in Randomized Trials},
  author = {Kahan, Brennan C. and Jairath, Vipul and Doré, Caroline J. and Morris, Tim P.},
  date = {2014-04-23},
  journaltitle = {Trials},
  shortjournal = {Trials},
  volume = {15},
  number = {1},
  pages = {139},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-15-139},
  url = {https://doi.org/10.1186/1745-6215-15-139},
  urldate = {2024-03-01},
  abstract = {Adjustment for prognostic covariates can lead to increased power in the analysis of randomized trials. However, adjusted analyses are not often performed in practice.},
  keywords = {Adjusted analysis,clinical trial,covariate adjustment,notion,power,randomized controlled trial,regression},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\kahan2014.pdf;C\:\\Users\\david\\Zotero\\storage\\TH3IYY3D\\1745-6215-15-139.html}
}

@article{kalbfleisch,
  title = {Estimation of the Average Hazard Ratio},
  author = {Kalbfleisch, John D},
  abstract = {Simple estimators of the average hazard ratio are obtained for the two sample problem with censored failure time data. These estimators are compared with estimators arising out of the partial likelihood within the proportional hazards class. Efficiency results are found to be generally quite favourable provided the hazard ratio is not too large. Some comments are made on extensions to the k sample problem.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\kalbfleisch.pdf}
}

@book{kalbfleisch2002,
  title = {The Statistical Analysis of Failure Time Data},
  author = {Kalbfleisch, J. D. and Prentice, Ross L.},
  date = {2002},
  series = {Wiley Series in Probability and Statistics},
  edition = {2nd ed},
  publisher = {J. Wiley},
  location = {Hoboken, N.J},
  isbn = {978-0-471-36357-6},
  langid = {english},
  pagetotal = {439},
  keywords = {Failure time data analysis,notion,Regression analysis,Survival analysis (Biometry)},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\kalbfleisch2002.pdf}
}

@article{kaplan1958,
  title = {Nonparametric {{Estimation}} from {{Incomplete Observations}}},
  author = {Kaplan, E. L. and Meier, Paul},
  date = {1958},
  journaltitle = {Journal of the American Statistical Association},
  volume = {53},
  number = {282},
  eprint = {2281868},
  eprinttype = {jstor},
  pages = {457--481},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0162-1459},
  doi = {10.2307/2281868},
  url = {https://www.jstor.org/stable/2281868},
  urldate = {2024-02-14},
  abstract = {In lifetesting, medical follow-up, and other fields the observation of the time of occurrence of the event of interest (called a death) may be prevented for some of the items of the sample by the previous occurrence of some other event (called a loss). Losses may be either accidental or controlled, the latter resulting from a decision to terminate certain observations. In either case it is usually assumed in this paper that the lifetime (age at death) is independent of the potential loss time; in practice this assumption deserves careful scrutiny. Despite the resulting incompleteness of the data, it is desired to estimate the proportion P(t) of items in the population whose lifetimes would exceed t (in the absence of such losses), without making any assumption about the form of the function P(t). The observation for each item of a suitable initial event, marking the beginning of its lifetime, is presupposed. For random samples of size N the product-limit (PL) estimate can be defined as follows: List and label the N observed lifetimes (whether to death or loss) in order of increasing magnitude, so that one has 0 ≤ t\textsubscript{1}' ≤ t\textsubscript{2}' ≤ ⋯ ≤ t\textsubscript{N}'. Then {$<$}tex-math{$>\$\backslash$}hat\{P\}(t) = \textbackslash prod\_r \textbackslash lbrack(N - r)/(N - r + 1)\textbackslash rbrack\${$<$}/tex-math{$>$}, where r assumes those values for which t\textsubscript{r}' ≤ t and for which t\textsubscript{r}' measures the time to death. This estimate is the distribution, unrestricted as to form, which maximizes the likelihood of the observations. Other estimates that are discussed are the actuarial estimates (which are also products, but with the number of factors usually reduced by grouping); and reduced-sample (RS) estimates, which require that losses not be accidental, so that the limits of observation (potential loss times) are known even for those items whose deaths are observed. When no losses occur at ages less than t, the estimate of P(t) in all cases reduces to the usual binomial estimate, namely, the observed proportion of survivors.},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\kaplan1958.pdf}
}

@article{karrison2018,
  title = {Restricted Mean Survival Time: {{Does}} Covariate Adjustment Improve Precision in Randomized Clinical Trials?},
  shorttitle = {Restricted Mean Survival Time},
  author = {Karrison, Theodore and Kocherginsky, Masha},
  date = {2018-04-01},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {15},
  number = {2},
  pages = {178--188},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774518759281},
  url = {https://doi.org/10.1177/1740774518759281},
  urldate = {2023-09-08},
  abstract = {Background: Restricted mean survival time is a measure of average survival time up to a specified time point. There has been an increased interest in using restricted mean survival time to compare treatment arms in randomized clinical trials because such comparisons do not rely on proportional hazards or other assumptions about the nature of the relationship between survival curves. Methods: This article addresses the question of whether covariate adjustment in randomized clinical trials that compare restricted mean survival times improves precision of the estimated treatment effect (difference in restricted mean survival times between treatment arms). Although precision generally increases in linear models when prognostic covariates are added, this is not necessarily the case in non-linear models. For example, in logistic and Cox regression, the standard error of the estimated treatment effect does not decrease when prognostic covariates are added, although the situation is complicated in those settings because the estimand changes as well. Because estimation of restricted mean survival time in the manner described in this article is also based on a model that is non-linear in the covariates, we investigate whether the comparison of restricted mean survival times with adjustment for covariates leads to a reduction in the standard error of the estimated treatment effect relative to the unadjusted estimator or whether covariate adjustment provides no improvement in precision. Chen and Tsiatis suggest that precision will increase if covariates are chosen judiciously. We present results of simulation studies that compare unadjusted versus adjusted comparisons of restricted mean survival time between treatment arms in randomized clinical trials. Results: We find that for comparison of restricted means in a randomized clinical trial, adjusting for covariates that are associated with survival increases precision and therefore statistical power, relative to the unadjusted estimator. Omitting important covariates results in less precision but estimates remain unbiased. Conclusion: When comparing restricted means in a randomized clinical trial, adjusting for prognostic covariates can improve precision and increase power.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\karrison2018.pdf}
}

@article{klein2008,
  title = {{{SAS}} and {{R Functions}} to {{Compute Pseudo-values}} for {{Censored Data Regression}}},
  author = {Klein, John P and Gerster, Mette and Andersen, Per Kragh and Tarima, Sergey and Perme, Maja Pohar},
  date = {2008-03},
  journaltitle = {Computer methods and programs in biomedicine},
  shortjournal = {Comput Methods Programs Biomed},
  volume = {89},
  number = {3},
  eprint = {18199521},
  eprinttype = {pmid},
  pages = {289--300},
  issn = {0169-2607},
  doi = {10.1016/j.cmpb.2007.11.017},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2533132/},
  urldate = {2023-09-12},
  abstract = {Recently, in a series of papers, a method based on pseudo-values has been proposed for direct regression modeling of the survival function, the restricted mean and cumulative incidence function with right censored data. The models, once the pseudo-values have been computed, can be fit using standard generalized estimating equation software. Here we present SAS macros and R functions to compute these pseudo-values. We illustrate the use of these routines and show how to obtain regression estimates for a study of bone marrow transplant patients.},
  pmcid = {PMC2533132},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\klein2008.pdf}
}

@online{klinglmuller2023,
  title = {A Neutral Comparison of Statistical Methods for Time-to-Event Analyses under Non-Proportional Hazards},
  author = {Klinglmüller, Florian and Fellinger, Tobias and König, Franz and Friede, Tim and Hooker, Andrew C. and Heinzl, Harald and Mittlböck, Martina and Brugger, Jonas and Bardo, Maximilian and Huber, Cynthia and Benda, Norbert and Posch, Martin and Ristl, Robin},
  date = {2023-10-09},
  eprint = {2310.05622},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2310.05622},
  url = {http://arxiv.org/abs/2310.05622},
  urldate = {2023-10-19},
  abstract = {While well-established methods for time-to-event data are available when the proportional hazards assumption holds, there is no consensus on the best inferential approach under non-proportional hazards (NPH). However, a wide range of parametric and non-parametric methods for testing and estimation in this scenario have been proposed. To provide recommendations on the statistical analysis of clinical trials where non proportional hazards are expected, we conducted a comprehensive simulation study under different scenarios of non-proportional hazards, including delayed onset of treatment effect, crossing hazard curves, subgroups with different treatment effect and changing hazards after disease progression. We assessed type I error rate control, power and confidence interval coverage, where applicable, for a wide range of methods including weighted log-rank tests, the MaxCombo test, summary measures such as the restricted mean survival time (RMST), average hazard ratios, and milestone survival probabilities as well as accelerated failure time regression models. We found a trade-off between interpretability and power when choosing an analysis strategy under NPH scenarios. While analysis methods based on weighted logrank tests typically were favorable in terms of power, they do not provide an easily interpretable treatment effect estimate. Also, depending on the weight function, they test a narrow null hypothesis of equal hazard functions and rejection of this null hypothesis may not allow for a direct conclusion of treatment benefit in terms of the survival function. In contrast, non-parametric procedures based on well interpretable measures as the RMST difference had lower power in most scenarios. Model based methods based on specific survival distributions had larger power, however often gave biased estimates and lower than nominal confidence interval coverage.},
  pubstate = {preprint},
  keywords = {notion,Statistics - Methodology},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\klinglmuller2023.pdf;C\:\\Users\\david\\Zotero\\storage\\PLLSCE2L\\2310.html}
}

@article{kloecker2020,
  title = {Uses and {{Limitations}} of the {{Restricted Mean Survival Time}}: {{Illustrative Examples From Cardiovascular Outcomes}} and {{Mortality Trials}} in {{Type}} 2 {{Diabetes}}},
  shorttitle = {Uses and {{Limitations}} of the {{Restricted Mean Survival Time}}},
  author = {Kloecker, David E. and Davies, Melanie J. and Khunti, Kamlesh and Zaccardi, Francesco},
  date = {2020-04-21},
  journaltitle = {Annals of Internal Medicine},
  shortjournal = {Ann Intern Med},
  volume = {172},
  number = {8},
  pages = {541--552},
  publisher = {American College of Physicians},
  issn = {0003-4819},
  doi = {10.7326/M19-3286},
  url = {https://www.acpjournals.org/doi/abs/10.7326/M19-3286},
  urldate = {2023-09-06},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\kloecker2020.pdf}
}

@article{kombrink2013,
  title = {Design and Semiparametric Analysis of Non-Inferiority Trials with Active and Placebo Control for Censored Time-to-Event Data},
  author = {Kombrink, Karola and Munk, Axel and Friede, Tim},
  date = {2013-08-15},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Statist. Med.},
  volume = {32},
  number = {18},
  pages = {3055--3066},
  issn = {02776715},
  doi = {10.1002/sim.5769},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.5769},
  urldate = {2023-08-21},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\kombrink2013.pdf}
}

@article{latimer2019,
  title = {Causal Inference for Long-Term Survival in Randomised Trials with Treatment Switching: {{Should}} Re-Censoring Be Applied When Estimating Counterfactual Survival Times?},
  shorttitle = {Causal Inference for Long-Term Survival in Randomised Trials with Treatment Switching},
  author = {family=Latimer, given=NR, given-i=NR and family=White, given=IR, given-i=IR and family=Abrams, given=KR, given-i=KR and Siebert, U},
  date = {2019-08-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {28},
  number = {8},
  pages = {2475--2493},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280218780856},
  url = {https://doi.org/10.1177/0962280218780856},
  urldate = {2023-09-06},
  abstract = {Treatment switching often has a crucial impact on estimates of effectiveness and cost-effectiveness of new oncology treatments. Rank preserving structural failure time models (RPSFTM) and two-stage estimation (TSE) methods estimate ‘counterfactual’ (i.e. had there been no switching) survival times and incorporate re-censoring to guard against informative censoring in the counterfactual dataset. However, re-censoring causes a loss of longer term survival information which is problematic when estimates of long-term survival effects are required, as is often the case for health technology assessment decision making. We present a simulation study designed to investigate applications of the RPSFTM and TSE with and without re-censoring, to determine whether re-censoring should always be recommended within adjustment analyses. We investigate a context where switching is from the control group onto the experimental treatment in scenarios with varying switch proportions, treatment effect sizes, treatment effect changes over time, survival function shapes, disease severity and switcher prognosis. Methods were assessed according to their estimation of control group restricted mean survival that would be observed in the absence of switching, up to the end of trial follow-up. We found that analyses which re-censored usually produced negative bias (i.e. underestimating control group restricted mean survival and overestimating the treatment effect), whereas analyses that did not re-censor consistently produced positive bias which was often smaller in magnitude than the bias associated with re-censored analyses, particularly when the treatment effect was high and the switching proportion was low. The RPSFTM with re-censoring generally resulted in increased bias compared to the other methods. We believe that analyses should be conducted with and without re-censoring, as this may provide decision-makers with useful information on where the true treatment effect is likely to lie. Incorporating re-censoring should not always represent the default approach when the objective is to estimate long-term survival times and treatment effects.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\latimer2019.pdf}
}

@article{lawrence2019,
  title = {Difference in {{Restricted Mean Survival Time}}: {{Small Sample Distribution}} and {{Asymptotic Relative Efficiency}}},
  shorttitle = {Difference in {{Restricted Mean Survival Time}}},
  author = {Lawrence, John and Qiu, Junshan and Bai, Steven and Hung, H. M. James},
  date = {2019-01-02},
  journaltitle = {Statistics in Biopharmaceutical Research},
  volume = {11},
  number = {1},
  pages = {61--66},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/19466315.2018.1527249},
  url = {https://doi.org/10.1080/19466315.2018.1527249},
  urldate = {2023-09-11},
  abstract = {The difference between two arms in the restricted mean survival time is an alternative to the hazard ratio. It provides a more easily understood measure of the treatment effect of an intervention in a controlled clinical trial with a time to event endpoint. In this article, we consider the distribution of the estimate for small or moderate sample sizes or when the event rate is low and compare the asymptotic relative efficiency (ARE) and relative efficiency for practical sample sizes of the test to the standard logrank test for Weibull parent distributions. We find that for small sample sizes, the distribution can depart markedly from the normal distribution. We propose a method to approximate the correct null distribution using the Cornish–Fisher expansion and provide an example. For Weibull parent distributions, we consider two scenarios with either proportional hazards or nonproportional hazards where the hazard functions and survival distribution curves crossover. The ARE is compared for these two scenarios.},
  keywords = {Active controlled trial,Cornish–Fisher expansion,Logrank test,Noninferiority testing,notion,Proportional hazards model,Weibull distribution},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\lawrence2019.pdf}
}

@article{lee2007,
  title = {On the Versatility of the Combination of the Weighted Log-Rank Statistics},
  author = {Lee, Seung-Hwan},
  date = {2007-08-15},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {51},
  number = {12},
  pages = {6557--6564},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2007.03.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0167947307001077},
  urldate = {2023-08-29},
  abstract = {In many applied situations, it is difficult to specify in advance the types of survival differences that may exist between two groups. Therefore, it is tempting to use some tests that emphasize these differences, but are sensitive to a wide range of the survival differences. In this paper such versatile tests are considered, whose procedures are based on the simultaneous use of the weighted log-rank statistics that are asymptotically normal under the null hypothesis of no difference between two groups. Simulations are performed to examine power of the tests in small and moderate sample sizes when the data are uncensored to heavily censored. Implementation of the procedures are discussed in a real data example for illustration.},
  keywords = {notion,Survival distribution,Versatile test,Weighted log-rank statistics},
  file = {C:\Users\david\Zotero\storage\KH4GXU8V\S0167947307001077.html}
}

@article{leton2001,
  title = {Equivalence {{Between Score}} and {{Weighted Tests}} for {{Survival Curves}}},
  author = {Letón, E. and Zuluaga, P.},
  date = {2001-03-31},
  journaltitle = {Communications in Statistics - Theory and Methods},
  volume = {30},
  number = {4},
  pages = {591--608},
  publisher = {Taylor \& Francis},
  issn = {0361-0926},
  doi = {10.1081/STA-100002138},
  url = {https://doi.org/10.1081/STA-100002138},
  urldate = {2023-08-30},
  abstract = {In this paper we describe 10 expressions of score and weighted tests, in such a way that the numerators and the denominators are completely specified, including always the possibility of tied observations. We establish the equivalence between score and weighted tests in the general setting of ties. Based upon this equivalence we enunciate two new tests, which complete the jigsaw of the classification of these non-parametric tests in Survival Analysis.},
  keywords = {Censored data,Comparison of survival functions,Non-parametric tests in survival analysis,notion,Regularity conditions,Tied observations}
}

@article{li2022,
  title = {Choosing and Changing the Analysis Scale in Non-Inferiority Trials with a Binary Outcome},
  author = {Li, Zhong and Quartagno, Matteo and Böhringer, Stefan and family=Geloven, given=Nan, prefix=van, useprefix=true},
  date = {2022-02-01},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {19},
  number = {1},
  pages = {14--21},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/17407745211053790},
  url = {https://doi.org/10.1177/17407745211053790},
  urldate = {2023-09-14},
  abstract = {Background The size of the margin strongly influences the required sample size in non-inferiority and equivalence trials. What is sometimes ignored, however, is that for trials with binary outcomes, the scale of the margin – risk difference, risk ratio or odds ratio – also has a large impact on power and thus on sample size requirement. When considering several scales at the design stage of a trial, these sample size consequences should be taken into account. Sometimes, changing the scale may be needed at a later stage of a trial, for example, when the event proportion in the control arm turns out different from expected. Also after completion of a trial, a switch to another scale is sometimes made, for example, when using a regression model in a secondary analysis or when combining study results in a meta-analysis that requires unifying scales. The exact consequences of such switches are currently unknown. Methods and Results This article first outlines sample size consequences for different choices of analysis scale at the design stage of a trial. We add a new result on sample size requirement comparing the risk difference scale with the risk ratio scale. Then, we study two different approaches to changing the analysis scale after the trial has commenced: (1) mapping the original non-inferiority margin using the event proportion in the control arm that was anticipated at the design stage or (2) mapping the original non-inferiority margin using the observed event proportion in the control arm. We use simulations to illustrate consequences on type I and type II error rates. Methods are illustrated on the INES trial, a non-inferiority trial that compared single birth rates in subfertile couples after different fertility treatments. Our results demonstrate large differences in required sample size when choosing between risk difference, risk ratio and odds ratio scales at the design stage of non-inferiority trials. In some cases, the sample size requirement is twice as large on one scale compared with another. Changing the scale after commencing the trial using anticipated proportions mainly impacts type II error rate, whereas switching using observed proportions is not advised due to not maintaining type I error rate. Differences were more pronounced with larger margins. Conclusions Trialists should be aware that the analysis scale can have large impact on type I and type II error rates in non-inferiority trials.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\li2022.pdf}
}

@article{liao2020,
  title = {Dynamic {{RMST}} Curves for Survival Analysis in Clinical Trials},
  author = {Liao, Jason J. Z. and Liu, G. Frank and Wu, Wen-Chi},
  date = {2020-08-27},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {218},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-01098-5},
  url = {https://doi.org/10.1186/s12874-020-01098-5},
  urldate = {2023-09-16},
  abstract = {The data from immuno-oncology (IO) therapy trials often show delayed effects, cure rate, crossing hazards, or some mixture of these phenomena. Thus, the proportional hazards (PH) assumption is often violated such that the commonly used log-rank test can be very underpowered. In these trials, the conventional hazard ratio for describing the treatment effect may not be a good estimand due to the lack of an easily understandable interpretation. To overcome this challenge, restricted mean survival time (RMST) has been strongly recommended for survival analysis in clinical literature due to its independence of the PH assumption as well as a more clinically meaningful interpretation. The RMST also aligns well with the estimand associated with the analysis from the recommendation in ICH E-9 (R1), and the test/estimation coherency. Currently, the Kaplan Meier (KM) curve is commonly applied to RMST related analyses. Due to some drawbacks of the KM approach such as the limitation in extrapolating to time points beyond the follow-up time, and the large variance at time points with small numbers of events, the RMST may be hindered.},
  keywords = {Estimand,Immuno-oncology trial,Log-rank test,Mixture Weibull,notion,RMST,Survival},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\liao2020.pdf;C\:\\Users\\david\\Zotero\\storage\\EJADDA8L\\s12874-020-01098-5.html}
}

@article{lin2020,
  title = {Alternative {{Analysis Methods}} for {{Time}} to {{Event Endpoints Under Nonproportional Hazards}}: {{A Comparative Analysis}}},
  shorttitle = {Alternative {{Analysis Methods}} for {{Time}} to {{Event Endpoints Under Nonproportional Hazards}}},
  author = {Lin, Ray S. and Lin, Ji and Roychoudhury, Satrajit and Anderson, Keaven M. and Hu, Tianle and Huang, Bo and Leon, Larry F and Liao, Jason J.Z. and Liu, Rong and Luo, Xiaodong and Mukhopadhyay, Pralay and Qin, Rui and Tatsuoka, Kay and Wang, Xuejing and Wang, Yang and Zhu, Jian and Chen, Tai-Tsang and Iacona, Renee},
  date = {2020-04-02},
  journaltitle = {Statistics in Biopharmaceutical Research},
  volume = {12},
  number = {2},
  pages = {187--198},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/19466315.2019.1697738},
  url = {https://doi.org/10.1080/19466315.2019.1697738},
  urldate = {2023-08-29},
  abstract = {The log-rank test is most powerful under proportional hazards (PH). In practice, non-PH patterns are often observed in clinical trials, such as in immuno-oncology; therefore, alternative methods are needed to restore the efficiency of statistical testing. Three categories of testing methods were evaluated, including weighted log-rank tests, Kaplan–Meier curve-based tests (including weighted Kaplan–Meier and restricted mean survival time), and combination tests (including Breslow test, Lee’s combo test, and MaxCombo test). Nine scenarios representing the PH and various non-PH patterns were simulated. The power, Type I error, and effect estimate of each method were compared. In general, all tests control Type I error well. There is not a single most powerful test across all scenarios. In the absence of prior knowledge regarding the underlying or non-PH patterns, the MaxCombo test is relatively robust across patterns. Since the treatment effect changes over time under non-PH, the overall profile of the treatment effect may not be represented comprehensively based on a single measure. Thus, multiple measures of the treatment effect should be prespecified as sensitivity analyses to describe the totality of the data. Supplementary materials for this article are available online.},
  keywords = {Fleming–Harrington test,Log-rank test,Nonproportional hazards,notion,Oncology trial,Survival analysis},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\lin2020.pdf}
}

@article{lin2023,
  title = {Matched Design for Marginal Causal Effect on Restricted Mean Survival Time in Observational Studies},
  author = {Lin, Zihan and Ni, Ai and Lu, Bo},
  date = {2023-01-01},
  journaltitle = {Journal of Causal Inference},
  volume = {11},
  number = {1},
  publisher = {De Gruyter},
  issn = {2193-3685},
  doi = {10.1515/jci-2022-0035},
  url = {https://www.degruyter.com/document/doi/10.1515/jci-2022-0035/html},
  urldate = {2023-09-06},
  abstract = {Investigating the causal relationship between exposure and time-to-event outcome is an important topic in biomedical research. Previous literature has discussed the potential issues of using hazard ratio (HR) as the marginal causal effect measure due to noncollapsibility. In this article, we advocate using restricted mean survival time (RMST) difference as a marginal causal effect measure, which is collapsible and has a simple interpretation as the difference of area under survival curves over a certain time horizon. To address both measured and unmeasured confounding, a matched design with sensitivity analysis is proposed. Matching is used to pair similar treated and untreated subjects together, which is generally more robust than outcome modeling due to potential misspecifications. Our propensity score matched RMST difference estimator is shown to be asymptotically unbiased, and the corresponding variance estimator is calculated by accounting for the correlation due to matching. Simulation studies also demonstrate that our method has adequate empirical performance and outperforms several competing methods used in practice. To assess the impact of unmeasured confounding, we develop a sensitivity analysis strategy by adapting the E -value approach to matched data. We apply the proposed method to the Atherosclerosis Risk in Communities Study (ARIC) to examine the causal effect of smoking on stroke-free survival.},
  langid = {english},
  keywords = {confounding bias,marginal effect,noncollapsibility,notion,propensity score matching,sensitivity analysis},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\lin2023.pdf}
}

@article{liu1988,
  title = {Bootstrap {{Procedures}} under Some {{Non-I}}.{{I}}.{{D}}. {{Models}}},
  author = {Liu, Regina Y.},
  date = {1988-12},
  journaltitle = {The Annals of Statistics},
  volume = {16},
  number = {4},
  pages = {1696--1708},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1176351062},
  url = {https://projecteuclid.org/journals/annals-of-statistics/volume-16/issue-4/Bootstrap-Procedures-under-some-Non-IID-Models/10.1214/aos/1176351062.full},
  urldate = {2024-02-07},
  abstract = {It is shown in this article that the classical i.i.d. bootstrap remains a valid procedure for estimating the sampling distributions of certain symmetric estimators of location, as long as the random observations are independently drawn from distributions with (essentially) a common location. This may be viewed as a robust property of the classical i.i.d. bootstrap. Also included is a study of the second order properties of a different bootstrap procedure proposed by Wu in the context of heteroscedasticity in regression.},
  keywords = {$L$-statistics,62G05,bootstrap,Edgeworth expansions,non-i.i.d. models,notion,regression,sampling distributions,second order asymptotics},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\liu1988.pdf}
}

@article{liu2020,
  title = {A Resampling-Based Test for Two Crossing Survival Curves},
  author = {Liu, Tiantian and Ditzhaus, Marc and Xu, Jin},
  date = {2020},
  journaltitle = {Pharmaceutical Statistics},
  volume = {19},
  number = {4},
  pages = {399--409},
  issn = {1539-1612},
  doi = {10.1002/pst.2000},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.2000},
  urldate = {2023-08-30},
  abstract = {The area between two survival curves is an intuitive test statistic for the classical two-sample testing problem. We propose a bootstrap version of it for assessing the overall homogeneity of these curves. Our approach allows ties in the data as well as independent right censoring, which may differ between the groups. The asymptotic distribution of the test statistic as well as of its bootstrap counterpart are derived under the null hypothesis, and their consistency is proven for general alternatives. We demonstrate the finite sample superiority of the proposed test over some existing methods in a simulation study and illustrate its application by a real-data example.},
  langid = {english},
  keywords = {area between curves,conditional bootstrap test,Kaplan-Meier estimator,log-rank test,nonproportional hazard,notion,Wilcoxon test},
  file = {C:\Users\david\Zotero\storage\2WLR5VA3\pst.html}
}

@article{long2000,
  title = {Using {{Heteroscedasticity Consistent Standard Errors}} in the {{Linear Regression Model}}},
  author = {Long, J. Scott and Ervin, Laurie H.},
  date = {2000},
  journaltitle = {The American Statistician},
  volume = {54},
  number = {3},
  eprint = {2685594},
  eprinttype = {jstor},
  pages = {217--224},
  publisher = {[American Statistical Association, Taylor \& Francis, Ltd.]},
  issn = {0003-1305},
  doi = {10.2307/2685594},
  url = {https://www.jstor.org/stable/2685594},
  urldate = {2024-02-14},
  abstract = {In the presence of heteroscedasticity, ordinary least squares (OLS) estimates are unbiased, but the usual tests of significance are generally inappropriate and their use can lead to incorrect inferences. Tests based on a heteroscedasticity consistent covariance matrix (HCCM), however, are consistent even in the presence of heteroscedasticity of an unknown form. Most applications that use a HCCM appear to rely on the asymptotic version known as HC0. Our Monte Carlo simulations show that HC0 often results in incorrect inferences when N ≤ 250, while three relatively unknown, small sample versions of the HCCM, and especially a version known as HC3, work well even for N's as small as 25. We recommend that: (1) data analysts should correct for heteroscedasticity using a HCCM whenever there is reason to suspect heteroscedasticity; (2) the decision to use HCCM-based tests should not be determined by a screening test for heteroscedasticity; and (3) when N ≤ 250, the HCCM known as HC3 should be used. Since HC3 is simple to compute, we encourage authors of statistical software to add this estimator to their programs.},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\long2000.pdf}
}

@article{luo2019,
  title = {Design and Monitoring of Survival Trials Based on Restricted Mean Survival Times},
  author = {Luo, Xiaodong and Huang, Bo and Quan, Hui},
  date = {2019-12-01},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {16},
  number = {6},
  pages = {616--625},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774519871447},
  url = {https://doi.org/10.1177/1740774519871447},
  urldate = {2023-09-13},
  abstract = {Background/Aims: Restricted mean survival time has become a popular treatment effect measurement because of its nice interpretability. However, study design based on restricted mean survival times often requires extensive simulation studies as the variance structure is hard to obtain analytically. This article aims to provide a flexible approach to conduct study design and monitoring based on the restricted mean survival times without resorting to simulation. Methods: We assume that both the event time and censoring time distributions are piecewise exponential, and the accrual distribution is piecewise uniform, with which the restricted mean survival times and their variance–covariance structure can be conveniently computed. Results: Since we allow arbitrary number of pieces in the piecewise exponential and uniform distributions, the resulting model can handle a wide range of scenarios. The usefulness of the approach is demonstrated via an example. Conclusion: The proposed approach is flexible and useful in the design and monitoring of survival trials based on restricted mean survival times.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\luo2019.pdf}
}

@article{mackinnon1985,
  title = {Some Heteroskedasticity-Consistent Covariance Matrix Estimators with Improved Finite Sample Properties},
  author = {MacKinnon, James G and White, Halbert},
  date = {1985-09-01},
  journaltitle = {Journal of Econometrics},
  shortjournal = {Journal of Econometrics},
  volume = {29},
  number = {3},
  pages = {305--325},
  issn = {0304-4076},
  doi = {10.1016/0304-4076(85)90158-7},
  url = {https://www.sciencedirect.com/science/article/pii/0304407685901587},
  urldate = {2023-12-08},
  abstract = {We examine several modified versions of the heteroskedasticity-consistent covariance matrix estimator of Hinkley (1977) and White (1980). On the basis of sampling experiments which compare the performance of quasi t-statistics, we find that one estimator, based on the jackknife, performs better in small samples than the rest. We also examine the finite-sample properties of using modified critical values based on Edgeworth approximations, as proposed by Rothenberg (1984). In addition, we compare the power of several tests for heteroskedasticity, and find that it may be wise to employ the jackknife heteroskedasticity-consistent covariance matrix even in the absence of detected heteroskedasticity.},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\mackinnon1985.pdf;C\:\\Users\\david\\Zotero\\storage\\ZLZ35J3X\\0304407685901587.html}
}

@incollection{mackinnon2009,
  title = {Bootstrap {{Hypothesis Testing}}},
  booktitle = {Handbook of {{Computational Econometrics}}},
  author = {MacKinnon, James G.},
  date = {2009},
  pages = {183--213},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9780470748916.ch6},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470748916.ch6},
  urldate = {2024-01-24},
  abstract = {This chapter contains sections titled: Introduction Bootstrap and Monte Carlo tests Finite-sample properties of bootstrap tests Double bootstrap and fast double bootstrap tests Bootstrap data generating processes Multiple test statistics Finite-sample properties of bootstrap supF tests Conclusion Acknowledgments References},
  isbn = {978-0-470-74891-6},
  langid = {english},
  keywords = {bootstrap and Monte Carlo tests,bootstrap data generating processes,bootstrap DGPs for multivariate regression models,bootstrap hypothesis testing,bootstrap supF tests,double bootstrap and fast double bootstrap,empirical distribution function (EDF),multiple test statistics,notion,residual bootstrap,simulation-based testing},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\mackinnon2009.pdf;C\:\\Users\\david\\Zotero\\storage\\TVZDAKAR\\9780470748916.html}
}

@article{magirr2019,
  title = {Modestly Weighted Logrank Tests},
  author = {Magirr, Dominic and Burman, Carl-Fredrik},
  date = {2019},
  journaltitle = {Statistics in Medicine},
  volume = {38},
  number = {20},
  pages = {3782--3790},
  issn = {1097-0258},
  doi = {10.1002/sim.8186},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8186},
  urldate = {2023-08-30},
  abstract = {We propose a new class of weighted logrank tests (WLRTs) that control the risk of concluding that a new drug is more efficacious than standard of care, when, in fact, it is uniformly inferior. Perhaps surprisingly, this risk is not controlled for WLRT in general. Tests from this new class can be constructed to have high power under a delayed-onset treatment effect scenario, as well as being almost as efficient as the standard logrank test under proportional hazards.},
  langid = {english},
  keywords = {drug regulation,immuno-oncology,nonproportional hazards,notion,weighted logrank test},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\magirr2019.pdf}
}

@article{magirr2021,
  title = {Non‐proportional Hazards in Immuno‐oncology: {{Is}} an Old Perspective Needed?},
  shorttitle = {Non‐proportional Hazards in Immuno‐oncology},
  author = {Magirr, Dominic},
  date = {2021-05},
  journaltitle = {Pharmaceutical Statistics},
  shortjournal = {Pharmaceutical Statistics},
  volume = {20},
  number = {3},
  pages = {512--527},
  issn = {1539-1604, 1539-1612},
  doi = {10.1002/pst.2091},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/pst.2091},
  urldate = {2023-08-21},
  abstract = {A fundamental concept in two-arm non-parametric survival analysis is the comparison of observed versus expected numbers of events on one of the treatment arms (the choice of which arm is arbitrary), where the expectation is taken assuming that the true survival curves in the two arms are identical. This concept is at the heart of the counting-process theory that provides a rigorous basis for methods such as the log-rank test. It is natural, therefore, to maintain this perspective when extending the log-rank test to deal with nonproportional hazards, for example, by considering a weighted sum of the “observed - expected” terms, where larger weights are given to time periods where the hazard ratio is expected to favor the experimental treatment. In doing so, however, one may stumble across some rather subtle issues, related to difficulties in the interpretation of hazard ratios, that may lead to strange conclusions. An alternative approach is to view non-parametric survival comparisons as permutation tests. With this perspective, one can easily improve on the efficiency of the log-rank test, while thoroughly controlling the false positive rate. In particular, for the field of immuno-oncology, where researchers often anticipate a delayed treatment effect, sample sizes could be substantially reduced without loss of power.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\magirr2021.pdf}
}

@article{manner2019,
  title = {Restricted Mean Survival Time for the Analysis of Cardiovascular Outcome Trials Assessing Non-Inferiority: {{Case}} Studies from Antihyperglycemic Drug Development},
  shorttitle = {Restricted Mean Survival Time for the Analysis of Cardiovascular Outcome Trials Assessing Non-Inferiority},
  author = {Manner, David H. and Battioui, Chakib and Hantel, Stefan and Beasley, B. Nhi and Wei, Lee-Jen and Geiger, Mary Jane and Turner, J. Rick and Abt, Markus},
  date = {2019-09-01},
  journaltitle = {American Heart Journal},
  shortjournal = {American Heart Journal},
  volume = {215},
  pages = {178--186},
  issn = {0002-8703},
  doi = {10.1016/j.ahj.2019.05.016},
  url = {https://www.sciencedirect.com/science/article/pii/S0002870319301425},
  urldate = {2023-09-08},
  abstract = {Cardiovascular outcome trials (CVOTs) have been employed in multiple therapeutic areas to explore whether a noncardiovascular drug increases the risk for cardiovascular events. These studies are now a central part of drug development programs for antihyperglycemic drugs. These programs are expected to demonstrate that new antihyperglycemic drugs for patients with Type 2 diabetes do not have unacceptable cardiovascular risk. The hazard ratio, which is usually provided as evidence that patients receiving the investigational treatment are not at statistically significantly greater cardiovascular risk than patients on the control treatment, can be difficult to interpret for various reasons. Therefore, an alternative approach known as the Restricted Mean Survival Time (RMST) or τ-year mean survival time is presented, and its ability to overcome interpretation challenges with the hazard ratio discussed. The RMST approach is applied to five completed CVOTs and is compared with the corresponding hazard ratios. Additionally, detailed considerations are given on how to design a non-inferiority CVOT using the RMST approach. The RMST methodology is shown to be a practical alternative to the hazard ratio methodology for designing a non-inferiority CVOT.},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\manner2019.pdf;C\:\\Users\\david\\Zotero\\storage\\B843X77N\\S0002870319301425.html}
}

@article{martinussen2020,
  title = {Subtleties in the Interpretation of Hazard Contrasts},
  author = {Martinussen, Torben and Vansteelandt, Stijn and Andersen, Per Kragh},
  date = {2020-10-01},
  journaltitle = {Lifetime Data Analysis},
  shortjournal = {Lifetime Data Anal},
  volume = {26},
  number = {4},
  pages = {833--855},
  issn = {1572-9249},
  doi = {10.1007/s10985-020-09501-5},
  url = {https://doi.org/10.1007/s10985-020-09501-5},
  urldate = {2023-09-14},
  abstract = {The hazard ratio is one of the most commonly reported measures of treatment effect in randomised trials, yet the source of much misinterpretation. This point was made clear by Hernán (Epidemiology (Cambridge, Mass) 21(1):13–15, 2010) in a commentary, which emphasised that the hazard ratio contrasts populations of treated and untreated individuals who survived a given period of time, populations that will typically fail to be comparable—even in a randomised trial—as a result of different pressures or intensities acting on different populations. The commentary has been very influential, but also a source of surprise and confusion. In this note, we aim to provide more insight into the subtle interpretation of hazard ratios and differences, by investigating in particular what can be learned about a treatment effect from the hazard ratio becoming 1 (or the hazard difference 0) after a certain period of time. We further define a hazard ratio that has a causal interpretation and study its relationship to the Cox hazard ratio, and we also define a causal hazard difference. These quantities are of theoretical interest only, however, since they rely on assumptions that cannot be empirically evaluated. Throughout, we will focus on the analysis of randomised experiments.},
  langid = {english},
  keywords = {Causality,Cox regression,Hazard difference,Hazard ratio,notion,Randomised study,Survival analysis},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\martinussen2020.pdf}
}

@article{martinussen2022,
  title = {Causality and the {{Cox Regression Model}}},
  author = {Martinussen, Torben},
  date = {2022-03-07},
  journaltitle = {Annual Review of Statistics and Its Application},
  shortjournal = {Annu. Rev. Stat. Appl.},
  volume = {9},
  number = {1},
  pages = {249--259},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-040320-114441},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-040320-114441},
  urldate = {2023-09-14},
  abstract = {This article surveys results concerning the interpretation of the Cox hazard ratio in connection to causality in a randomized study with a time-to-event response. The Cox model is assumed to be correctly specified, and we investigate whether the typical end product of such an analysis, the estimated hazard ratio, has a causal interpretation as a hazard ratio. It has been pointed out that this is not possible due to selection. We provide more insight into the interpretation of hazard ratios and differences, investigating what can be learned about a treatment effect from the hazard ratio approaching unity after a certain period of time. The conclusion is that the Cox hazard ratio is not causally interpretable as a hazard ratio unless there is no treatment effect or an untestable and unrealistic assumption holds. We give a hazard ratio that has a causal interpretation and study its relationship to the Cox hazard ratio.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\Zotero\storage\C5D2V786\Martinussen - 2022 - Causality and the Cox Regression Model.pdf}
}

@article{mccaw2021,
  title = {Choosing Clinically Interpretable Summary Measures and Robust Analytic Procedures for Quantifying the Treatment Difference in Comparative Clinical Studies},
  author = {McCaw, Zachary R. and Tian, Lu and Wei, Jiawei and Claggett, Brian Lee and Bretz, Frank and Fitzmaurice, Garrett and Wei, Lee-Jen},
  date = {2021-12-10},
  journaltitle = {Statistics in medicine},
  shortjournal = {Stat Med},
  volume = {40},
  number = {28},
  eprint = {34783094},
  eprinttype = {pmid},
  pages = {6235--6242},
  issn = {0277-6715},
  doi = {10.1002/sim.8971},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8687139/},
  urldate = {2023-10-04},
  pmcid = {PMC8687139},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\mccaw2021.pdf}
}

@article{mollenhoff2023,
  title = {Investigating Non-Inferiority or Equivalence in Time-to-Event Data under Non-Proportional Hazards},
  author = {Möllenhoff, Kathrin and Tresch, Achim},
  date = {2023-07},
  journaltitle = {Lifetime Data Analysis},
  shortjournal = {Lifetime Data Anal},
  volume = {29},
  number = {3},
  pages = {483--507},
  issn = {1380-7870, 1572-9249},
  doi = {10.1007/s10985-023-09589-5},
  url = {https://link.springer.com/10.1007/s10985-023-09589-5},
  urldate = {2023-08-23},
  abstract = {The classical approach to analyze time-to-event data, e.g. in clinical trials, is to fit Kaplan–Meier curves yielding the treatment effect as the hazard ratio between treatment groups. Afterwards, a log-rank test is commonly performed to investigate whether there is a difference in survival or, depending on additional covariates, a Cox proportional hazard model is used. However, in numerous trials these approaches fail due to the presence of non-proportional hazards, resulting in difficulties of interpreting the hazard ratio and a loss of power. When considering equivalence or non-inferiority trials, the commonly performed log-rank based tests are similarly affected by a violation of this assumption. Here we propose a parametric framework to assess equivalence or non-inferiority for survival data. We derive pointwise confidence bands for both, the hazard ratio and the difference of the survival curves. Further we propose a test procedure addressing non-inferiority and equivalence by directly comparing the survival functions at certain time points or over an entire range of time. Once the model’s suitability is proven the method provides a noticeable power benefit, irrespectively of the shape of the hazard ratio. On the other hand, model selection should be carried out carefully as misspecification may cause type I error inflation in some situations. We investigate the robustness and demonstrate the advantages and disadvantages of the proposed methods by means of a simulation study. Finally, we demonstrate the validity of the methods by a clinical trial example.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\mollenhoff2023.pdf}
}

@article{morris2019,
  title = {Proposals on {{Kaplan}}–{{Meier}} Plots in Medical Research and a Survey of Stakeholder Views: {{KMunicate}}},
  shorttitle = {Proposals on {{Kaplan}}–{{Meier}} Plots in Medical Research and a Survey of Stakeholder Views},
  author = {Morris, Tim P. and Jarvis, Christopher I. and Cragg, William and Phillips, Patrick P. J. and Choodari-Oskooei, Babak and Sydes, Matthew R.},
  date = {2019-09-01},
  journaltitle = {BMJ Open},
  volume = {9},
  number = {9},
  eprint = {31575572},
  eprinttype = {pmid},
  pages = {e030215},
  publisher = {British Medical Journal Publishing Group},
  issn = {2044-6055, 2044-6055},
  doi = {10.1136/bmjopen-2019-030215},
  url = {https://bmjopen.bmj.com/content/9/9/e030215},
  urldate = {2023-08-30},
  abstract = {Objectives To examine reactions to the proposed improvements to standard Kaplan–Meier plots, the standard way to present time-to-event data, and to understand which (if any) facilitated better depiction of (1) the state of patients over time, and (2) uncertainty over time in the estimates of survival. Design A survey of stakeholders’ opinions on the proposals. Setting A web-based survey, open to international participation, for those with an interest in visualisation of time-to-event data. Participants 1174 people participated in the survey over a 6-week period. Participation was global (although primarily Europe and North America) and represented a wide range of researchers (primarily statisticians and clinicians). Main outcome measures Two outcome measures were of principal importance: (1) participants’ opinions of each proposal compared with a ‘standard’ Kaplan–Meier plot; and (2) participants’ overall ranking of the proposals (including the standard). Results Most proposals were more popular than the standard Kaplan–Meier plot. The most popular proposals in the two categories, respectively, were an extended table beneath the plot depicting the numbers at risk, censored and having experienced an event at periodic timepoints, and CIs around each Kaplan–Meier curve. Conclusions This study produced a high response number, reflecting the importance of graphics for time-to-event data. Those producing and publishing Kaplan–Meier plots—both authors and journals—should, as a starting point, consider using the combination of the two favoured proposals.},
  langid = {english},
  keywords = {Clinical trials,Data visualisation,Figures,Graphs,Kaplan–Meier,notion,Survival analysis},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\morris2019.pdf}
}

@article{morris2019a,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  date = {2019},
  journaltitle = {Statistics in Medicine},
  volume = {38},
  number = {11},
  pages = {2074--2102},
  issn = {1097-0258},
  doi = {10.1002/sim.8086},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8086},
  urldate = {2024-02-22},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudo-random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods because some “truth” (usually some parameter/s of interest) is known from the process of generating the data. This allows us to consider properties of methods, such as bias. While widely used, simulation studies are often poorly designed, analyzed, and reported. This tutorial outlines the rationale for using simulation studies and offers guidance for design, execution, analysis, reporting, and presentation. In particular, this tutorial provides a structured approach for planning and reporting simulation studies, which involves defining aims, data-generating mechanisms, estimands, methods, and performance measures (“ADEMP”); coherent terminology for simulation studies; guidance on coding simulation studies; a critical discussion of key performance measures and their estimation; guidance on structuring tabular and graphical presentation of results; and new graphical presentations. With a view to describing recent practice, we review 100 articles taken from Volume 34 of Statistics in Medicine, which included at least one simulation study and identify areas for improvement.},
  langid = {english},
  keywords = {graphics for simulation,Monte Carlo,notion,simulation design,simulation reporting,simulation studies},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\morris2019a.pdf;C\:\\Users\\david\\Zotero\\storage\\QENRQRZE\\sim.html}
}

@article{mukhopadhyay2020,
  title = {Statistical and Practical Considerations in Designing of Immuno-Oncology Trials},
  author = {Mukhopadhyay, Pralay and Huang, Wenmei and Metcalfe, Paul and Öhrn, Fredrik and Jenner, Mary and Stone, Andrew},
  date = {2020-11-01},
  journaltitle = {Journal of Biopharmaceutical Statistics},
  volume = {30},
  number = {6},
  eprint = {33706684},
  eprinttype = {pmid},
  pages = {1130--1146},
  publisher = {Taylor \& Francis},
  issn = {1054-3406},
  doi = {10.1080/10543406.2020.1815035},
  url = {https://doi.org/10.1080/10543406.2020.1815035},
  urldate = {2023-08-30},
  abstract = {The novel mechanism of action of immunotherapy agents, in treatment of various types of cancer, poses unique challenges during the designing of clinical trials. It is important to account for possibility of a delayed treatment effect and adjust sample size accordingly. This paper provides an analytical approach for computing sample size in the presence of a delayed effect using a piece-wise proportional hazards model. Failing to account for an anticipated treatment delay may result in considerable loss in power. The overall hazard ratio (HR), which now represents the average HR across the entire treatment period, can remain a meaningful measure of average benefit to patients in the trial. We show that, special consideration needs to be given for the designing of interim analyses related to futility, so as not to increase the probability of incorrectly stopping an effective agent. It is shown that the weighted log-rank test, using the Fleming-Harrington class of weights, can be used as supportive analysis to better reflect the impact of a delayed effect and possible long-term benefit in a subset of the overall population.},
  keywords = {Delayed treatment effect,futility,interim analysis,notion,piece-wise proportional hazards model,weighted log-rank test},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\mukhopadhyay2020.pdf}
}

@article{mukhopadhyay2022,
  title = {Log-{{Rank Test}} vs {{MaxCombo}} and {{Difference}} in {{Restricted Mean Survival Time Tests}} for {{Comparing Survival Under Nonproportional Hazards}} in {{Immuno-oncology Trials}}: {{A Systematic Review}} and {{Meta-analysis}}},
  shorttitle = {Log-{{Rank Test}} vs {{MaxCombo}} and {{Difference}} in {{Restricted Mean Survival Time Tests}} for {{Comparing Survival Under Nonproportional Hazards}} in {{Immuno-oncology Trials}}},
  author = {Mukhopadhyay, Pralay and Ye, Jiabu and Anderson, Keaven M. and Roychoudhury, Satrajit and Rubin, Eric H. and Halabi, Susan and Chappell, Richard J.},
  date = {2022-09-01},
  journaltitle = {JAMA Oncology},
  shortjournal = {JAMA Oncology},
  volume = {8},
  number = {9},
  pages = {1294--1300},
  issn = {2374-2437},
  doi = {10.1001/jamaoncol.2022.2666},
  url = {https://doi.org/10.1001/jamaoncol.2022.2666},
  urldate = {2023-08-29},
  abstract = {The log-rank test is considered the criterion standard for comparing 2 survival curves in pivotal registrational trials. However, with novel immunotherapies that often violate the proportional hazards assumptions over time, log-rank can lose power and may fail to detect treatment benefit. The MaxCombo test, a combination of weighted log-rank tests, retains power under different types of nonproportional hazards. The difference in restricted mean survival time (dRMST) test is frequently proposed as an alternative to the log-rank under nonproportional hazard scenarios.To compare the log-rank with the MaxCombo and dRMST in immuno-oncology trials to evaluate their performance in practice.Comprehensive literature review using Google Scholar, PubMed, and other sources for randomized clinical trials published in peer-reviewed journals or presented at major clinical conferences before December 2019 assessing efficacy of anti-programmed cell death protein-1 or anti-programmed death/ligand 1 monoclonal antibodies.Pivotal studies with overall survival or progression-free survival as the primary or key secondary end point with a planned statistical comparison in the protocol. Sixty-three studies on anti-programmed cell death protein-1 or anti-programmed death/ligand 1 monoclonal antibodies used as monotherapy or in combination with other agents in 35\,902 patients across multiple solid tumor types were identified.Statistical comparisons (n\,=\,150) were made between the 3 tests using the analysis populations as defined in the original protocol of each trial.Nominal significance based on a 2-sided .05-level test was used to evaluate concordance. Case studies featuring different types of nonproportional hazards were used to discuss more robust ways of characterizing treatment benefit instead of sole reliance on hazard ratios.In this systematic review and meta-analysis of 63 studies including 35\,902 patients, between the log-rank and MaxCombo, 135 of 150 comparisons (90\%) were concordant; MaxCombo achieved nominal significance in 15 of 15 discordant cases, while log-rank did not. Several cases appeared to have clinically meaningful benefits that would not have been detected using log-rank. Between the log-rank and dRMST tests, 137 of 150 comparisons (91\%) were concordant; log-rank was nominally significant in 5 of 13 cases, while dRMST was significant in 8 of 13. Among all 3 tests, 127 comparisons (85\%) were concordant.The findings of this review show that MaxCombo may provide a pragmatic alternative to log-rank when departure from proportional hazards is anticipated. Both tests resulted in the same statistical decision in most comparisons. Discordant studies had modest to meaningful improvements in treatment effect. The dRMST test provided no added sensitivity for detecting treatment differences over log-rank.},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\mukhopadhyay2022.pdf;C\:\\Users\\david\\Zotero\\storage\\WWF7HRS4\\2794629.html}
}

@online{munko2023,
  title = {{{RMST-based}} Multiple Contrast Tests in General Factorial Designs},
  author = {Munko, Merle and Ditzhaus, Marc and Dobler, Dennis and Genuneit, Jon},
  date = {2023-08-16},
  eprint = {2308.08346},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.2308.08346},
  url = {http://arxiv.org/abs/2308.08346},
  urldate = {2024-02-14},
  abstract = {Several methods in survival analysis are based on the proportional hazards assumption. However, this assumption is very restrictive and often not justifiable in practice. Therefore, effect estimands that do not rely on the proportional hazards assumption are highly desirable in practical applications. One popular example for this is the restricted mean survival time (RMST). It is defined as the area under the survival curve up to a prespecified time point and, thus, summarizes the survival curve into a meaningful estimand. For two-sample comparisons based on the RMST, previous research found the inflation of the type I error of the asymptotic test for small samples and, therefore, a two-sample permutation test has already been developed. The first goal of the present paper is to further extend the permutation test for general factorial designs and general contrast hypotheses by considering a Wald-type test statistic and its asymptotic behavior. Additionally, a groupwise bootstrap approach is considered. Moreover, when a global test detects a significant difference by comparing the RMSTs of more than two groups, it is of interest which specific RMST differences cause the result. However, global tests do not provide this information. Therefore, multiple tests for the RMST are developed in a second step to infer several null hypotheses simultaneously. Hereby, the asymptotically exact dependence structure between the local test statistics is incorporated to gain more power. Finally, the small sample performance of the proposed global and multiple testing procedures is analyzed in simulations and illustrated in a real data example.},
  pubstate = {preprint},
  keywords = {62N03,notion,Statistics - Methodology},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\munko2023.pdf;C\:\\Users\\david\\Zotero\\storage\\6PYSIQA5\\2308.html}
}

@article{nemes2020,
  title = {A {{Brief Overview}} of {{Restricted Mean Survival Time Estimators}} and {{Associated Variances}}},
  author = {Nemes, Szilárd and Bülow, Erik and Gustavsson, Andreas},
  date = {2020-06},
  journaltitle = {Stats},
  volume = {3},
  number = {2},
  pages = {107--119},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2571-905X},
  doi = {10.3390/stats3020010},
  url = {https://www.mdpi.com/2571-905X/3/2/10},
  urldate = {2023-09-13},
  abstract = {Restricted Mean Survival Time (    R M S T    ) experiences a renaissance and is advocated as a model-free, easy to interpret alternative to proportional hazards regression and hazard rates with implication in causal inference. Estimation of     R M S T     and associated variance is mainly done by numerical integration of Kaplan–Meier curves. In this paper we briefly review the two main alternatives to the Kaplan–Meier method; analysis based on pseudo-observations, and the flexible parametric survival method. Using computer simulations, we assess the efficacy of the three methods compared to a fully parametric approach where the distribution of survival times is known. Thereafter, the three methods are directly compared without any distributional assumption for the survival data. Generally, flexible parametric survival methods outperform both competitors, however the differences are small.},
  issue = {2},
  langid = {english},
  keywords = {censoring,efficacy,flexible-survival methods,Kaplan–Meier,notion,pseudo-observation,RMST,variance estimator},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\nemes2020.pdf}
}

@article{norskov2021,
  title = {Assessment of Assumptions of Statistical Analysis Methods in Randomised Clinical Trials: The What and How},
  shorttitle = {Assessment of Assumptions of Statistical Analysis Methods in Randomised Clinical Trials},
  author = {Nørskov, Anders Kehlet and Lange, Theis and Nielsen, Emil Eik and Gluud, Christian and Winkel, Per and Beyersmann, Jan and family=Uña-Álvarez, given=Jacobo, prefix=de, useprefix=false and Torri, Valter and Billot, Laurent and Putter, Hein and Wetterslev, Jørn and Thabane, Lehana and Jakobsen, Janus Christian},
  date = {2021-06-01},
  journaltitle = {BMJ Evidence-Based Medicine},
  volume = {26},
  number = {3},
  eprint = {31988195},
  eprinttype = {pmid},
  pages = {121--126},
  publisher = {Royal Society of Medicine},
  issn = {2515-446X, 2515-4478},
  doi = {10.1136/bmjebm-2019-111268},
  url = {https://ebm.bmj.com/content/26/3/121},
  urldate = {2023-09-14},
  abstract = {When analysing and presenting results of randomised clinical trials, trialists rarely report if or how underlying statistical assumptions were validated. To avoid data-driven biased trial results, it should be common practice to prospectively describe the assessments of underlying assumptions. In existing literature, there is no consensus on how trialists should assess and report underlying assumptions for the analyses of randomised clinical trials. With this study, we developed suggestions on how to test and validate underlying assumptions behind logistic regression, linear regression, and Cox regression when analysing results of randomised clinical trials. Two investigators compiled an initial draftbased on a review of the literature. Experienced statisticians and trialists from eight different research centres and trial units then participated in a anonymised consensus process, where we reached agreement on the suggestions presented in this paper. This paper provides detailed suggestions on 1) which underlying statistical assumptions behind logistic regression, multiple linear regression and Cox regression each should be assessed; 2) how these underlying assumptions may be assessed; and 3) what to do if these assumptions are violated. We believe that the validity of randomised clinical trial results will increase if our recommendations for assessing and dealing with violations of the underlying statistical assumptions are followed.},
  langid = {english},
  keywords = {epidemiology,notion,statistics & research methods},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\norskov2021.pdf}
}

@article{nygardjohansen2020,
  title = {Regression Models Using Parametric Pseudo-Observations},
  author = {Nygård Johansen, Martin and Lundbye-Christensen, Søren and Thorlund Parner, Erik},
  date = {2020},
  journaltitle = {Statistics in Medicine},
  volume = {39},
  number = {22},
  pages = {2949--2961},
  issn = {1097-0258},
  doi = {10.1002/sim.8586},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8586},
  urldate = {2023-11-01},
  abstract = {Pseudo-observations based on the nonparametric Kaplan-Meier estimator of the survival function have been proposed as an alternative to the widely used Cox model for analyzing censored time-to-event data. Using a spline-based estimator of the survival has some potential benefits over the nonparametric approach in terms of less variability. We propose to define pseudo-observations based on a flexible parametric estimator and use these for analysis in regression models to estimate parameters related to the cumulative risk. We report the results of a simulation study that compares the empirical standard errors of estimates based on parametric and nonparametric pseudo-observations in various settings. Our simulations show that in some situations there is a substantial gain in terms of reduced variability using the proposed parametric pseudo-observations compared with the nonparametric pseudo-observations. The gain can be measured as a reduction of the empirical standard error by up to about one third; corresponding to an additional 125\% larger sample size. We illustrate the use of the proposed method in a brief data example.},
  langid = {english},
  keywords = {flexible parametric models,notion,pseudo-observations,time-to-event},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\nygardjohansen2020.pdf;C\:\\Users\\david\\Zotero\\storage\\NFWA259M\\sim.html}
}

@article{oken1982,
  title = {Toxicity and Response Criteria of the {{Eastern Cooperative Oncology Group}}},
  author = {Oken, M. M. and Creech, R. H. and Tormey, D. C. and Horton, J. and Davis, T. E. and McFadden, E. T. and Carbone, P. P.},
  date = {1982-12},
  journaltitle = {American Journal of Clinical Oncology},
  shortjournal = {Am J Clin Oncol},
  volume = {5},
  number = {6},
  eprint = {7165009},
  eprinttype = {pmid},
  pages = {649--655},
  issn = {0277-3732},
  langid = {english},
  keywords = {Antineoplastic Agents,Drug Evaluation,Humans,Neoplasms,notion,United States}
}

@article{overgaard2018,
  title = {Estimating the Variance in a Pseudo-Observation Scheme with Competing Risks},
  author = {Overgaard, Morten and Parner, Erik Thorlund and Pedersen, Jan},
  date = {2018},
  journaltitle = {Scandinavian Journal of Statistics},
  volume = {45},
  number = {4},
  pages = {923--940},
  issn = {1467-9469},
  doi = {10.1111/sjos.12328},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12328},
  urldate = {2024-02-07},
  abstract = {The Huber–White sandwich variance estimator of the variance of estimates from a regression using jack-knife pseudo-observations from the Aalen–Johansen estimator in a competing risks situation is known to be biased. In this paper, an expression of the asymptotic bias of the Huber–White variance estimator is found, and the Huber–White variance estimator is seen to be biased upwards. An alternative variance estimator, obtained by plugging in empirical values in the true variance expression, is studied by simulation, and its performance is compared with the performance of the biased Huber–White variance estimator. On the basis of the simulation study, recommendations of its use are given. The alternative variance estimator works well in large samples but, because it estimates the asymptotic variance, may be less useful on small samples.},
  langid = {english},
  keywords = {Aalen–Johansen functional,functional differentiability,generalized estimating equation,jack-knife pseudo-values,notion,time-to-event analysis},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\overgaard2018.pdf}
}

@article{overgaard2019,
  title = {Pseudo-Observations under Covariate-Dependent Censoring},
  author = {Overgaard, Morten and Parner, Erik Thorlund and Pedersen, Jan},
  date = {2019-09-01},
  journaltitle = {Journal of Statistical Planning and Inference},
  shortjournal = {Journal of Statistical Planning and Inference},
  volume = {202},
  pages = {112--122},
  issn = {0378-3758},
  doi = {10.1016/j.jspi.2019.02.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0378375819300175},
  urldate = {2023-11-01},
  abstract = {A regression analysis using jack-knife pseudo-observations from the Kaplan–Meier estimator, or related estimators, can be biased when censoring times depend on event times or covariates. We study ways in which other, covariate-dependent, estimators can be used in place of the Kaplan–Meier related estimators to overcome the problem. These estimators are inverse probability weighted estimators, weighting with an estimate of the probability of observation based on a model of the censoring distribution. We study an additive hazard model and a proportional hazards model for the censoring distribution. We argue that, under certain assumptions, the pseudo-observation method with pseudo-observations from such estimators will produce consistent and asymptotically normal parameter estimates.},
  keywords = {-variation,Functional approach,IPCW,Pseudo-value,Survival analysis},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\overgaard2019.pdf;C\:\\Users\\david\\Zotero\\storage\\BMAJFPPZ\\S0378375819300175.html}
}

@article{paparoditis2005,
  title = {Bootstrap Hypothesis Testing in Regression Models},
  author = {Paparoditis, Efstathios and Politis, Dimitris N.},
  date = {2005-10-15},
  journaltitle = {Statistics \& Probability Letters},
  shortjournal = {Statistics \& Probability Letters},
  volume = {74},
  number = {4},
  pages = {356--365},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2005.04.057},
  url = {https://www.sciencedirect.com/science/article/pii/S0167715205001951},
  urldate = {2024-01-24},
  abstract = {The paper investigates how the particular choice of residuals used in a bootstrap-based testing procedure affects the properties of the test. The properties of the tests are investigated both under the null and under the alternative. It is shown that for non-pivotal test statistics, the method used to obtain residuals largely affects the power behavior of the tests. For instance, imposing the null hypothesis in the residual estimation step—although it does not affect the behavior of the test if the null is true—it leads to a loss of power under the alternative as compared to tests based on resampling unrestricted residuals. Residuals obtained using a parameter estimator which minimizes their variance maximizes the power of the corresponding bootstrap-based tests. In this context, studentizing makes the tests more robust to such residual effects.},
  keywords = {Hypothesis testing,notion,Parametric models,Resampling,Residuals},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\paparoditis2005.pdf;C\:\\Users\\david\\Zotero\\storage\\HMZNYK2C\\S0167715205001951.html}
}

@article{parner2023,
  title = {Regression Models for Censored Time-to-Event Data Using Infinitesimal Jack-Knife Pseudo-Observations, with Applications to Left-Truncation},
  author = {Parner, Erik T. and Andersen, Per K. and Overgaard, Morten},
  date = {2023-07-01},
  journaltitle = {Lifetime Data Analysis},
  shortjournal = {Lifetime Data Anal},
  volume = {29},
  number = {3},
  pages = {654--671},
  issn = {1572-9249},
  doi = {10.1007/s10985-023-09597-5},
  url = {https://doi.org/10.1007/s10985-023-09597-5},
  urldate = {2023-11-01},
  abstract = {Jack-knife pseudo-observations have in recent decades gained popularity in regression analysis for various aspects of time-to-event data. A limitation of the jack-knife pseudo-observations is that their computation is time consuming, as the base estimate needs to be recalculated when leaving out each observation. We show that jack-knife pseudo-observations can be closely approximated using the idea of the infinitesimal jack-knife residuals. The infinitesimal jack-knife pseudo-observations are much faster to compute than jack-knife pseudo-observations. A key assumption of the unbiasedness of the jack-knife pseudo-observation approach is on the influence function of the base estimate. We reiterate why the condition on the influence function is needed for unbiased inference and show that the condition is not satisfied for the Kaplan–Meier base estimate in a left-truncated cohort. We present a modification of the infinitesimal jack-knife pseudo-observations that provide unbiased estimates in a left-truncated cohort. The computational speed and medium and large sample properties of the jack-knife pseudo-observations and infinitesimal jack-knife pseudo-observation are compared and we present an application of the modified infinitesimal jack-knife pseudo-observations in a left-truncated cohort of Danish patients with diabetes.},
  langid = {english},
  keywords = {Competing risks,Cumulative incidence,Cumulative risk,Left-truncation,notion,Pseudo-observations},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\parner2023.pdf}
}

@article{paukner2021,
  title = {Window Mean Survival Time},
  author = {Paukner, Mitchell and Chappell, Richard},
  date = {2021},
  journaltitle = {Statistics in Medicine},
  volume = {40},
  number = {25},
  pages = {5521--5533},
  issn = {1097-0258},
  doi = {10.1002/sim.9138},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9138},
  urldate = {2023-09-14},
  abstract = {We propose a class of alternative estimates and tests to restricted mean survival time (RMST) which improves power in numerous survival scenarios while maintaining a level of interpretability. The industry standards for interpretable hypothesis tests in survival analysis, RMST and logrank tests (LRTs), can suffer from low power in cases where the proportional hazards assumption fails. In particular, when late differences occur between survival curves, our proposed estimate and class of tests, window mean survival time (WMST), outperforms both RMST and LRT without sacrificing interpretability, unlike weighted rank tests (WRTs). WMST has the added advantage of maintaining high power when the proportional hazards assumption is met, while WRTs do not. With testing methods often being chosen in advance of data collection, WMST can ensure adequate power without distributional assumptions and is robust to the choice of its restriction parameters. Functions for performing WMST analysis are provided in the survWM2 package in R.},
  langid = {english},
  keywords = {logrank test,nonproportional hazards,notion,survival data,weighted rank test},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\paukner2021.pdf;C\:\\Users\\david\\Zotero\\storage\\J6QWVG9F\\sim.html}
}

@article{paukner2022,
  title = {Versatile Tests for Window Mean Survival Time},
  author = {Paukner, Mitchell and Chappell, Richard},
  date = {2022},
  journaltitle = {Statistics in Medicine},
  volume = {41},
  number = {19},
  pages = {3720--3736},
  issn = {1097-0258},
  doi = {10.1002/sim.9444},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9444},
  urldate = {2023-09-14},
  abstract = {Window mean survival time (WMST) evaluates the mean survival between a lower time horizon, τ0\textbackslash{} \textbackslash tau\_0 \textbackslash, and an upper time horizon, τ1\textbackslash{} \textbackslash tau\_1 \textbackslash. As a flexible extension of restricted mean survival time, specific clinically relevant windows of time can be assessed for survival difference accompanied by a communicable interpretation of estimates and tests. In its original application, WMST required the pre-specification of a window through the selection of appropriate window bounds, τ0\textbackslash{} \textbackslash tau\_0 \textbackslash{} and τ1\textbackslash{} \textbackslash tau\_1 \textbackslash. In the instance of severe window misspecification of τ0\textbackslash{} \textbackslash tau\_0 \textbackslash{} and τ1\textbackslash{} \textbackslash tau\_1 \textbackslash, the analysis may suffer from low power and a less meaningful interpretation. In this article, we introduce versatile tests whose procedures are based on the simultaneous use of multiple WMST test statistics that are asymptotically normal under the null hypothesis of no difference between two groups. Simulations are performed to examine the power of the tests in moderate sample sizes when the data are uncensored to heavily censored with a ramp-up enrollment period. The survival scenarios chosen for simulation are intended to imitate those which are commonly encountered in oncology, especially in trials involving immunotherapies. Implementation of the procedures is discussed in two real data examples for illustration. Functions for performing versatile WMST tests are provided in the survWMST package in R.},
  langid = {english},
  keywords = {logrank test,nonproportional hazards,notion,survival data,weighted rank test},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\paukner2022.pdf}
}

@article{paukner2023,
  title = {Designing Superiority Trials with Window Mean Survival Time as a Primary Endpoint},
  author = {Paukner, Mitchell and Chappell, Richard},
  date = {2023},
  journaltitle = {Statistics in Medicine},
  volume = {42},
  number = {15},
  pages = {2590--2599},
  issn = {1097-0258},
  doi = {10.1002/sim.9738},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9738},
  urldate = {2023-09-14},
  abstract = {Window mean survival time (WMST), a simple extension of restricted mean survival time (RMST), allows for clinicians to evaluate the mean survival difference between treatment groups in specific windows of time during the follow-up period of a trial. The advantages of WMST are numerous. Not only does it produce estimates of treatment effect that can be meaningfully interpreted, but also has power advantages over competing methods when hazards are non-proportional (NPH). WMST, like RMST, is currently underutilized due to clinicians' lack of familiarity with tests comparing mean survival times and the lack of tools to facilitate trial design with this endpoint. The aim of this article is to provide investigators with insights and software to design trials with WMST as the primary endpoint. Functions for performing power and sample size calculations are provided in the survWMST package in R available on GitHub.},
  langid = {english},
  keywords = {logrank test,non-proportional hazards,notion,restricted mean survival time,survival data},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\paukner2023.pdf;C\:\\Users\\david\\Zotero\\storage\\J8VSWFCK\\sim.html}
}

@article{peron2016,
  title = {The {{Net Chance}} of a {{Longer Survival}} as a {{Patient-Oriented Measure}} of {{Treatment Benefit}} in {{Randomized Clinical Trials}}},
  author = {Péron, Julien and Roy, Pascal and Ozenne, Brice and Roche, Laurent and Buyse, Marc},
  date = {2016-07-01},
  journaltitle = {JAMA Oncology},
  shortjournal = {JAMA Oncology},
  volume = {2},
  number = {7},
  pages = {901--905},
  issn = {2374-2437},
  doi = {10.1001/jamaoncol.2015.6359},
  url = {https://doi.org/10.1001/jamaoncol.2015.6359},
  urldate = {2023-09-26},
  abstract = {Time to events, or survival end points, are common end points in randomized clinical trials. They are usually analyzed under the assumption of proportional hazards, and the treatment effect is reported as a hazard ratio, which is neither an intuitive measure nor a meaningful one if the assumption of proportional hazards is not met.To demonstrate that a different measure of treatment effect, called the net chance of a longer survival, is a meaningful measure of treatment effect in clinical trials whether or not the assumption of proportional hazards is met.In this simulation study, the net chance of a longer survival by at least m months, where m months is considered clinically worthwhile and relevant to the patient, was calculated as the probability that a random patient in the treatment group has a longer survival by at least m months than does a random patient in the control group minus the probability of the opposite situation. The net chance of a longer survival is equal to zero if treatment does not differ from control and ranges from –100\% if all patients in the control group fare better than all patients in the treatment group up to 100\% in the opposite situation. We simulated data sets for realistic trials under various scenarios of proportional and nonproportional survival hazards and plotted the Kaplan-Meier survival curves as well as the net chance of a longer survival as a function of m. Data analysis was performed from August 14 to 18, 2015.The net chance of a longer survival calculated for values of m ranging from 0 to 40 months.When hazards are proportional, the net chance of a longer survival approaches zero as m increases. The net chance of a longer survival (Δ) was 13\% (95\% CI, 6.5\%-19.4\%; P\,\&lt;\,.001) when any survival difference was considered clinically relevant (m\,=\,0 months). When survival differences larger than 20 months were considered relevant (m\,=\,20), the net chance of a longer survival was very close to zero (Δ[20]\,=\,0.5\%; 95\% CI, –0.1\% to 1.1\%; P\,=\,.09). In contrast, when treatment effects are delayed or when some patients are cured by treatment, the net chance of a longer survival benefit remains high and tends to the cure rate. For crossing hazards, the Δ was negative (Δ\,=\,–6.9\%; 95\% CI, –14.0\% to –0.5\%; P\,=\,.047). However when large survival differences were considered (m\,=\,20), the Δ(m) was positive (Δ[20]\,=\,8.9\%; 95\% CI, 6.7\%-11.1\%; P\,\&lt;\,.001).The net chance of a longer survival is useful whether or not the assumption of proportional hazards is met in the analysis of survival end points and may be helpful as a measure of treatment benefit that has direct relevance to patients and health care professionals.},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\peron2016.pdf;C\:\\Users\\david\\Zotero\\storage\\NDJ4DK6R\\2517399.html}
}

@article{peron2018,
  title = {An Extension of Generalized Pairwise Comparisons for Prioritized Outcomes in the Presence of Censoring},
  author = {Péron, Julien and Buyse, Marc and Ozenne, Brice and Roche, Laurent and Roy, Pascal},
  date = {2018-04-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {27},
  number = {4},
  pages = {1230--1239},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280216658320},
  url = {https://doi.org/10.1177/0962280216658320},
  urldate = {2023-10-01},
  abstract = {Generalized pairwise comparisons have been proposed to permit a comprehensive assessment of several prioritized outcomes between two groups of observations. This procedure estimates Δ, the net chance of a better outcome with treatment than with control by comparing the patients outcomes among all possible pairs taking one patient from the treatment group and one patient from the control group. For time to event outcomes, the standard procedure of generalized pairwise comparisons is analogous to the Gehan’s modification of the Mann-Whitney test which is biased in presence of censored observation and less powerful than Efron’s modification of this test. We adapt Efron’s modification to generalized pairwise comparisons. We show how a pairwise contribution to Δ can be calculated from the estimates of the survival function in the presence of right-censored data. We performed a simulation study to assess the bias, the type I error and the power of the new procedure. The estimate of Δ with the new procedure is only slightly biased even in presence of heavy censoring. We also show how this bias can be corrected when only one time-to-event outcome is analyzed. The new procedure has higher power in most cases compared to the standard procedure.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\peron2018.pdf}
}

@article{peron2019,
  title = {Assessing {{Long-Term Survival Benefits}} of {{Immune Checkpoint Inhibitors Using}} the {{Net Survival Benefit}}},
  author = {Péron, Julien and Lambert, Alexandre and Munier, Stephane and Ozenne, Brice and Giai, Joris and Roy, Pascal and Dalle, Stéphane and Machingura, Abigirl and Maucort-Boulch, Delphine and Buyse, Marc},
  date = {2019-11-01},
  journaltitle = {JNCI: Journal of the National Cancer Institute},
  shortjournal = {JNCI: Journal of the National Cancer Institute},
  volume = {111},
  number = {11},
  pages = {1186--1191},
  issn = {0027-8874},
  doi = {10.1093/jnci/djz030},
  url = {https://doi.org/10.1093/jnci/djz030},
  urldate = {2023-10-01},
  abstract = {The treatment effect in survival analysis is commonly quantified as the hazard ratio, and tested statistically using the standard log-rank test. Modern anticancer immunotherapies are successful in a proportion of patients who remain alive even after a long-term follow-up. This new phenomenon induces a nonproportionality of the underlying hazards of death.The properties of the net survival benefit were illustrated using the dataset from a trial evaluating ipilimumab in metastatic melanoma. The net survival benefit was then investigated through simulated datasets under typical scenarios of proportional hazards, delayed treatment effect, and cure rate. The net survival benefit test was computed according to the value of the minimal survival difference considered clinically relevant. As comparators, the standard and the weighted log-rank tests were also performed.In the illustrative dataset, the net survival benefit favored ipilimumab [Δ(0) = 15.8\%, 95\% confidence interval = 4.6\% to 27.3\%, P\,=\,.006]. This favorable effect was maintained when the analysis was focused on long-term survival differences (eg, \&gt;12\,months, Δ(12) = 12.5\% (95\% confidence interval = 4.4\% to 20.6\%, P\,=\,.002). Under the scenarios of a delayed treatment effect and cure rate, the power of the net survival benefit test compared favorably to the standard log-rank test power and was comparable to the power of the weighted log-rank test for large values of the threshold of clinical relevance.The net long-term survival benefit is a measure of treatment effect that is meaningful whether or not hazards are proportional. The associated statistical test is more powerful than the standard log-rank test when a delayed treatment effect is anticipated.},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\peron2019.pdf;C\:\\Users\\david\\Zotero\\storage\\CDZFG398\\5369916.html}
}

@article{peto1972,
  title = {Asymptotically {{Efficient Rank Invariant Test Procedures}}},
  author = {Peto, Richard and Peto, Julian},
  date = {1972},
  journaltitle = {Journal of the Royal Statistical Society. Series A (General)},
  volume = {135},
  number = {2},
  eprint = {2344317},
  eprinttype = {jstor},
  pages = {185--207},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {0035-9238},
  doi = {10.2307/2344317},
  url = {https://www.jstor.org/stable/2344317},
  urldate = {2024-02-12},
  abstract = {Asymptotically efficient rank invariant test procedures for detecting differences between two groups of independent observations are derived. These are generalized to test between two groups of independent censored observations, to test between many groups of observations, and to test between groups after allowance for the effects of concomitant variables. One of these test procedures--the logrank--is particularly appropriate for comparing life tables, and can therefore be used in the analysis of clinical trials, industrial life-testing experiments and laboratory studies of animal carcinogenesis. It has greater local power than any other rank-invariant test procedure for detecting Lehmann-type differences between groups of independent observations subject to some right-censoring. The logrank test, although a rank test, can be presented in a format which exhibits the physical significance as well as the statistical significance of any important differences between groups of events.},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\peto1972.pdf}
}

@article{quartagno2020,
  title = {Handling an Uncertain Control Group Event Risk in Non-Inferiority Trials: Non-Inferiority Frontiers and the Power-Stabilising Transformation},
  shorttitle = {Handling an Uncertain Control Group Event Risk in Non-Inferiority Trials},
  author = {Quartagno, Matteo and Walker, A. Sarah and Babiker, Abdel G. and Turner, Rebecca M. and Parmar, Mahesh K. B. and Copas, Andrew and White, Ian R.},
  date = {2020-02-06},
  journaltitle = {Trials},
  shortjournal = {Trials},
  volume = {21},
  number = {1},
  pages = {145},
  issn = {1745-6215},
  doi = {10.1186/s13063-020-4070-4},
  url = {https://doi.org/10.1186/s13063-020-4070-4},
  urldate = {2023-09-11},
  abstract = {Non-inferiority trials are increasingly used to evaluate new treatments that are expected to have secondary advantages over standard of care, but similar efficacy on the primary outcome. When designing a non-inferiority trial with a binary primary outcome, the choice of effect measure for the non-inferiority margin (e.g. risk ratio or risk difference) has an important effect on sample size calculations; furthermore, if the control event risk observed is markedly different from that assumed, the trial can quickly lose power or the results become difficult to interpret.},
  keywords = {Non-inferiority,notion,Power-stabilising transformation,Resilience},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\quartagno2020.pdf;C\:\\Users\\david\\Zotero\\storage\\GWKQ9UCN\\s13063-020-4070-4.html}
}

@article{quartagno2021,
  title = {Why Restricted Mean Survival Time Methods Are Especially Useful for Non-Inferiority Trials},
  author = {Quartagno, Matteo and Morris, Tim P and White, Ian R},
  date = {2021-12-01},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {18},
  number = {6},
  pages = {743--745},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/17407745211045124},
  url = {https://doi.org/10.1177/17407745211045124},
  urldate = {2023-09-08},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\quartagno2021.pdf}
}

@article{quartagno2023,
  title = {A Comparison of Different Population-Level Summary Measures for Randomised Trials with Time-to-Event Outcomes, with a Focus on Non-Inferiority Trials},
  author = {Quartagno, Matteo and Morris, Tim P and Gilbert, Duncan C and Langley, Ruth E and Nankivell, Matthew G and Parmar, Mahesh KB and White, Ian R},
  date = {2023-06-20},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  pages = {17407745231181907},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/17407745231181907},
  url = {https://doi.org/10.1177/17407745231181907},
  urldate = {2023-09-08},
  abstract = {Background The population-level summary measure is a key component of the estimand for clinical trials with time-to-event outcomes. This is particularly the case for non-inferiority trials, because different summary measures imply different null hypotheses. Most trials are designed using the hazard ratio as summary measure, but recent studies suggested that the difference in restricted mean survival time might be more powerful, at least in certain situations. In a recent letter, we conjectured that differences between summary measures can be explained using the concept of the non-inferiority frontier and that for a fair simulation comparison of summary measures, the same analysis methods, making the same assumptions, should be used to estimate different summary measures. The aim of this article is to make such a comparison between three commonly used summary measures: hazard ratio, difference in restricted mean survival time and difference in survival at a fixed time point. In addition, we aim to investigate the impact of using an analysis method that assumes proportional hazards on the operating characteristics of a trial designed with any of the three summary measures. Methods We conduct a simulation study in the proportional hazards setting. We estimate difference in restricted mean survival time and difference in survival non-parametrically, without assuming proportional hazards. We also estimate all three measures parametrically, using flexible survival regression, under the proportional hazards assumption. Results Comparing the hazard ratio assuming proportional hazards with the other summary measures not assuming proportional hazards, relative performance varies substantially depending on the specific scenario. Fixing the summary measure, assuming proportional hazards always leads to substantial power gains compared to using non-parametric methods. Fixing the modelling approach to flexible parametric regression assuming proportional hazards, difference in restricted mean survival time is most often the most powerful summary measure among those considered. Conclusion When the hazards are likely to be approximately proportional, reflecting this in the analysis can lead to large gains in power for difference in restricted mean survival time and difference in survival. The choice of summary measure for a non-inferiority trial with time-to-event outcomes should be made on clinical grounds; when any of the three summary measures discussed here is equally justifiable, difference in restricted mean survival time is most often associated with the most powerful test, on the condition that it is estimated under proportional hazards.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\quartagno2023.pdf}
}

@article{rauch2018,
  title = {The {{Average Hazard Ratio}} – {{A Good Effect Measure}} for {{Time-to-event Endpoints}} When the {{Proportional Hazard Assumption}} Is {{Violated}}?},
  author = {Rauch, Geraldine and Brannath, Werner and Brückner, Matthias and Kieser, Meinhard},
  date = {2018-05},
  journaltitle = {Methods of Information in Medicine},
  shortjournal = {Methods Inf Med},
  volume = {57},
  number = {03},
  pages = {089--100},
  issn = {0026-1270, 2511-705X},
  doi = {10.3414/ME17-01-0058},
  url = {http://www.thieme-connect.de/DOI/DOI?10.3414/ME17-01-0058},
  urldate = {2023-08-21},
  abstract = {Methods: We conduct a systematic comparative study based on Monte-Carlo simulations and by a real clinical trial example. Results: Our results suggest that the properties of the average hazard ratio depend on the underlying weighting function. The two approaches to construct estimators and related tests show very similar performance for adequately chosen weights. In general, the average hazard ratio defines a more valid effect measure than the standard hazard ratio under non-proportional hazards and the corresponding tests provide a power advantage over the common logrank test. Conclusions: As non-proportional hazards are often met in clinical practice and the average hazard ratio tests often outperform the common logrank test, this approach should be used more routinely in applications.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\rauch2018.pdf}
}

@article{ristl2021,
  title = {Delayed Treatment Effects, Treatment Switching and Heterogeneous Patient Populations: {{How}} to Design and Analyze {{RCTs}} in Oncology},
  shorttitle = {Delayed Treatment Effects, Treatment Switching and Heterogeneous Patient Populations},
  author = {Ristl, Robin and Ballarini, Nicolás M and Götte, Heiko and Schüler, Armin and Posch, Martin and König, Franz},
  date = {2021},
  journaltitle = {Pharmaceutical Statistics},
  shortjournal = {Pharm Stat},
  volume = {20},
  number = {1},
  eprint = {32830428},
  eprinttype = {pmid},
  pages = {129--145},
  issn = {1539-1604},
  doi = {10.1002/pst.2062},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7818232/},
  urldate = {2023-08-30},
  abstract = {In the analysis of survival times, the logrank test and the Cox model have been established as key tools, which do not require specific distributional assumptions. Under the assumption of proportional hazards, they are efficient and their results can be interpreted unambiguously. However, delayed treatment effects, disease progression, treatment switchers or the presence of subgroups with differential treatment effects may challenge the assumption of proportional hazards. In practice, weighted logrank tests emphasizing either early, intermediate or late event times via an appropriate weighting function may be used to accommodate for an expected pattern of non‐proportionality. We model these sources of non‐proportional hazards via a mixture of survival functions with piecewise constant hazard. The model is then applied to study the power of unweighted and weighted log‐rank tests, as well as maximum tests allowing different time dependent weights. Simulation results suggest a robust performance of maximum tests across different scenarios, with little loss in power compared to the most powerful among the considered weighting schemes and huge power gain compared to unfavorable weights. The actual sources of non‐proportional hazards are not obvious from resulting populationwise survival functions, highlighting the importance of detailed simulations in the planning phase of a trial when assuming non‐proportional hazards.We provide the required tools in a software package, allowing to model data generating processes under complex non‐proportional hazard scenarios, to simulate data from these models and to perform the weighted logrank tests.},
  pmcid = {PMC7818232},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\ristl2021.pdf}
}

@unpublished{roychoudhury,
  title = {Robust {{Design}} and {{Analysis}} of {{Clinical Trials}} with {{Non-proportional Hazards}}: {{Methodology}} and {{Implementation}} with {{R}}},
  author = {Roychoudhury, Satrajit and Anderson, Keaven},
  langid = {english},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\roychoudhury.pdf}
}

@article{roychoudhury2020,
  title = {Bayesian Leveraging of Historical Control Data for a Clinical Trial with Time-to-Event Endpoint},
  author = {Roychoudhury, Satrajit and Neuenschwander, Beat},
  date = {2020},
  journaltitle = {Statistics in Medicine},
  volume = {39},
  number = {7},
  pages = {984--995},
  issn = {1097-0258},
  doi = {10.1002/sim.8456},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8456},
  urldate = {2023-10-10},
  abstract = {The recent 21st Century Cures Act propagates innovations to accelerate the discovery, development, and delivery of 21st century cures. It includes the broader application of Bayesian statistics and the use of evidence from clinical expertise. An example of the latter is the use of trial-external (or historical) data, which promises more efficient or ethical trial designs. We propose a Bayesian meta-analytic approach to leverage historical data for time-to-event endpoints, which are common in oncology and cardiovascular diseases. The approach is based on a robust hierarchical model for piecewise exponential data. It allows for various degrees of between trial-heterogeneity and for leveraging individual as well as aggregate data. An ovarian carcinoma trial and a non-small cell cancer trial illustrate methodological and practical aspects of leveraging historical data for the analysis and design of time-to-event trials.},
  langid = {english},
  keywords = {hierarchical model,Historical data,meta-analysis,notion,piecewise exponential model,prior distribution,time-to-event data},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\roychoudhury2020.pdf}
}

@article{roychoudhury2023,
  title = {Robust {{Design}} and {{Analysis}} of {{Clinical Trials With Nonproportional Hazards}}: {{A Straw Man Guidance From}} a {{Cross-Pharma Working Group}}},
  shorttitle = {Robust {{Design}} and {{Analysis}} of {{Clinical Trials With Nonproportional Hazards}}},
  author = {Roychoudhury, Satrajit and Anderson, Keaven M and Ye, Jiabu and Mukhopadhyay, Pralay},
  date = {2023-04-03},
  journaltitle = {Statistics in Biopharmaceutical Research},
  volume = {15},
  number = {2},
  pages = {280--294},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/19466315.2021.1874507},
  url = {https://doi.org/10.1080/19466315.2021.1874507},
  urldate = {2023-08-29},
  abstract = {Loss of power and clear description of treatment differences are key issues in designing and analyzing a clinical trial where nonproportional hazard (NPH) is a possibility. A log-rank test may be inefficient and interpretation of the hazard ratio estimated using Cox regression is potentially problematic. In this case, the current ICH E9 (R1) addendum would suggest designing a trial with a clinically relevant estimand, for example, expected life gain. This approach considers appropriate analysis methods for supporting the chosen estimand. However, such an approach is case specific and may suffer from lack of power for important choices of the underlying alternate hypothesis distribution. On the other hand, there may be a desire to have robust power under different deviations from proportional hazards. We would contend that no single number adequately describes treatment effect under NPH scenarios. The cross-pharma working group has proposed a combination test to provide robust power under a variety of alternative hypotheses. These can be specified for primary analysis at the design stage and methods appropriately accounting for combination test correlations are efficient for a variety of scenarios. We have provided design and analysis considerations based on a combination test under different NPH types and present a straw man proposal for practitioners. The proposals are illustrated with real life example and simulation.},
  keywords = {Clinical trial design,Combination test,Log-rank test,Nonproportional hazards,notion,Weighted log-rank test},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\roychoudhury22.pdf}
}

@article{royston2002,
  title = {Flexible Parametric Proportional-Hazards and Proportional-Odds Models for Censored Survival Data, with Application to Prognostic Modelling and Estimation of Treatment Effects},
  author = {Royston, Patrick and Parmar, Mahesh K. B.},
  date = {2002},
  journaltitle = {Statistics in Medicine},
  volume = {21},
  number = {15},
  pages = {2175--2197},
  issn = {1097-0258},
  doi = {10.1002/sim.1203},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.1203},
  urldate = {2023-09-06},
  abstract = {Modelling of censored survival data is almost always done by Cox proportional-hazards regression. However, use of parametric models for such data may have some advantages. For example, non-proportional hazards, a potential difficulty with Cox models, may sometimes be handled in a simple way, and visualization of the hazard function is much easier. Extensions of the Weibull and log-logistic models are proposed in which natural cubic splines are used to smooth the baseline log cumulative hazard and log cumulative odds of failure functions. Further extensions to allow non-proportional effects of some or all of the covariates are introduced. A hypothesis test of the appropriateness of the scale chosen for covariate effects (such as of treatment) is proposed. The new models are applied to two data sets in cancer. The results throw interesting light on the behaviour of both the hazard function and the hazard ratio over time. The tools described here may be a step towards providing greater insight into the natural history of the disease and into possible underlying causes of clinical events. We illustrate these aspects by using the two examples in cancer. Copyright © 2002 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {notion,parametric models,proportional hazards,proportional odds,splines,survival analysis},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\royston2002.pdf;C\:\\Users\\david\\Zotero\\storage\\FS8V8CA6\\sim.html}
}

@article{royston2011,
  title = {The Use of Restricted Mean Survival Time to Estimate the Treatment Effect in Randomized Clinical Trials When the Proportional Hazards Assumption Is in Doubt},
  author = {Royston, Patrick and Parmar, Mahesh K. B.},
  date = {2011},
  journaltitle = {Statistics in Medicine},
  volume = {30},
  number = {19},
  pages = {2409--2421},
  issn = {1097-0258},
  doi = {10.1002/sim.4274},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4274},
  urldate = {2023-08-25},
  abstract = {In most randomized clinical trials (RCTs) with a right-censored time-to-event outcome, the hazard ratio is taken as an appropriate measure of the effectiveness of a new treatment compared with a standard-of-care or control treatment. However, it has long been known that the hazard ratio is valid only under the proportional hazards (PH) assumption. This assumption is formally checked only rarely. Some recent trials, particularly the IPASS trial in lung cancer and the ICON7 trial in ovarian cancer, have alerted researchers to the possibility of gross non-PH, raising the critical question of how such data should be analyzed. Here, we propose the use of the restricted mean survival time at a prespecified, fixed time point as a useful general measure to report the difference between two survival curves. We describe different methods of estimating it and we illustrate its application to three RCTs in cancer. The examples are graded from a trial in kidney cancer in which there is no evidence of non-PH, to IPASS, where the opposite is clearly the case. We propose a simple, general scheme for the analysis of data from such RCTs. Key elements of our approach are Andersen's method of ‘pseudo-observations,’ which is based on the Kaplan–Meier estimate of the survival function, and Royston and Parmar's class of flexible parametric survival models, which may be used for analyzing data in the presence or in the absence of PH of the treatment effect. Copyright © 2011 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {flexible parametric survival models,hazard ratio,non-proportional hazards,notion,randomized controlled trials,restricted mean survival time,time-to-event data},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\royston2011.pdf;C\:\\Users\\david\\Zotero\\storage\\ND84ZYPK\\sim.html}
}

@article{royston2013,
  title = {Restricted Mean Survival Time: An Alternative to the Hazard Ratio for the Design and Analysis of Randomized Trials with a Time-to-Event Outcome},
  shorttitle = {Restricted Mean Survival Time},
  author = {Royston, Patrick and Parmar, Mahesh Kb},
  date = {2013-12},
  journaltitle = {BMC Medical Research Methodology},
  shortjournal = {BMC Med Res Methodol},
  volume = {13},
  number = {1},
  pages = {152},
  issn = {1471-2288},
  doi = {10.1186/1471-2288-13-152},
  url = {https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-13-152},
  urldate = {2023-08-21},
  abstract = {Background: Designs and analyses of clinical trials with a time-to-event outcome almost invariably rely on the hazard ratio to estimate the treatment effect and implicitly, therefore, on the proportional hazards assumption. However, the results of some recent trials indicate that there is no guarantee that the assumption will hold. Here, we describe the use of the restricted mean survival time as a possible alternative tool in the design and analysis of these trials. Methods: The restricted mean is a measure of average survival from time 0 to a specified time point, and may be estimated as the area under the survival curve up to that point. We consider the design of such trials according to a wide range of possible survival distributions in the control and research arm(s). The distributions are conveniently defined as piecewise exponential distributions and can be specified through piecewise constant hazards and time-fixed or time-dependent hazard ratios. Such designs can embody proportional or non-proportional hazards of the treatment effect. Results: We demonstrate the use of restricted mean survival time and a test of the difference in restricted means as an alternative measure of treatment effect. We support the approach through the results of simulation studies and in real examples from several cancer trials. We illustrate the required sample size under proportional and non-proportional hazards, also the significance level and power of the proposed test. Values are compared with those from the standard approach which utilizes the logrank test. Conclusions: We conclude that the hazard ratio cannot be recommended as a general measure of the treatment effect in a randomized controlled trial, nor is it always appropriate when designing a trial. Restricted mean survival time may provide a practical way forward and deserves greater attention.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\royston2013.pdf}
}

@article{royston2014,
  title = {An Approach to Trial Design and Analysis in the Era of Non-Proportional Hazards of the Treatment Effect},
  author = {Royston, Patrick and Parmar, Mahesh Kb},
  date = {2014-12},
  journaltitle = {Trials},
  shortjournal = {Trials},
  volume = {15},
  number = {1},
  pages = {314},
  issn = {1745-6215},
  doi = {10.1186/1745-6215-15-314},
  url = {https://trialsjournal.biomedcentral.com/articles/10.1186/1745-6215-15-314},
  urldate = {2023-08-21},
  abstract = {Background: Most randomized controlled trials with a time-to-event outcome are designed and analysed under the proportional hazards assumption, with a target hazard ratio for the treatment effect in mind. However, the hazards may be non-proportional. We address how to design a trial under such conditions, and how to analyse the results. Methods: We propose to extend the usual approach, a logrank test, to also include the Grambsch-Therneau test of proportional hazards. We test the resulting composite null hypothesis using a joint test for the hazard ratio and for time-dependent behaviour of the hazard ratio. We compute the power and sample size for the logrank test under proportional hazards, and from that we compute the power of the joint test. For the estimation of relevant quantities from the trial data, various models could be used; we advocate adopting a pre-specified flexible parametric survival model that supports time-dependent behaviour of the hazard ratio. Results: We present the mathematics for calculating the power and sample size for the joint test. We illustrate the methodology in real data from two randomized trials, one in ovarian cancer and the other in treating cellulitis. We show selected estimates and their uncertainty derived from the advocated flexible parametric model. We demonstrate in a small simulation study that when a treatment effect either increases or decreases over time, the joint test can outperform the logrank test in the presence of both patterns of non-proportional hazards. Conclusions: Those designing and analysing trials in the era of non-proportional hazards need to acknowledge that a more complex type of treatment effect is becoming more common. Our method for the design of the trial retains the tools familiar in the standard methodology based on the logrank test, and extends it to incorporate a joint test of the null hypothesis with power against non-proportional hazards. For the analysis of trial data, we propose the use of a pre-specified flexible parametric model that can represent a time-dependent hazard ratio if one is present.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\royston2014.pdf}
}

@article{royston2020,
  title = {A Simulation Study Comparing the Power of Nine Tests of the Treatment Effect in Randomized Controlled Trials with a Time-to-Event Outcome},
  author = {Royston, Patrick and B. Parmar, Mahesh K.},
  date = {2020-04-06},
  journaltitle = {Trials},
  shortjournal = {Trials},
  volume = {21},
  number = {1},
  pages = {315},
  issn = {1745-6215},
  doi = {10.1186/s13063-020-4153-2},
  url = {https://doi.org/10.1186/s13063-020-4153-2},
  urldate = {2023-08-30},
  abstract = {The logrank test is routinely applied to design and analyse randomized controlled trials (RCTs) with time-to-event outcomes. Sample size and power calculations assume the treatment effect follows proportional hazards (PH). If the PH assumption is false, power is reduced and interpretation of the hazard ratio (HR) as the estimated treatment effect is compromised. Using statistical simulation, we investigated the type 1 error and power of the logrank (LR)test and eight alternatives. We aimed to identify test(s) that improve power with three types of non-proportional hazards (non-PH): early, late or near-PH treatment effects.},
  keywords = {Hazard ratio,Logrank test,Non-proportional hazards,notion,Power,Randomized controlled trials,Robustness,Simulation,Time-to-event outcome,Versatile test},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\royston2020.pdf;C\:\\Users\\david\\Zotero\\storage\\QL5HFM2F\\s13063-020-4153-2.html}
}

@article{rufibach2019,
  title = {Treatment Effect Quantification for Time-to-Event Endpoints–{{Estimands}}, Analysis Strategies, and Beyond},
  author = {Rufibach, Kaspar},
  date = {2019},
  journaltitle = {Pharmaceutical Statistics},
  volume = {18},
  number = {2},
  pages = {145--165},
  issn = {1539-1612},
  doi = {10.1002/pst.1917},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.1917},
  urldate = {2023-08-30},
  abstract = {A draft addendum to ICH E9 has been released for public consultation in August 2017. The addendum focuses on two topics particularly relevant for randomized confirmatory clinical trials: estimands and sensitivity analyses. The need to amend ICH E9 grew out of the realization of a lack of alignment between the objectives of a clinical trial stated in the protocol and the accompanying quantification of the “treatment effect” reported in a regulatory submission. We embed time-to-event endpoints in the estimand framework and discuss how the four estimand attributes described in the addendum apply to time-to-event endpoints. We point out that if the proportional hazards assumption is not met, the estimand targeted by the most prevalent methods used to analyze time-to-event endpoints, logrank test, and Cox regression depends on the censoring distribution. We discuss for a large randomized clinical trial how the analyses for the primary and secondary endpoints as well as the sensitivity analyses actually performed in the trial can be seen in the context of the addendum. To the best of our knowledge, this is the first attempt to do so for a trial with a time-to-event endpoint. Questions that remain open with the addendum for time-to-event endpoints and beyond are formulated, and recommendations for planning of future trials are given. We hope that this will provide a contribution to developing a common framework based on the final version of the addendum that can be applied to design, protocols, statistical analysis plans, and clinical study reports in the future.},
  langid = {english},
  keywords = {causal inference,censoring,estimand,notion,randomized clinical trial,sensitivity analysis,time-to-event},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\rufibach2019.pdf;C\:\\Users\\david\\Zotero\\storage\\AMSTAS7S\\pst.html}
}

@article{saad2018,
  title = {Understanding and {{Communicating Measures}} of {{Treatment Effect}} on {{Survival}}: {{Can We Do Better}}?},
  shorttitle = {Understanding and {{Communicating Measures}} of {{Treatment Effect}} on {{Survival}}},
  author = {Saad, Everardo D and Zalcberg, John R and Péron, Julien and Coart, Elisabeth and Burzykowski, Tomasz and Buyse, Marc},
  date = {2018-03-01},
  journaltitle = {JNCI: Journal of the National Cancer Institute},
  shortjournal = {JNCI: Journal of the National Cancer Institute},
  volume = {110},
  number = {3},
  pages = {232--240},
  issn = {0027-8874},
  doi = {10.1093/jnci/djx179},
  url = {https://doi.org/10.1093/jnci/djx179},
  urldate = {2023-10-01},
  abstract = {Time-to-event end points are the most frequent primary end points in phase III oncology trials, both in the adjuvant and advanced settings. The evaluation of these end points is important to inform clinical practice. However, although different measures can be used to describe the effect of treatment on these end points, we believe that any treatment benefit in a given trial is best reported using various absolute and relative measures. Our goal is to help clinicians understand the strengths and limitations of the traditional and novel measures used to denote the effect of treatment in randomized trials. Although none of these measures can reliably predict the outcome of individual patients, some measures could be added to the commonly used hazard ratio to provide a more patient-oriented assessment of treatment benefit. In particular, the difference of mean survival times quantifies the average survival benefit for a patient receiving a new treatment compared with a patient treated with standard of care, whereas the net benefit quantifies the probability of a patient receiving the new treatment to live longer by at least m months (for any number of months m of interest) than a patient receiving the standard treatment. We encourage statisticians and clinical scientists to include various measures of treatment benefit in the reports of phase III trials, acknowledging that different clinical situations may call for different measures of treatment effect. By using the various available measures, we may better inform ourselves and communicate results to our patients.},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\saad2018.pdf;C\:\\Users\\david\\Zotero\\storage\\FYKJSBDJ\\4259425.html}
}

@article{sachs2022,
  title = {Event {{History Regression}} with {{Pseudo-Observations}}: {{Computational Approaches}} and an {{Implementation}} in {{R}}},
  shorttitle = {Event {{History Regression}} with {{Pseudo-Observations}}},
  author = {Sachs, Michael C. and Gabriel, Erin E.},
  date = {2022-05-02},
  journaltitle = {Journal of Statistical Software},
  volume = {102},
  pages = {1--34},
  issn = {1548-7660},
  doi = {10.18637/jss.v102.i09},
  url = {https://doi.org/10.18637/jss.v102.i09},
  urldate = {2023-11-01},
  abstract = {Due to tradition and ease of estimation, the vast majority of clinical and epidemiological papers with time-to-event data report hazard ratios from Cox proportional hazards regression models. Although hazard ratios are well known, they can be difficult to interpret, particularly as causal contrasts, in many settings. Nonparametric or fully parametric estimators allow for the direct estimation of more easily causally interpretable estimands such as the cumulative incidence and restricted mean survival. However, modeling these quantities as functions of covariates is limited to a few categorical covariates with nonparametric estimators, and often requires simulation or numeric integration with parametric estimators. Combining pseudo-observations based on non-parametric estimands with parametric regression on the pseudo-observations allows for the best of these two approaches and has many nice properties. In this paper, we develop a user friendly, easy to understand way of doing event history regression for the cumulative incidence and the restricted mean survival, using the pseudo-observation framework for estimation. The interface uses the well known formulation of a generalized linear model and allows for features including plotting of residuals, the use of sampling weights, and correct variance estimation.},
  langid = {english},
  keywords = {competing risks,notion,pseudo observations,R,regression,survival analysis},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\sachs2022.pdf}
}

@article{schemper2009,
  title = {The Estimation of Average Hazard Ratios by Weighted {{Cox}} Regression: {{THE ESTIMATION OF AHR BY WEIGHTED COX REGRESSION}}},
  shorttitle = {The Estimation of Average Hazard Ratios by Weighted {{Cox}} Regression},
  author = {Schemper, Michael and Wakounig, Samo and Heinze, Georg},
  date = {2009-08-30},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Statist. Med.},
  volume = {28},
  number = {19},
  pages = {2473--2489},
  issn = {02776715},
  doi = {10.1002/sim.3623},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/sim.3623},
  urldate = {2023-08-21},
  abstract = {Often the effect of at least one of the prognostic factors in a Cox regression model changes over time, which violates the proportional hazards assumption of this model. As a consequence, the average hazard ratio for such a prognostic factor is under- or overestimated. While there are several methods to appropriately cope with non-proportional hazards, in particular by including parameters for time-dependent effects, weighted estimation in Cox regression is a parsimonious alternative without additional parameters. The methodology, which extends the weighted k-sample logrank tests of the Tarone-Ware scheme to models with multiple, binary and continuous covariates, has been introduced in the nineties of the last century and is further developed and re-evaluated in this contribution. The notion of an average hazard ratio is defined and its connection to the effect size measure P(X},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\schemper2009.pdf}
}

@article{schmidli2014,
  title = {Robust Meta-Analytic-Predictive Priors in Clinical Trials with Historical Control Information},
  author = {Schmidli, Heinz and Gsteiger, Sandro and Roychoudhury, Satrajit and O'Hagan, Anthony and Spiegelhalter, David and Neuenschwander, Beat},
  date = {2014},
  journaltitle = {Biometrics},
  volume = {70},
  number = {4},
  pages = {1023--1032},
  issn = {1541-0420},
  doi = {10.1111/biom.12242},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12242},
  urldate = {2023-10-10},
  abstract = {Historical information is always relevant for clinical trial design. Additionally, if incorporated in the analysis of a new trial, historical data allow to reduce the number of subjects. This decreases costs and trial duration, facilitates recruitment, and may be more ethical. Yet, under prior-data conflict, a too optimistic use of historical data may be inappropriate. We address this challenge by deriving a Bayesian meta-analytic-predictive prior from historical data, which is then combined with the new data. This prospective approach is equivalent to a meta-analytic-combined analysis of historical and new data if parameters are exchangeable across trials. The prospective Bayesian version requires a good approximation of the meta-analytic-predictive prior, which is not available analytically. We propose two- or three-component mixtures of standard priors, which allow for good approximations and, for the one-parameter exponential family, straightforward posterior calculations. Moreover, since one of the mixture components is usually vague, mixture priors will often be heavy-tailed and therefore robust. Further robustness and a more rapid reaction to prior-data conflicts can be achieved by adding an extra weakly-informative mixture component. Use of historical prior information is particularly attractive for adaptive trials, as the randomization ratio can then be changed in case of prior-data conflict. Both frequentist operating characteristics and posterior summaries for various data scenarios show that these designs have desirable properties. We illustrate the methodology for a phase II proof-of-concept trial with historical controls from four studies. Robust meta-analytic-predictive priors alleviate prior-data conflicts ' they should encourage better and more frequent use of historical data in clinical trials.},
  langid = {english},
  keywords = {Adaptive design,Adaptive randomization,Bayesian inference,Clinical trials,Exponential family,Meta-analysis,Mixture distribution,notion,Robustness},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\schmidli2014.pdf;C\:\\Users\\david\\Zotero\\storage\\5E3SSZA7\\biom.html}
}

@article{stensrud2019,
  title = {Limitations of Hazard Ratios in Clinical Trials},
  author = {Stensrud, Mats J and Aalen, John M and Aalen, Odd O and Valberg, Morten},
  date = {2019-05-01},
  journaltitle = {European Heart Journal},
  shortjournal = {European Heart Journal},
  volume = {40},
  number = {17},
  pages = {1378--1383},
  issn = {0195-668X},
  doi = {10.1093/eurheartj/ehy770},
  url = {https://doi.org/10.1093/eurheartj/ehy770},
  urldate = {2023-09-06},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\stensrud2019.pdf;C\:\\Users\\david\\Zotero\\storage\\YTIHABWE\\5219649.html}
}

@article{stensrud2019a,
  title = {On Null Hypotheses in Survival Analysis},
  author = {Stensrud, Mats J. and Røysland, Kjetil and Ryalen, Pål C.},
  date = {2019},
  journaltitle = {Biometrics},
  volume = {75},
  number = {4},
  pages = {1276--1287},
  issn = {1541-0420},
  doi = {10.1111/biom.13102},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13102},
  urldate = {2023-09-29},
  abstract = {The conventional nonparametric tests in survival analysis, such as the log-rank test, assess the null hypothesis that the hazards are equal at all times. However, hazards are hard to interpret causally, and other null hypotheses are more relevant in many scenarios with survival outcomes. To allow for a wider range of null hypotheses, we present a generic approach to define test statistics. This approach utilizes the fact that a wide range of common parameters in survival analysis can be expressed as solutions of differential equations. Thereby, we can test hypotheses based on survival parameters that solve differential equations driven by cumulative hazards, and it is easy to implement the tests on a computer. We present simulations, suggesting that our tests perform well for several hypotheses in a range of scenarios. As an illustration, we apply our tests to evaluate the effect of adjuvant chemotherapies in patients with colon cancer, using data from a randomized controlled trial.},
  langid = {english},
  keywords = {causal inference,failure time analysis,hazard rates,hypothesis testing,notion,time to event},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\stensrud2019a.pdf}
}

@article{tai2023,
  title = {Two-Sample Inference Procedures under Nonproportional Hazards},
  author = {Tai, Yi-Cheng and Wang, Weijing and Wells, Martin T.},
  date = {2023},
  journaltitle = {Pharmaceutical Statistics},
  volume = {n/a},
  number = {n/a},
  issn = {1539-1612},
  doi = {10.1002/pst.2324},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/pst.2324},
  urldate = {2023-09-26},
  abstract = {We introduce a new two-sample inference procedure to assess the relative performance of two groups over time. Our model-free method does not assume proportional hazards, making it suitable for scenarios where nonproportional hazards may exist. Our procedure includes a diagnostic tau plot to identify changes in hazard timing and a formal inference procedure. The tau-based measures we develop are clinically meaningful and provide interpretable estimands to summarize the treatment effect over time. Our proposed statistic is a U-statistic and exhibits a martingale structure, allowing us to construct confidence intervals and perform hypothesis testing. Our approach is robust with respect to the censoring distribution. We also demonstrate how our method can be applied for sensitivity analysis in scenarios with missing tail information due to insufficient follow-up. Without censoring, Kendall's tau estimator we propose reduces to the Wilcoxon-Mann–Whitney statistic. We evaluate our method using simulations to compare its performance with the restricted mean survival time and log-rank statistics. We also apply our approach to data from several published oncology clinical trials where nonproportional hazards may exist.},
  langid = {english},
  keywords = {crossing survival functions,delayed treatment effect,interpretable estimand,IPCW,Kendall's tau,MaxCombo,nonproportional hazards,notion,restricted mean survival time,sensitivity analysis},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\tai2023.pdf;C\:\\Users\\david\\Zotero\\storage\\HK2G8TS7\\pst.html}
}

@article{tang2021,
  title = {Some New Confidence Intervals for {{Kaplan-Meier}} Based Estimators from One and Two Sample Survival Data},
  author = {Tang, Yongqiang},
  date = {2021},
  journaltitle = {Statistics in Medicine},
  volume = {40},
  number = {23},
  pages = {4961--4976},
  issn = {1097-0258},
  doi = {10.1002/sim.9105},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9105},
  urldate = {2023-10-04},
  abstract = {The restricted mean survival time (RMST) has been popularly used to assess the treatment effect in survival trials. Greenwood's formula is often used to estimate the variance of RMST, and the resulting Wald confidence interval (CI) tends to be liberal in small and moderate samples. We propose the empirical likelihood ratio, score-type, and loglog transformed CIs for RMST in a single sample. The method of variance estimates recovery technique is used to derive the CIs for the difference and ratio parameters in the two sample inference. A variance estimate, which assumes equal survival curves, but possibly different censoring rates in the two groups, is proposed for comparing two groups. The new variance estimate shows excellent performance in testing for superiority, and also works well for a noninferiority test with a small margin, and for the interval estimation when the two survival curves are close. We use similar techniques to construct CIs for comparing two milestone survival probabilities. Numerical examples are used to assess these interval estimation methods.},
  langid = {english},
  keywords = {empirical likelihood ratio test,Fieller confidence interval,loglog transformation,method of variance estimates recovery,restricted mean survival time},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\tang2021.pdf;C\:\\Users\\david\\Zotero\\storage\\K94DVHF8\\sim.html}
}

@article{taylor2015,
  title = {Statistical Learning and Selective Inference},
  author = {Taylor, Jonathan and Tibshirani, Robert J.},
  date = {2015-06-23},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {25},
  pages = {7629--7634},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1507583112},
  url = {https://www.pnas.org/doi/10.1073/pnas.1507583112},
  urldate = {2023-10-11},
  abstract = {We describe the problem of “selective inference.” This addresses the following challenge: Having mined a set of data to find potential associations, how do we properly assess the strength of these associations? The fact that we have “cherry-picked”—searched for the strongest associations—means that we must set a higher bar for declaring significant the associations that we see. This challenge becomes more important in the era of big data and complex statistical modeling. The cherry tree (dataset) can be very large and the tools for cherry picking (statistical learning methods) are now very sophisticated. We describe some recent new developments in selective inference and illustrate their use in forward stepwise regression, the lasso, and principal components analysis.},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\taylor2015.pdf}
}

@online{thurow2023,
  title = {How to {{Simulate Realistic Survival Data}}? {{A Simulation Study}} to {{Compare Realistic Simulation Models}}},
  shorttitle = {How to {{Simulate Realistic Survival Data}}?},
  author = {Thurow, Maria and Dormuth, Ina and Sauer, Christina and Ditzhaus, Marc and Pauly, Markus},
  date = {2023-08-15},
  eprint = {2308.07842},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2308.07842},
  urldate = {2023-08-31},
  abstract = {In statistics, it is important to have realistic data sets available for a particular context to allow an appropriate and objective method comparison. For many use cases, benchmark data sets for method comparison are already available online. However, in most medical applications and especially for clinical trials in oncology, there is a lack of adequate benchmark data sets, as patient data can be sensitive and therefore cannot be published. A potential solution for this are simulation studies. However, it is sometimes not clear, which simulation models are suitable for generating realistic data. A challenge is that potentially unrealistic assumptions have to be made about the distributions. Our approach is to use reconstructed benchmark data sets as a basis for the simulations, which has the following advantages: the actual properties are known and more realistic data can be simulated. There are several possibilities to simulate realistic data from benchmark data sets. We investigate simulation models based upon kernel density estimation, fitted distributions, case resampling and conditional bootstrapping. In order to make recommendations on which models are best suited for a specific survival setting, we conducted a comparative simulation study. Since it is not possible to provide recommendations for all possible survival settings in a single paper, we focus on providing realistic simulation models for two-armed phase III lung cancer studies. To this end we reconstructed benchmark data sets from recent studies. We used the runtime and different accuracy measures (effect sizes and p-values) as criteria for comparison.},
  langid = {english},
  pubstate = {preprint},
  keywords = {notion,Statistics - Applications},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\thurow2023.pdf}
}

@article{tian2018,
  title = {Efficiency of {{Two Sample Tests}} via the {{Restricted Mean Survival Time}} for {{Analyzing Event Time Observations}}},
  author = {Tian, Lu and Fu, Haoda and Ruberg, Stephen J. and Uno, Hajime and family=Wei, given=LJ, given-i=LJ},
  date = {2018-06},
  journaltitle = {Biometrics},
  shortjournal = {Biometrics},
  volume = {74},
  number = {2},
  eprint = {28901017},
  eprinttype = {pmid},
  pages = {694--702},
  issn = {0006-341X},
  doi = {10.1111/biom.12770},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5847424/},
  urldate = {2023-08-31},
  abstract = {In comparing two treatments with the event time observations, the hazard ratio (HR) estimate is routinely used to quantify the treatment difference. However, this model dependent estimate may be difficult to interpret clinically especially when the proportional hazards (PH) assumption is violated. An alternative estimation procedure for treatment efficacy based on the restricted means survival time or t-year mean survival time (t-MST) has been discussed extensively in the statistical and clinical literature. On the other hand, a statistical test via the HR or its asymptotically equivalent counterpart, the logrank test, is asymptotically distribution-free. In this paper, we assess the relative efficiency of the hazard ratio and t-MST tests with respect to the statistical power under various PH and non-PH models theoretically and empirically. When the PH assumption is valid, the t-MST test performs almost as well as the HR test. For non-PH models, the t-MST test can substantially outperform its HR counterpart. On the other hand, the HR test can be powerful when the true difference of two survival functions is quite large at end but not the beginning of the study. Unfortunately, for this case, the HR estimate may not have a simple clinical interpretation for the treatment effect due to the violation of the PH assumption.},
  pmcid = {PMC5847424},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\tian22.pdf}
}

@article{tian2020,
  title = {On the Empirical Choice of the Time Window for Restricted Mean Survival Time},
  author = {Tian, Lu and Jin, Hua and Uno, Hajime and Lu, Ying and Huang, Bo and Anderson, Keaven M. and Wei, L. J.},
  date = {2020-12},
  journaltitle = {Biometrics},
  shortjournal = {Biometrics},
  volume = {76},
  number = {4},
  eprint = {32061098},
  eprinttype = {pmid},
  pages = {1157--1166},
  issn = {1541-0420},
  doi = {10.1111/biom.13237},
  abstract = {The t-year mean survival or restricted mean survival time (RMST) has been used as an appealing summary of the survival distribution within a time window [0, t]. RMST is the patient's life expectancy until time t and can be estimated nonparametrically by the area under the Kaplan-Meier curve up to t. In a comparative study, the difference or ratio of two RMSTs has been utilized to quantify the between-group-difference as a clinically interpretable alternative summary to the hazard ratio. The choice of the time window [0, t] may be prespecified at the design stage of the study based on clinical considerations. On the other hand, after the survival data have been collected, the choice of time point t could be data-dependent. The standard inferential procedures for the corresponding RMST, which is also data-dependent, ignore this subtle yet important issue. In this paper, we clarify how to make inference about a random "parameter." Moreover, we demonstrate that under a rather mild condition on the censoring distribution, one can make inference about the RMST up to t, where t is less than or even equal to the largest follow-up time (either observed or censored) in the study. This finding reduces the subjectivity of the choice of t empirically. The proposal is illustrated with the survival data from a primary biliary cirrhosis study, and its finite sample properties are investigated via an extensive simulation~study.},
  langid = {english},
  pmcid = {PMC8687138},
  keywords = {Computer Simulation,hazard ratio,Humans,Kaplan-Meier estimator,Life Expectancy,logrank test,notion,Proportional Hazards Models,RMST,Survival Rate},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\tian2020.pdf}
}

@article{trinquart2016,
  title = {Comparison of {{Treatment Effects Measured}} by the {{Hazard Ratio}} and by the {{Ratio}} of {{Restricted Mean Survival Times}} in {{Oncology Randomized Controlled Trials}}},
  author = {Trinquart, Ludovic and Jacot, Justine and Conner, Sarah C. and Porcher, Raphaël},
  date = {2016-05-20},
  journaltitle = {Journal of Clinical Oncology},
  shortjournal = {JCO},
  volume = {34},
  number = {15},
  pages = {1813--1819},
  issn = {0732-183X, 1527-7755},
  doi = {10.1200/JCO.2015.64.2488},
  url = {https://ascopubs.org/doi/10.1200/JCO.2015.64.2488},
  urldate = {2023-08-30},
  abstract = {Purpose We aimed to compare empirically the treatment effects measured by the hazard ratio (HR) and by the difference (and ratio) of restricted mean survival times (RMST) in oncology randomized trials. Methods We selected oncology randomized controlled trials from five leading journals during the last 6 months of 2014. We reconstructed individual patient data for one time-to-event outcome from each trial, preferably the primary outcome. We reanalyzed each trial and compared the treatment effect estimated by the HR with that by the difference (and ratio) of RMST. We estimated an average ratio of the HR to the ratio of RMST; an average ratio less than one indicates more optimistic assessments with HRs. Results We analyzed 54 randomized controlled trials totaling 33,212 patients. The selected outcome was overall survival in 21 (39\%) trials. There was evidence of nonproportionality of hazards in 13 (24\%) trials. The HR and RMST-based measures were in agreement regarding the statistical significance of the effect, except in one case. The median HR was 0.84 (Q1 to Q3 range, 0.67 to 0.97) and the median difference in RMST was 1.12 months (range, 0.22 to 2.75 months). The average ratio of the HR to the ratio of RMST was 1.11 (95\% CI, 1.07 to 1.15), with substantial between-trial variability (I2 = 86\%). Results were consistent by outcome type (overall survival v other outcomes) and whether the proportional hazard assumption held or not. Conclusion On average, the HR provided significantly larger treatment effect estimates than the ratio of RMST. The HR may seem large when the absolute effect is small. RMST-based measures should be routinely reported in randomized trials with time-to-event outcomes.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\Zotero\storage\Z47BBI9Y\Trinquart et al. - 2016 - Comparison of Treatment Effects Measured by the Ha.pdf}
}

@article{turner2023,
  title = {Practical Approaches to {{Bayesian}} Sample Size Determination in Non-Inferiority Trials with Binary Outcomes},
  author = {Turner, Rebecca M. and Clements, Michelle N. and Quartagno, Matteo and Cornelius, Victoria and Cro, Suzie and Ford, Deborah and Tweed, Conor D. and Walker, A. Sarah and White, Ian R.},
  date = {2023},
  journaltitle = {Statistics in Medicine},
  volume = {42},
  number = {8},
  pages = {1127--1138},
  issn = {1097-0258},
  doi = {10.1002/sim.9661},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9661},
  urldate = {2023-09-14},
  abstract = {Bayesian analysis of a non-inferiority trial is advantageous in allowing direct probability statements to be made about the relative treatment difference rather than relying on an arbitrary and often poorly justified non-inferiority margin. When the primary analysis will be Bayesian, a Bayesian approach to sample size determination will often be appropriate for consistency with the analysis. We demonstrate three Bayesian approaches to choosing sample size for non-inferiority trials with binary outcomes and review their advantages and disadvantages. First, we present a predictive power approach for determining sample size using the probability that the trial will produce a convincing result in the final analysis. Next, we determine sample size by considering the expected posterior probability of non-inferiority in the trial. Finally, we demonstrate a precision-based approach. We apply these methods to a non-inferiority trial in antiretroviral therapy for treatment of HIV-infected children. A predictive power approach would be most accessible in practical settings, because it is analogous to the standard frequentist approach. Sample sizes are larger than with frequentist calculations unless an informative analysis prior is specified, because appropriate allowance is made for uncertainty in the assumed design parameters, ignored in frequentist calculations. An expected posterior probability approach will lead to a smaller sample size and is appropriate when the focus is on estimating posterior probability rather than on testing. A precision-based approach would be useful when sample size is restricted by limits on recruitment or costs, but it would be difficult to decide on sample size using this approach alone.},
  langid = {english},
  keywords = {Bayesian methods,non-inferiority trials,notion,sample size,trial design},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\turner2023.pdf;C\:\\Users\\david\\Zotero\\storage\\7PQZL5AF\\sim.html}
}

@article{uno2014,
  title = {Moving {{Beyond}} the {{Hazard Ratio}} in {{Quantifying}} the {{Between-Group Difference}} in {{Survival Analysis}}},
  author = {Uno, Hajime and Claggett, Brian and Tian, Lu and Inoue, Eisuke and Gallo, Paul and Miyata, Toshio and Schrag, Deborah and Takeuchi, Masahiro and Uyama, Yoshiaki and Zhao, Lihui and Skali, Hicham and Solomon, Scott and Jacobus, Susanna and Hughes, Michael and Packer, Milton and Wei, Lee-Jen},
  date = {2014-08-01},
  journaltitle = {Journal of Clinical Oncology},
  shortjournal = {JCO},
  volume = {32},
  number = {22},
  pages = {2380--2385},
  issn = {0732-183X, 1527-7755},
  doi = {10.1200/JCO.2014.55.2208},
  url = {https://ascopubs.org/doi/10.1200/JCO.2014.55.2208},
  urldate = {2023-08-21},
  abstract = {In a longitudinal clinical study to compare two groups, the primary end point is often the time to a specific event (eg, disease progression, death). The hazard ratio estimate is routinely used to empirically quantify the between-group difference under the assumption that the ratio of the two hazard functions is approximately constant over time. When this assumption is plausible, such a ratio estimate may capture the relative difference between two survival curves. However, the clinical meaning of such a ratio estimate is difficult, if not impossible, to interpret when the underlying proportional hazards assumption is violated (ie, the hazard ratio is not constant over time). Although this issue has been studied extensively and various alternatives to the hazard ratio estimator have been discussed in the statistical literature, such crucial information does not seem to have reached the broader community of health science researchers. In this article, we summarize several critical concerns regarding this conventional practice and discuss various well-known alternatives for quantifying the underlying differences between groups with respect to a time-to-event end point. The data from three recent cancer clinical trials, which reflect a variety of scenarios, are used throughout to illustrate our discussions. When there is not sufficient information about the profile of the between-group difference at the design stage of the study, we encourage practitioners to consider a prespecified, clinically meaningful, model-free measure for quantifying the difference and to use robust estimation procedures to draw primary inferences.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\uno2014.pdf}
}

@article{uno2015,
  title = {Alternatives to {{Hazard Ratios}} for {{Comparing}} the {{Efficacy}} or {{Safety}} of {{Therapies}} in {{Noninferiority Studies}}},
  author = {Uno, Hajime and Wittes, Janet and Fu, Haoda and Solomon, Scott D. and Claggett, Brian and Tian, Lu and Cai, Tianxi and Pfeffer, Marc A. and Evans, Scott R. and Wei, Lee-Jen},
  date = {2015-07-21},
  journaltitle = {Annals of Internal Medicine},
  shortjournal = {Ann Intern Med},
  volume = {163},
  number = {2},
  pages = {127--134},
  publisher = {American College of Physicians},
  issn = {0003-4819},
  doi = {10.7326/M14-1741},
  url = {https://www.acpjournals.org/doi/10.7326/M14-1741},
  urldate = {2023-08-29},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\uno2015.pdf}
}

@article{uno2019,
  title = {Assessing {{Clinical Equivalence}} in {{Oncology Biosimilar Trials With Time-to-Event Outcomes}}},
  author = {Uno, Hajime and Schrag, Deborah and Kim, Dae Hyun and Tang, Dejun and Tian, Lu and Rugo, Hope S and Wei, Lee-Jen},
  date = {2019-08-01},
  journaltitle = {JNCI Cancer Spectrum},
  shortjournal = {JNCI Cancer Spectr},
  volume = {3},
  number = {4},
  eprint = {32337484},
  eprinttype = {pmid},
  pages = {pkz058},
  issn = {2515-5091},
  doi = {10.1093/jncics/pkz058},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7050006/},
  urldate = {2023-09-08},
  abstract = {A typical biosimilar study in oncology uses the overall response evaluated at a specific time point as the primary endpoint, which is generally acceptable regulatorily, to assess clinical equivalence between a biosimilar and its reference product. The standard primary endpoint for evaluating an anticancer therapy, progression-free or overall survival would be a secondary endpoint in a biosimilar trial. With a conventional analytic procedure via, for example, hazard ratio to quantify the group difference, it is difficult and challenging to assess clinical equivalence with respect to progression-free or overall survival because the study generally has a limited number of clinical events observed in the study. In this article, we show that an alternative procedure based on the restricted mean survival time, which has been discussed extensively for design and analysis of a general equivalence study, is readily applicable to a biosimilar trial. Unlike the hazard ratio, this procedure provides a clinically interpretable estimate for assessing equivalence. Using the restricted mean survival time as a summary measure of the survival curve will enhance better treatment decision making in adopting a biosimilar product over the reference product.},
  pmcid = {PMC7050006},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\uno2019.pdf}
}

@article{uno2023,
  title = {Ratio and Difference of Average Hazard with Survival Weight: {{New}} Measures to Quantify Survival Benefit of New Therapy},
  shorttitle = {Ratio and Difference of Average Hazard with Survival Weight},
  author = {Uno, Hajime and Horiguchi, Miki},
  date = {2023},
  journaltitle = {Statistics in Medicine},
  volume = {42},
  number = {7},
  pages = {936--952},
  issn = {1097-0258},
  doi = {10.1002/sim.9651},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9651},
  urldate = {2023-09-13},
  abstract = {The hazard ratio (HR) has been the most popular measure to quantify the magnitude of treatment effect on time-to-event outcomes in clinical research. However, the traditional Cox's HR approach has several drawbacks. One major issue is that there is no clear interpretation when the proportional hazards (PH) assumption does not hold, because the estimated HR is affected by study-specific censoring time distribution in non-PH cases. Another major issue is that the lack of a group-specific absolute hazard value in each group obscures the clinical significance of the magnitude of the treatment effect. Given these, we propose average hazard with survival weight (AH-SW) as a summary metric of event time distribution and will use difference in AH-SW (DAH-SW) or ratio of AH-SW (RAH-SW) to quantify the treatment effect magnitude. The AH-SW is interpreted as a person-time incidence rate that does not depend on random censoring. It is defined as the ratio of cumulative incidence probability and restricted mean survival time (RMST), which can be estimated non-parametrically. Numerical studies demonstrate that DAH-SW and RAH-SW offer almost identical power to Cox's HR-based tests under PH scenarios and can be more powerful for delayed-difference patterns often seen in immunotherapy trials. Like median and RMST differences, the proposed approach is a good model-free alternative to the HR-based approach for evaluating the treatment effect magnitude. Such a model-free measure will increase the likelihood that results from clinical studies are correctly interpreted and generalized to future populations.},
  langid = {english},
  keywords = {cumulative incidence probability,immunotherapy studies,non-proportional hazards,notion,person-time incidence rate,restricted mean survival time},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\uno2023.pdf}
}

@article{vanrosmalen2018,
  title = {Including Historical Data in the Analysis of Clinical Trials: {{Is}} It Worth the Effort?},
  shorttitle = {Including Historical Data in the Analysis of Clinical Trials},
  author = {family=Rosmalen, given=Joost, prefix=van, useprefix=true and Dejardin, David and family=Norden, given=Yvette, prefix=van, useprefix=true and Löwenberg, Bob and Lesaffre, Emmanuel},
  date = {2018-10-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {27},
  number = {10},
  pages = {3167--3182},
  publisher = {SAGE Publications Ltd STM},
  issn = {0962-2802},
  doi = {10.1177/0962280217694506},
  url = {https://doi.org/10.1177/0962280217694506},
  urldate = {2023-10-10},
  abstract = {Data of previous trials with a similar setting are often available in the analysis of clinical trials. Several Bayesian methods have been proposed for including historical data as prior information in the analysis of the current trial, such as the (modified) power prior, the (robust) meta-analytic-predictive prior, the commensurate prior and methods proposed by Pocock and Murray et~al. We compared these methods and illustrated their use in a practical setting, including an assessment of the comparability of the current and the historical data. The motivating data set consists of randomised controlled trials for acute myeloid leukaemia. A simulation study was used to compare the methods in terms of bias, precision, power and type I error rate. Methods that estimate parameters for the between-trial heterogeneity generally offer the best trade-off of power, precision and type I error, with the meta-analytic-predictive prior being the most promising method. The results show that it can be feasible to include historical data in the analysis of clinical trials, if an appropriate method is used to estimate the heterogeneity between trials, and the historical data satisfy criteria for comparability.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\vanrosmalen2018.pdf}
}

@article{verbeeck2020,
  title = {Evaluation of Inferential Methods for the Net Benefit and Win Ratio Statistics},
  author = {Verbeeck, Johan and Ozenne, Brice and Anderson, William N.},
  date = {2020-09-02},
  journaltitle = {Journal of Biopharmaceutical Statistics},
  volume = {30},
  number = {5},
  eprint = {32097079},
  eprinttype = {pmid},
  pages = {765--782},
  publisher = {Taylor \& Francis},
  issn = {1054-3406},
  doi = {10.1080/10543406.2020.1730873},
  url = {https://doi.org/10.1080/10543406.2020.1730873},
  urldate = {2023-10-01},
  abstract = {General Pairwise Comparison (GPC) statistics, such as the net benefit and the win ratio, have been applied in clinical trial data analysis and design. In the literature, inferential methods based on re-sampling,~asymptotic or exact methods have been proposed for these GPC statistics, but they have not been compared to each other. In this paper, the small sample bias of the variance estimation, Type I~error control and 95\% confidence interval coverage of the GPC inferential methods are evaluated using simulations. The exact permutation and bootstrap tests perform best in all evaluated aspects for the net benefit, while the exact bootstrap test performs best for the win ratio.},
  keywords = {bootstrap,General Pairwise Comparisons,net benefit,notion,permutation,statistical inference,win Ratio},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\verbeeck2020.pdf}
}

@article{wei2015,
  title = {Meta-Analysis of Time-to-Event Outcomes from Randomized Trials Using Restricted Mean Survival Time: Application to Individual Participant Data},
  shorttitle = {Meta-Analysis of Time-to-Event Outcomes from Randomized Trials Using Restricted Mean Survival Time},
  author = {Wei, Yinghui and Royston, Patrick and Tierney, Jayne F. and Parmar, Mahesh K. B.},
  date = {2015},
  journaltitle = {Statistics in Medicine},
  volume = {34},
  number = {21},
  pages = {2881--2898},
  issn = {1097-0258},
  doi = {10.1002/sim.6556},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6556},
  urldate = {2023-09-06},
  abstract = {Meta-analysis of time-to-event outcomes using the hazard ratio as a treatment effect measure has an underlying assumption that hazards are proportional. The between-arm difference in the restricted mean survival time is a measure that avoids this assumption and allows the treatment effect to vary with time. We describe and evaluate meta-analysis based on the restricted mean survival time for dealing with non-proportional hazards and present a diagnostic method for the overall proportional hazards assumption. The methods are illustrated with the application to two individual participant meta-analyses in cancer. The examples were chosen because they differ in disease severity and the patterns of follow-up, in order to understand the potential impacts on the hazards and the overall effect estimates. We further investigate the estimation methods for restricted mean survival time by a simulation study. Copyright © 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {meta-analysis,non-proportional hazards,notion,restricted mean survival time,time-to-event outcomes},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\wei2015.pdf}
}

@article{weir2018,
  title = {Design of Non-Inferiority Randomized Trials Using the Difference in Restricted Mean Survival Times},
  author = {Weir, Isabelle R and Trinquart, Ludovic},
  date = {2018-10},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {15},
  number = {5},
  pages = {499--508},
  issn = {1740-7745, 1740-7753},
  doi = {10.1177/1740774518792259},
  url = {http://journals.sagepub.com/doi/10.1177/1740774518792259},
  urldate = {2023-08-23},
  abstract = {Background/aims: Non-inferiority trials with time-to-event outcomes are becoming increasingly common. Designing non-inferiority trials is challenging, in particular, they require very large sample sizes. We hypothesized that the difference in restricted mean survival time, an alternative to the hazard ratio, could lead to smaller required sample sizes. Methods: We show how to convert a margin for the hazard ratio into a margin for the difference in restricted mean survival time and how to calculate the required sample size under a Weibull survival distribution. We systematically selected non-inferiority trials published between 2013 and 2016 in seven major journals. Based on the protocol and article of each trial, we determined the clinically relevant time horizon of interest. We reconstructed individual patient data for the primary outcome and fit a Weibull distribution to the comparator arm. We converted the margin for the hazard ratio into the margin for the difference in restricted mean survival time. We tested for non-inferiority using the difference in restricted mean survival time and hazard ratio. We determined the required sample size based on both measures, using the type I error risk and power from the original trial design. Results: We included 35 trials. We found evidence of non-proportional hazards in five (14\%) trials. The hazard ratio and the difference in restricted mean survival time were consistent regarding non-inferiority testing, except in one trial where the difference in restricted mean survival time led to evidence of non-inferiority while the hazard ratio did not. The median hazard ratio margin was 1.43 (Q1–Q3, 1.29–1.75). The median of the corresponding margins for the difference in restricted mean survival time was 221 days (Q1–Q3, 236 to 28) for a median time horizon of 2.0 years (Q1–Q3, 1–3 years). The required sample size according to the difference in restricted mean survival time was smaller in 71\% of trials, with a median relative decrease of 8.5\% (Q1–Q3, 0.4\%–38.0\%). Across all 35 trials, about 25,000 participants would have been spared from enrollment using the difference in restricted mean survival time compared to hazard ratio for trial design. Conclusion: The margins for the hazard ratio may seem large but translate to relatively small differences in restricted mean survival time. The difference in restricted mean survival time offers meaningful interpretation and can result in considerable reductions in sample size. Restricted mean survival time-based measures should be considered more widely in the design and analysis of non-inferiority trials with time-to-event outcomes.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\weir2018.pdf}
}

@article{weir2021,
  title = {Multivariate Meta-Analysis Model for the Difference in Restricted Mean Survival Times},
  author = {Weir, Isabelle R and Tian, Lu and Trinquart, Ludovic},
  date = {2021-01-01},
  journaltitle = {Biostatistics},
  shortjournal = {Biostatistics},
  volume = {22},
  number = {1},
  pages = {82--96},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxz018},
  url = {https://doi.org/10.1093/biostatistics/kxz018},
  urldate = {2023-10-10},
  abstract = {In randomized controlled trials (RCTs) with time-to-event outcomes, the difference in restricted mean survival times (RMSTD) offers an absolute measure of the treatment effect on the time scale. Computation of the RMSTD relies on the choice of a time horizon, \$\textbackslash tau\$. In a meta-analysis, varying follow-up durations may lead to the exclusion of RCTs with follow-up shorter than \$\textbackslash tau\$. We introduce an individual patient data multivariate meta-analysis model for RMSTD estimated at multiple time horizons. We derived the within-trial covariance for the RMSTD enabling the synthesis of all data by borrowing strength from multiple time points. In a simulation study covering 60 scenarios, we compared the statistical performance of the proposed method to that of two univariate meta-analysis models, based on available data at each time point and based on predictions from flexible parametric models. Our multivariate model yields smaller mean squared error over univariate methods at all time points. We illustrate the method with a meta-analysis of five RCTs comparing transcatheter aortic valve replacement (TAVR) with surgical replacement in patients with aortic stenosis. Over 12, 24, and 36 months of follow-up, those treated by TAVR live 0.28 [95\% confidence interval (CI) 0.01 to 0.56], 0.46 (95\% CI \$-\$0.08 to 1.01), and 0.79 (95\% CI \$-\$0.43 to 2.02) months longer on average compared to those treated by surgery, respectively.},
  keywords = {notion},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\weir2021.pdf;C\:\\Users\\david\\Zotero\\storage\\BRR5M6RR\\5512964.html}
}

@article{xu2000,
  title = {Estimating Average Regression Effect under Non-Proportional Hazards},
  author = {Xu, Ronghui and O’Quigley, John},
  date = {2000-12-01},
  journaltitle = {Biostatistics},
  shortjournal = {Biostatistics},
  volume = {1},
  number = {4},
  pages = {423--439},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/1.4.423},
  url = {https://doi.org/10.1093/biostatistics/1.4.423},
  urldate = {2023-08-30},
  abstract = {We present an estimator of average regression effect under a non-proportional hazards model, where the regression effect of the covariates on the log hazard ratio changes with time. In the absence of censoring, the new estimate coincides with the usual partial likelihood estimate, both estimates being consistent for a parameter having an interpretation as an average population regression effect. In the presence of an independent censorship, the new estimate is still consistent for this same population parameter, whereas the partial likelihood estimate will converge to a different quantity that depends on censoring. We give an approximation of the population average effect as \textbackslash batchmode \textbackslash documentclass[fleqn,10pt,legalpaper]\{article\} \textbackslash usepackage\{amssymb\} \textbackslash usepackage\{amsfonts\} \textbackslash usepackage\{amsmath\} \textbackslash pagestyle\{empty\} \textbackslash begin\{document\} \textbackslash (\{\textbackslash int\}\textbackslash{} \{\textbackslash beta\}(t)dF(t)\textbackslash ) \textbackslash end\{document\}. The new estimate is easy to compute, requiring only minor modifications to existing softwares. We illustrate the use of the average effect estimate on a breast cancer dataset from Institut Curie. The behavior of the estimator, its comparison with the partial likelihood estimate, as well as the approximation by \textbackslash batchmode \textbackslash documentclass[fleqn,10pt,legalpaper]\{article\} \textbackslash usepackage\{amssymb\} \textbackslash usepackage\{amsfonts\} \textbackslash usepackage\{amsmath\} \textbackslash pagestyle\{empty\} \textbackslash begin\{document\} \textbackslash (\{\textbackslash int\}\textbackslash{} \{\textbackslash beta\}(t)dF(t)\textbackslash ) \textbackslash end\{document\}are studied via simulation.*To whom correspondence should be addressed},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\xu2000.pdf}
}

@article{zeileis2004,
  title = {Econometric {{Computing}} with {{HC}} and {{HAC Covariance Matrix Estimators}}},
  author = {Zeileis, Achim},
  date = {2004-11-29},
  journaltitle = {Journal of Statistical Software},
  volume = {11},
  pages = {1--17},
  issn = {1548-7660},
  doi = {10.18637/jss.v011.i10},
  url = {https://doi.org/10.18637/jss.v011.i10},
  urldate = {2024-02-21},
  abstract = {Data described by econometric models typically contains autocorrelation and/or heteroskedasticity of unknown form and for inference in such models it is essential to use covariance matrix estimators that can consistently estimate the covariance of the model parameters. Hence, suitable heteroskedasticity consistent (HC) and heteroskedasticity and autocorrelation consistent (HAC) estimators have been receiving attention in the econometric literature over the last 20 years. To apply these estimators in practice, an implementation is needed that preferably translates the conceptual properties of the underlying theoretical frameworks into computational tools. In this paper, such an implementation in the package sandwich in the R system for statistical computing is described and it is shown how the suggested functions provide reusable components that build on readily existing functionality and how they can be integrated easily into new inferential procedures or applications. The toolbox contained in sandwich is extremely flexible and comprehensive, including specific functions for the most important HC and HAC estimators from the econometric literature. Several real-world data sets are used to illustrate how the functionality can be integrated into applications.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\zeileis22.pdf}
}

@article{zeileis2006,
  title = {Object-Oriented {{Computation}} of {{Sandwich Estimators}}},
  author = {Zeileis, Achim},
  date = {2006-08-15},
  journaltitle = {Journal of Statistical Software},
  volume = {16},
  pages = {1--16},
  issn = {1548-7660},
  doi = {10.18637/jss.v016.i09},
  url = {https://doi.org/10.18637/jss.v016.i09},
  urldate = {2024-02-21},
  abstract = {Sandwich covariance matrix estimators are a popular tool in applied regression modeling for performing inference that is robust to certain types of model misspecification. Suitable implementations are available in the R system for statistical computing for certain model fitting functions only (in particular lm()), but not for other standard regression functions, such as glm(), nls(), or survreg().  Therefore, conceptual tools and their translation to computational tools in the package sandwich are discussed, enabling the computation of sandwich estimators in general parametric models. Object orientation can be achieved by providing a few extractor functions' most importantly for the empirical estimating functions' from which various types of sandwich estimators can be computed.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\zeileis2006.pdf}
}

@article{zeileis2020,
  title = {Various {{Versatile Variances}}: {{An Object-Oriented Implementation}} of {{Clustered Covariances}} in {{R}}},
  shorttitle = {Various {{Versatile Variances}}},
  author = {Zeileis, Achim and Köll, Susanne and Graham, Nathaniel},
  date = {2020-10-07},
  journaltitle = {Journal of Statistical Software},
  volume = {95},
  pages = {1--36},
  issn = {1548-7660},
  doi = {10.18637/jss.v095.i01},
  url = {https://doi.org/10.18637/jss.v095.i01},
  urldate = {2023-12-08},
  abstract = {Clustered covariances or clustered standard errors are very widely used to account for correlated or clustered data, especially in economics, political sciences, and other social sciences. They are employed to adjust the inference following estimation of a standard least-squares regression or generalized linear model estimated by maximum likelihood. Although many publications just refer to "the" clustered standard errors, there is a surprisingly wide variety of clustered covariances, particularly due to different flavors of bias corrections. Furthermore, while the linear regression model is certainly the most important application case, the same strategies can be employed in more general models (e.g., for zero-inflated, censored, or limited responses). In R, functions for covariances in clustered or panel models have been somewhat scattered or available only for certain modeling functions, notably the (generalized) linear regression model. In contrast, an object-oriented approach to "robust" covariance matrix estimation  -  applicable beyond lm() and glm()  -  is available in the sandwich package but has been limited to the case of cross-section or time series data. Starting with sandwich 2.4.0, this shortcoming has been corrected: Based on methods for two generic functions (estfun() and bread()), clustered and panel covariances are provided in vcovCL(), vcovPL(), and vcovPC(). Moreover, clustered bootstrap covariances are provided in vcovBS(), using model update() on bootstrap samples. These are directly applicable to models from packages including MASS, pscl, countreg, and betareg, among many others. Some empirical illustrations are provided as well as an assessment of the methods' performance in a simulation study.},
  langid = {english},
  keywords = {clustered data,covariance matrix estimator,notion,object orientation,R,simulation},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\zeileis2020.pdf}
}

@article{zhang2023,
  title = {Bayesian Nonparametric Analysis of Restricted Mean Survival Time},
  author = {Zhang, Chenyang and Yin, Guosheng},
  date = {2023},
  journaltitle = {Biometrics},
  volume = {79},
  number = {2},
  pages = {1383--1396},
  issn = {1541-0420},
  doi = {10.1111/biom.13622},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13622},
  urldate = {2023-09-28},
  abstract = {The restricted mean survival time (RMST) evaluates the expectation of survival time truncated by a prespecified time point, because the mean survival time in the presence of censoring is typically not estimable. The frequentist inference procedure for RMST has been widely advocated for comparison of two survival curves, while research from the Bayesian perspective is rather limited. For the RMST of both right- and interval-censored data, we propose Bayesian nonparametric estimation and inference procedures. By assigning a mixture of Dirichlet processes (MDP) prior to the distribution function, we can estimate the posterior distribution of RMST. We also explore another Bayesian nonparametric approach using the Dirichlet process mixture model and make comparisons with the frequentist nonparametric method. Simulation studies demonstrate that the Bayesian nonparametric RMST under diffuse MDP priors leads to robust estimation and under informative priors it can incorporate prior knowledge into the nonparametric estimator. Analysis of real trial examples demonstrates the flexibility and interpretability of the Bayesian nonparametric RMST for both right- and interval-censored data.},
  langid = {english},
  keywords = {Bayesian hypothesis testing,clinical trials,interval censoring,mixture of Dirichlet processes,notion,survival analysis},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\zhang2023.pdf;C\:\\Users\\david\\Zotero\\storage\\8RCFY8GH\\biom.html}
}

@article{zhao2012,
  title = {Utilizing the Integrated Difference of Two Survival Functions to Quantify the Treatment Contrast for Designing, Monitoring, and Analyzing a Comparative Clinical Study},
  author = {Zhao, Lihui and Tian, Lu and Uno, Hajime and Solomon, Scott D and Pfeffer, Marc A and Schindler, Jerald S and Wei, Lee Jen},
  date = {2012-10-01},
  journaltitle = {Clinical Trials},
  shortjournal = {Clinical Trials},
  volume = {9},
  number = {5},
  pages = {570--577},
  publisher = {SAGE Publications},
  issn = {1740-7745},
  doi = {10.1177/1740774512455464},
  url = {https://doi.org/10.1177/1740774512455464},
  urldate = {2023-08-29},
  abstract = {Background Consider a comparative, randomized clinical study with a specific event time as the primary end point. In the presence of censoring, standard methods of summarizing the treatment difference are based on Kaplan–Meier curves, the logrank test, and the point and interval estimates via Cox’s procedure. Moreover, for designing and monitoring the study, one usually utilizes an event-driven scheme to determine the sample sizes and interim analysis time points. Purpose When the proportional hazards (PHs) assumption is violated, the logrank test may not have sufficient power to detect the difference between two event time distributions. The resulting hazard ratio estimate is difficult, if not impossible, to interpret as a treatment contrast. When the event rates are low, the corresponding interval estimate for the ‘hazard ratio’ can be quite large due to the fact that the interval length depends on the observed numbers of events. This may indicate that there is not enough information for making inferences about the treatment comparison even when there is no difference between two groups. This situation is quite common for a postmarketing safety study. We need an alternative way to quantify the group difference. Methods Instead of quantifying the treatment group difference using the hazard ratio, we consider an easily interpretable and model-free parameter, the integrated survival rate difference over a prespecified time interval, as an alternative. We present the inference procedures for such a treatment contrast. This approach is purely nonparametric and does not need any model assumption such as the PHs. Moreover, when we deal with equivalence or noninferiority studies and the event rates are low, our procedure would provide more information about the treatment difference. We used a cardiovascular trial data set to illustrate our approach. Results The results using the integrated event rate differences have a heuristic interpretation for the treatment difference even when the PHs assumption is not valid. When the event rates are low, for example, for the cardiovascular study discussed in this article, the procedure for the integrated event rate difference provides tight interval estimates in contrast to those based on the event-driven inference method. Limitations The design of a trial with the integrated event rate difference may be more complicated than that using the event-driven procedure. One may use simulation to determine the sample size and the estimated duration of the study. Conclusions The procedure discussed in this article can be a useful alternative to the standard PHs method in the survival analysis.},
  langid = {english},
  keywords = {notion},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\zhao2012.pdf}
}

@article{zhao2016,
  title = {On the Restricted Mean Survival Time Curve in Survival Analysis},
  author = {Zhao, Lihui and Claggett, Brian and Tian, Lu and Uno, Hajime and Pfeffer, Marc A. and Solomon, Scott D. and Trippa, Lorenzo and Wei, L. J.},
  date = {2016},
  journaltitle = {Biometrics},
  volume = {72},
  number = {1},
  pages = {215--221},
  issn = {1541-0420},
  doi = {10.1111/biom.12384},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12384},
  urldate = {2023-08-29},
  abstract = {For a study with an event time as the endpoint, its survival function contains all the information regarding the temporal, stochastic profile of this outcome variable. The survival probability at a specific time point, say t, however, does not transparently capture the temporal profile of this endpoint up to t. An alternative is to use the restricted mean survival time (RMST) at time t to summarize the profile. The RMST is the mean survival time of all subjects in the study population followed up to t, and is simply the area under the survival curve up to t. The advantages of using such a quantification over the survival rate have been discussed in the setting of a fixed-time analysis. In this article, we generalize this approach by considering a curve based on the RMST over time as an alternative summary to the survival function. Inference, for instance, based on simultaneous confidence bands for a single RMST curve and also the difference between two RMST curves are proposed. The latter is informative for evaluating two groups under an equivalence or noninferiority setting, and quantifies the difference of two groups in a time scale. The proposal is illustrated with the data from two clinical trials, one from oncology and the other from cardiology.},
  langid = {english},
  keywords = {Equivalence/noninferiority study,Gaussian process,Martingale,notion,Simultaneous confidence band,Survival function},
  file = {C\:\\Users\\david\\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\\Uni\\05 Masterthesis\\literature\\zhao2016.pdf;C\:\\Users\\david\\Zotero\\storage\\ICHQ9TG3\\biom.html}
}

@article{zheng2023,
  title = {A Win Ratio Approach for Comparing Crossing Survival Curves in Clinical Trials},
  author = {Zheng, Sirui and Wang, Duolao and Qiu, Junshan and Chen, Tao and Gamalo, Margaret},
  date = {2023-07-04},
  journaltitle = {Journal of Biopharmaceutical Statistics},
  volume = {33},
  number = {4},
  eprint = {36749067},
  eprinttype = {pmid},
  pages = {488--501},
  publisher = {Taylor \& Francis},
  issn = {1054-3406},
  doi = {10.1080/10543406.2023.2170393},
  url = {https://doi.org/10.1080/10543406.2023.2170393},
  urldate = {2023-10-01},
  abstract = {Many clinical trials include time-to-event or survival data as an outcome. To compare two survival distributions, the log-rank test is often used to produce a P-value for a statistical test of the null hypothesis that the two survival curves are identical. However, such a P-value does not provide the magnitude of the difference between the curves regarding the treatment effect. As a result, the P-value is often accompanied by an estimate of the hazard ratio from the proportional hazards model or Cox model as a measurement of treatment difference. However, one of the most important assumptions for Cox model is that the hazard functions for the two treatment groups are proportional. When the hazard curves cross, the Cox model could lead to misleading results and the log-rank test could also perform poorly. To address the problem of crossing curves in survival analysis, we propose the use of the win ratio method put forward by Pocock et al. as an estimand for analysing such data. The subjects in the test and control treatment groups are formed into all possible pairs. For each pair, the test treatment subject is labelled a winner or a loser if it is known who had the event of interest such as death. The win ratio is the total number of winners divided by the total number of losers and its standard error can be estimated using Bebu and Lachin method. Using real trial datasets and Monte Carlo simulations, this study investigates the power and type I error and compares the win ratio method with the log-rank test and Cox model under various scenarios of crossing survival curves with different censoring rates and distribution parameters. The results show that the win ratio method has similar power as the log-rank test and Cox model to detect the treatment difference when the assumption of proportional hazards holds true, and that the win ratio method outperforms log-rank test and Cox model in terms of power to detect the treatment difference when the survival curves cross.},
  keywords = {Cox model,crossing survival curves,estimand,log-rank test,notion,survival analysis,Win ratio},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\zheng2023.pdf}
}

@article{zhou2021,
  title = {Restricted Mean Survival Time and Confidence Intervals by Empirical Likelihood Ratio},
  author = {Zhou, Mai},
  date = {2021-05-04},
  journaltitle = {Journal of Biopharmaceutical Statistics},
  volume = {31},
  number = {3},
  pages = {362--374},
  publisher = {Taylor \& Francis},
  issn = {1054-3406},
  doi = {10.1080/10543406.2020.1862143},
  url = {https://doi.org/10.1080/10543406.2020.1862143},
  urldate = {2023-09-29},
  abstract = {Recently, an increasing number of researchers propose to use restricted mean survival time (RMST) to evaluate medical treatment outcomes when the proportional hazards assumption is in doubt. Using the empirical likelihood ratio for the Kaplan-Meier mean, we show how the statistical analysis for the RMST and difference/ratio of two RMSTs can be obtained. Examples are given using R software and package emplik. We argue that the confidence intervals produced with this new method has several intrinsic advantages over the current method implemented in R package survRM2. A simulation is also included.},
  keywords = {notion,Nuisance parameter,profile empirical likelihood,Wilks confidence interval},
  file = {C:\Users\david\ownCloud - david.jesse@stud.uni-goettingen.de@owncloud.gwdg.de\Uni\05 Masterthesis\literature\zhou2021.pdf}
}

@online{zotero-528,
  title = {Relative Likelihood Ratios for Neutral Comparisons of Statistical Tests in Simulation Studies - {{Huang}} - {{Biometrical Journal}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/bimj.202200102},
  urldate = {2023-09-28},
  keywords = {notion},
  file = {C:\Users\david\Zotero\storage\344YNIRJ\bimj.html}
}

@online{zotero-558,
  title = {An Extension of Generalized Pairwise Comparisons for Prioritized Outcomes in the Presence of Censoring - {{Julien Péron}}, {{Marc Buyse}}, {{Brice Ozenne}}, {{Laurent Roche}}, {{Pascal Roy}}, 2018},
  url = {https://journals.sagepub.com/doi/full/10.1177/0962280216658320},
  urldate = {2023-10-01},
  keywords = {notion},
  file = {C:\Users\david\Zotero\storage\4DZGNNJQ\0962280216658320.html}
}

@online{zotero-735,
  title = {Survival/Vignette2/Pseudo.Pdf at Master · Therneau/Survival},
  url = {https://github.com/therneau/survival/blob/master/vignette2/pseudo.pdf},
  urldate = {2023-12-12},
  abstract = {Survival package for R. Contribute to therneau/survival development by creating an account on GitHub.},
  langid = {english},
  organization = {GitHub},
  keywords = {notion},
  file = {C:\Users\david\Zotero\storage\JNI33HXK\pseudo.html}
}
