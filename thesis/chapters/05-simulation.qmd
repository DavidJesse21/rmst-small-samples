---
bibliography: ["../bib/references.bib", "../bib/packages.bib"]
---

# Simulation Study {#sec-simulation}

In order to investigate the empirical performance of the presented methods, in particular in settings with small to moderate sample sizes, we now set up a simulation study pursuing to cover a satisfactory range of different scenarios.
In order to embed the design and planning of this study into a structured approach we make use of the ADEMP framework introduced by @morris2019a.
ADEMP is an acronym translating to *Aims*, *Data-generating mechanisms*, *Estimands* (and other targets), *Methods* and *Performance measures*.
The concrete configuration of these aspects is presented in the first subsection.
Thereafter, we present the results of this simulation study.


## Study Design {#sec-sim-design}

*Aims*\newline
The aim of this simulation study is to investigate the performance of different methods for estimating and testing the RMST difference between two samples, e.g. a control and a treatment group in a clinical trial.
In particular, the focus is on settings with small to moderate sample sizes as all methods under investigation either explicitly or implicitly rely on large sample approximations based on asymptotic theory.

*Data-generating mechanisms*\newline
The data-generating mechanisms are in principle adopted from the simulation study by @ditzhaus2023, including five factors being varied:
The event time distributions, the censoring time distributions, the base allocation of the sample size, the sample multiplier as well as the effect size, i.e. the true RMST difference.
However, we do not evaluate all of the scenarios constructed by @ditzhaus2023 but instead concentrate only on a subset of them that we consider to be representative in the sense of covering a range of possibly heterogeneous scenarios.
For the effect sizes $\Delta = \mu_1(t^*) - \mu_0(t^*)$ @ditzhaus2023 considered four values in total.
Here, we only look at $\Delta \in \{0, 1.5\}$, reflecting scenarios under the null and under the alternative hypothesis, respectively.
In addition, we only investigate three of the nine pairs of event time distributions depicted by @ditzhaus2023:
\begin{itemize}
  \item[S1] Exponential distributions and proportional hazard alternatives: $T_0 \sim \dexp(0.2)$ and $T_1 \sim \dexp(\theta_{\Delta})$
  \item[S7] Exponential vs piecewise Exponential: $T_0 \sim \dexp(0.2)$ and $T_1$ with piecewise constant hazard function $\lambda_1(t) = 0.5 \cdot \mathbf{1}\{t \leq \theta\} + 0.05 \cdot \mathbf{1}\{t > \theta\}$
  \item[S8] Weibull distributions with crossing curves and shape alternatives: $T_0 \sim \dweib(3, 1/8)$ and $T_1 \sim \dweib(\theta, 1/14)$
\end{itemize}
Here, $\theta$ denotes a generic parameter that gets calibrated according to the effect size $\Delta$.
For S1 this is simply the rate parameter of the exponential distribution for the treatment group.
For S7 this is the time point at which the hazard of the treatment group changes.
For S8 this is the scale parameter of the Weibull distribution for the treatment group.[^1]
In contrast, we keep using all levels of the censoring distributions:
\begin{itemize}
  \item[C1] unequally Weibull distributed censoring (Weib, uneq): $C_0 \sim \dweib(3, 18)$ and $C_1 \dweib(0.5, 40)$
  \item[C2] equally uniformly distributed censoring (Unif, eq): $C_0 \sim \dunif(0, 25)$ and $C_1 \sim \dunif(0, 25)$
  \item[C3] equally Weibull distributed censoring (Weib, eq): $C_0 \sim \dweib(3, 15)$ and $C_1 \sim \dweib(3, 15)$
\end{itemize}
A visual representation of the pairs of survival and censoring distributions are given in TODO1 and TODO2, respectively.
Moreover, we also utilize the assumptions about the sample sizes and their allocations from @ditzhaus2023.
In particular, we consider three base settings of how the samples are allocated between the groups, i.e. $(n_0, n_1) \in \{(12, 18); (15, 15); (18, 12)\}$.
To obtain different total sample sizes these base allocations get multiplied with an integer $K$.
In comparison to @ditzhaus2023, who considered $K = 1, 2, 4$ we add $K = 6$ to this list in order to incorporate scenarios for which we expect the asymptotic theory to take effect.
A summary of all design factors and their associated levels is given in TODO3.
In accordance with @ditzhaus2023 and @horiguchi2020a we regenerated the data when the RMST is inestimable for at least one of the two groups using the nonparametric Kaplan-Meier estimator.
216 scenarios in total blabla TODO

```{r}
#| label: fig-models
#| fig-cap: "Simulation models for the event (top) and censoring (bottom) times"
#| fig-width: 10
#| fig-height: 8

wrap_plots(
  plot_surv_models(linewidth = 1.1, align = "h"),
  plot_cens_models(linewidth = 1.1, align = "h"),
  ncol = 1
)
```

*Estimands and other targets*\newline
Although all of the methods involve the estimation of a particular parameter, namely the two-sample RMST difference $\muhat_1(t^*) - \muhat_0^{t^*}$, they are all based on the Kaplan-Meier estimator of the survival function.
Therefore the point estimates of the different methods will either be exactly or nearly identical.
The main distinction is how the statistical test of the null hypothesis in (@eq-test-problem) is conducted.
Therefore, that null hypothesis is the target of the simulation study.

*Methods*\newline
All methods that will be assessed in this study have been presented and described in detail in @sec-inference.
Of primary interest are the two methods based on using pseudo-observations, i.e. one employing an asymptotic test and the other one using a bootstrap test.
Moreover, as the bootstrap test, in contrast to the asymptotic test, uses IJ pseudo-observations, we also include an asymptotic test, which uses IJ pseudo-observations instead of the ordinary ones, in order to obtain an intuition for deviating performance blabla.
The asymptotic test has the role of a reference being most commonly employed in practice blabla.
On the other hand, the studentized permutation approach by @ditzhaus2023 is seen as the current gold standard exhibiting the best performance among all tests.
Other methods that could have been included are the unstudentized permutation test by 
@horiguchi2020a and the approach based on the empirical likelihood ratio by @zhou2021.
Both are not covered here because we do not consider them to fall into either of the two mentioned categories, i.e. being a standard method in practice or being the method with the (arguably) best performance.

*Performance measures*\newline
As the target of the simulation study is the null hypothesis in (@eq-test-problem) the primary performance measures for this simulation study are naturally the type I error rate and the power associated with the respective test.
As the main motivation of this investigation is the inflated type I error rate of the asymptotic test, however, more emphasis is put on this performance measure.
Additionally, we consider the coverage of the confidence intervals associated with each method.


## Computational Details {#sec-sim-comp}

For each of the scenarios described in @sec-sim-design we generated $N_{\text{sim}} = 5000$ data sets as it has been done by @ditzhaus2023.
Similarly, for the studentized permutation test we used $N_{\text{res}} = 2000$ resamples.
For a fair comparison we used the same number of resampling iterations for the bootstrap approach.
Furthermore, we also set the nominal significance level to $\alpha = 5\%$ and set the restriction time to $t^* = 10$.

All computations have been carried out using the `R` programming language in version 4.3.0 [@R-base] in the high performance computing cluster of the GWDG in Goettingen.
The asymptotic test as well as the studentized permutation test have been implemented by ourselves based on original code supplied by Marc Ditzhaus.
For the approaches using pseudo-observations we used the `rmeanglm()` function from the `{eventglm}` package [@sachs2022], though the functions for calculating the pseudo-observations were programmed by ourselves.
For the implementation of the IJ pseudo-observations we used the `pseudo()` function from the `{survival}` package [@R-survival].
The versions of these packages as well as all other primary or secondary package dependencies have been tracked using the `{renv}` package [@R-renv].

To further ensure reproducibility of the obtained results we used the `with_seed()` function from the `{withr}` package [@R-withr] for handling seeds of the pseudo random number generator.
Within each scenario we then simulated the data sets sequentially using the Mersenne-Twister algorithm, avoiding the need to worry about correlated data sets or using RNG streams for this [@morris2019a].
On the other hand, the application of the presented methods on the simulated data sets has been parallelized using the `{parallel}`, which gets shipped with base R [@R-base].
Here, we kept using the `with_seed()` function but instead employed the L'Ecuyer-CMRG algorithm for creating pseudo random numbers.
For the asymptotic test and the standard test based on pseudo-observations this has no implications, but it matters for the studentized permutation and bootstrap approaches as these entail stochasticity themselves.


## Results {#sec-sim-results}

```{r}
#| label: tbl-type1err
#| tbl-cap: "Type-I error rates in % (nominal level $\\alpha = 5\\%$)"

# Calculate rejection rates
# For sake of illustration:
# - ignore pseudo_ij (no bootstrap)
# - ignore k = 6
dt_err = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 0 & samples_k != 6, scenario.id] &
        algo.id != 4],
  stats_NA = FALSE
)
dt_err = merge(dt_err, dts[rmst_diff == 0 & samples_k != 6], by = "scenario.id")

# For latex, directly starting off with a table in wide format might be easier
dt_err = dcast(dt_err, scenario.id ~ algo.id, value.var = "reject")
setnames(dt_err, old = as.character(c(1:3, 5)), new = paste0("algo", c(1:3, 5)))

# Append scenario information...
dt_err = merge(dt_err, dts, by = "scenario.id")
setj_percent(dt_err, paste0("algo", c(1:3, 5)))
for (j in paste0("algo", c(1:3, 5))) {
  set(dt_err, j = j, value = round(dt_err[[j]], 1))
}
dt_err[, samples_alloc := factor(
  sprintf("n_%d_%d", n0 / samples_k, n1 / samples_k),
  levels = sprintf("n_%d_%d", c(12, 15, 18), c(18, 15, 12))
)]
# ... but some columns are or have become redundant
for (j in c("scenario.id", "rmst_diff", "n0", "n1")) {
  set(dt_err, j = j, value = NULL)
}

dt_err = dcast(
  dt_err,
  surv_model + cens_model + samples_k ~ samples_alloc, value.var = paste0("algo", c(1:3, 5))
)

# Order columns and rows
setcolorder(
  dt_err, neworder = c(
    "surv_model", "cens_model", "samples_k",
    sprintf("algo%d_n_12_18", c(1:3, 5)),
    sprintf("algo%d_n_15_15", c(1:3, 5)),
    sprintf("algo%d_n_18_12", c(1:3, 5))
  )
)
dt_err[, `:=`(
  surv_model = factor(surv_model, levels = c("ph_exp", "crossing_pwexp", "crossing_wb")),
  cens_model = factor(cens_model, levels = c("uneq_wb", "eq_unif", "eq_wb"))
)]
setorder(dt_err, surv_model, cens_model, samples_k)

# Make some cells of column `cens_model` empty
na_idx = setdiff(1:nrow(dt_err), seq(1, nrow(dt_err), by = 3))
dt_err[na_idx, cens_model := NA]

# Also need to relabel this
dt_err[, cens_model := as.character(cens_model)]
dt_err[, cens_model := fcase(
  cens_model == "uneq_wb", "un. W.",
  cens_model == "eq_unif", "eq. U.",
  cens_model == "eq_wb", "eq. W."
)]


# NAs as empty cells
options(knitr.kable.NA = '')

# Conditional formatting
for (j in colnames(dt_err)[-(1:3)]) {
  set(
    dt_err, j = j, value = cell_spec(
      format(dt_err[[j]], digits = 1, nsmall = 1), bold = between(dt_err[[j]], 4.4, 5.6)
    )
  )
}

# latex table
kbl(
  copy(dt_err)[, surv_model := NULL],
  format = "latex",
  digits = 1,
  booktabs = TRUE,
  centering = TRUE,
  escape = FALSE,
  align = c("l", rep("c", 13)),
  col.names = c("Censoring", "K", rep(c("Asy", "Perm", "PO1", "PO2"), 3)),
  linesep = c(rep("", 8), "\\hline")
) |>
  # Group the columns by their sample allocation
  add_header_above(
    setNames(
      c(1, 1, 4, 4, 4),
      c(" ", " ", sprintf(r"($N = K \\cdot (%d, %d)$)", c(12, 15, 18), c(18, 15, 12)))
    ),
    escape = FALSE, bold = TRUE
  ) |>
  # Borders between the groups make it more accessible
  column_spec(2, border_right = TRUE) |>
  column_spec(6, border_right = TRUE) |>
  column_spec(10, border_right = TRUE) |>
  # Group table (rows) by survival distributions
  pack_rows(
    "S1: Exponential distributions", 1, 9,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
  pack_rows(
    "S7: Exponential and piecewise exponential distributions with crossing curves", 10, 18,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
    pack_rows(
      "S8: Weibull distributions with crossing curves and shape alternatives", 19, 27,
      bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
    ) |>
  # Make it more accessible using stripes option
  kable_styling(
    latex_options = c("striped", "scale_down"),
    stripe_index = c(4:6, 13:15, 22:24),
    stripe_color = "#e9ecef"
  ) |>
  # Explain binomial confidence interval and abbreviations
  footnote(
    general = c(
      paste0(
        r"(\\textit{Note:} )",
        "The values inside the binomial confidence interval [4.4\\\\%, 5.6\\\\%] are printed bold."
      ),
      paste0(
        r"(\\textit{Abbreviations:} )",
        "Asy, asymptotic test; ", "Perm, studentized permutation test; ",
        "PO1, pseudo-observations; ",
        "PO2, pseudo-observations + bootstrap test; ",
        "un. W., unequal Weibull censoring; ",
        "eq. U., equal uniform censoring; ",
        "eq. W., equal Weibull censoring."
      )
    ),
    general_title = "",
    escape = FALSE,
    threeparttable = TRUE
  )
```

```{r}
#| label: fig-sim-type1err
#| fig-cap: "Type-I error rates in % (nominal level $\\alpha = 5\\%$)"
#| fig-width: 10
#| fig-height: 7

dt1 = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 0, scenario.id]],
  stats_NA = FALSE
)
dt1 = merge(
  dt1, dts[rmst_diff == 0, .(scenario.id, num_samples = n0 + n1)],
  by = "scenario.id"
)
setj_percent(dt1, "reject")

ggplot(dt1, aes(factor(algo.id), reject)) +
  geom_boxplot() +
  theme_bw() +
  # Binomial confidence interval
  geom_hline(yintercept = 4.4, linetype = "dashed") +
  geom_hline(yintercept = 5.6, linetype = "dashed") +
  # Labels etc.
  scale_y_continuous(
    name = "Type I error in %\n"
  ) +
  scale_x_discrete(
    name = "\nMethods",
    labels = c("Asy", "Stud. Perm.", "PO", "PO IJ", "PO IJ Boot")
  ) +
  # Differentiate between total sample sizes
  facet_wrap(
    ~ num_samples,
    labeller = labeller(num_samples = \(x) paste0("N = ", x))
  )
```

```{r}
#| label: fig-sim-power
#| fig-cap: "Power values in % ($\\alpha = 5\\%$)"
#| fig-width: 10
#| fig-height: 7

dt2 = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 1.5, scenario.id]],
  stats_NA = FALSE
)
dt2 = merge(
  dt2, dts[rmst_diff == 1.5, .(scenario.id, num_samples = n0 + n1)],
  by = "scenario.id"
)
setj_percent(dt2, "reject")

ggplot(dt2, aes(factor(algo.id), reject)) +
  geom_boxplot() +
  theme_bw() +
  scale_y_continuous(
    name = "Power in %\n"
  ) +
  scale_x_discrete(
    name = "\nMethods",
    labels = c("Asy", "Stud. Perm.", "PO", "PO IJ", "PO IJ Boot")
  ) +
  facet_wrap(
    ~ num_samples,
    labeller = labeller(num_samples = \(x) paste0("N = ", x)),
    scales = "free_y"
  )
```

```{r}
#| label: fig-sim-coverage
#| fig-cap: "Coverage in % (nominal level $\\alpha = 5\\%$)"
#| fig-width: 10
#| fig-height: 9

dt4 = calc_ci_metrics(
  merge(dtr, dts[, .(scenario.id, rmst_diff)], by = "scenario.id"),
  stats_NA = FALSE
)
dt4 = merge(dt4, dts[, .(scenario.id, samples_k, n0, n1)], by = "scenario.id")
setj_percent(dt4, "coverage")
setj_samples_alloc(dt4)

ggplot(dt4, aes(factor(algo.id), coverage)) +
  geom_boxplot() +
  facet_grid(
    rows = vars(samples_k), cols = vars(samples_alloc),
    labeller = labeller(samples_k = \(x) sprintf("K = %s", x))
  ) +
  theme_bw() +
  # Binomial confidence interval
  geom_hline(yintercept = 94.4, linetype = "dashed") +
  geom_hline(yintercept = 95.6, linetype = "dashed") +
  # y-axis
  scale_y_continuous(
    name = "Coverage in %\n"
  ) +
  # x-axis
  scale_x_discrete(
    name = "\nMethods",
    labels = c("Asy", "Stud. Perm.", "PO", "PO IJ", "PO IJ Boot")
  )
```

[^1]: In comparison to the parametrization of the Weibull distribution in @eq-wb-hazard, @ditzhaus2023 use a different parametrization where the scale parameter is $\sigma = \lambda^{-1}$.
For the sake of consistency we keep employing the parametrization as it is used in @eq-wb-hazard.
Nonetheless, the distributional assumptions are equivalent.
