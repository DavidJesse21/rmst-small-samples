---
bibliography: ["../bib/references.bib", "../bib/packages.bib"]
---

# Simulation Study {#sec-simulation}

With the intention of evaluating and comparing the empirical performance of the presented methods, in particular in settings with small to moderate sample sizes, we now set up a simulation study pursuing to cover a satisfactory range of different scenarios.
In order to embed the design of this study into a structured framework we make use of the ADEMP methodology introduced by @morris2019a.
ADEMP is an acronym translating to *Aims*, *Data-generating mechanisms*, *Estimands* (and other targets), *Methods* and *Performance measures*.
The concrete configuration of these aspects is described in the first subsection.
Thereafter, we briefly comment computational aspects impacting the results and their reproducibility.
Lastly, we present the results of the study and discuss our findings.


## Design {#sec-sim-design}

*Aims*\newline
The aim of this simulation study is to investigate the performance of different methods for estimating and testing the RMST difference between two samples, e.g. a control and a treatment group in a clinical trial.
In particular, the focus is on settings with small to moderate sample sizes as all methods under investigation either directly or indirectly rely on large sample approximations based on asymptotic theory.

*Data-generating mechanisms*\newline
The data-generating mechanisms are in principle adopted from the simulation study by @ditzhaus2023, including five factors being varied:
The event time distributions, the censoring time distributions, the base allocation of the sample size, the sample multiplier as well as the effect size, i.e. the true RMST difference.
However, we do not evaluate all of the scenarios constructed by @ditzhaus2023 but instead concentrate only on a subset of them that we consider to be representative in the sense of covering a range of possibly heterogeneous scenarios.
For the effect sizes $\Delta = \mu_1(t^*) - \mu_0(t^*)$ @ditzhaus2023 considered four values in total.
Here, we only look at $\Delta \in \{0, 1.5\}$, reflecting scenarios under the null and under the alternative hypothesis, respectively.
In addition, we only investigate three of the nine pairs of event time distributions depicted by @ditzhaus2023:
\begin{itemize}
  \item[S1] Exponential distributions and proportional hazard alternatives: $T_0 \sim \dexp(0.2)$ and $T_1 \sim \dexp(\theta_{\Delta})$
  \item[S7] Exponential vs piecewise Exponential: $T_0 \sim \dexp(0.2)$ and $T_1$ with piecewise constant hazard function $\lambda_1(t) = 0.5 \cdot \mathbf{1}\{t \leq \theta\} + 0.05 \cdot \mathbf{1}\{t > \theta\}$
  \item[S8] Weibull distributions with crossing curves and shape alternatives: $T_0 \sim \dweib(3, 1/8)$ and $T_1 \sim \dweib(\theta, 1/14)$
\end{itemize}
Here, $\theta$ denotes some generic parameter that gets calibrated according to the given effect size $\Delta$ and the restriction time $t^*$, the latter being fixed to $t^* = 10$.
For S1 this corresponds to the rate parameter of the exponential distribution for the treatment group, whereas for S7 this is the time point at which the hazard of the treatment group changes.
On the other hand, for S8 $\theta$ is the scale parameter of the Weibull distribution for the treatment group.[^1]
For the censoring distributions, in contrast, we keep using all levels:
\begin{itemize}
  \item[C1] unequally Weibull distributed censoring (Weib, uneq): $C_0 \sim \dweib(3, 1/18)$ and $C_1 \dweib(0.5, 1/40)$
  \item[C2] equally uniformly distributed censoring (Unif, eq): $C_0 \sim \dunif(0, 25)$ and $C_1 \sim \dunif(0, 25)$
  \item[C3] equally Weibull distributed censoring (Weib, eq): $C_0 \sim \dweib(3, 1/15)$ and $C_1 \sim \dweib(3, 1/15)$
\end{itemize}
A visual representation of the pairs of both, the survival and censoring distributions, is given in @fig-models.

```{r}
#| label: fig-models
#| fig-cap: "Simulation models for the event (top) and censoring (bottom) times"
#| fig-width: 10
#| fig-height: 8

wrap_plots(
  plot_surv_models(linewidth = 1.1, align = "h"),
  plot_cens_models(linewidth = 1.1, align = "h"),
  ncol = 1
)
```

Moreover, we utilize the same assumptions about the sample sizes and their allocations as @ditzhaus2023.
In particular, we consider three base settings of how the samples are allocated between the groups, i.e. $(n_0, n_1) \in \{(12, 18); (15, 15); (18, 12)\}$.
To obtain different total sample sizes these base allocations get multiplied with the integer $K$.
In comparison to @ditzhaus2023, who considered $K = 1, 2, 4$, we add $K = 6$ to this list in order to incorporate scenarios for which we believe the asymptotic theory to take effect.
The combination of all these parameters, summarized in @tbl-simdesign, leads to a total of 216 scenarios that are being evaluated.

```{r}
#| label: tbl-simdesign
#| tbl-cap: "Factors and levels for the data-generating mechanisms"

dt = data.table(
  Factor = c(
    rep("Survival models", 3),
    rep("Censoring models", 3),
    r"(RMST difference ($\mu_1(t^*) - \mu_0(t^*)$))",
    "Sample allocations",
    r"(Sample multipliers ($K$))"
  ),
  Levels = c(
    "S1: Exponential distributions",
    "S7: Exponential and piecewise exponential distributions with crossing curves",
    "S8: Weibull distributions with crossing curves and shape alternatives",
    "C1: unequal Weibull",
    "C2: equal uniform",
    "C3: equal Weibull",
    "0; 1.5",
    "(12, 18); (15, 15); (18, 12)",
    "1; 2; 4; 6"
  )
)

kbl(
  dt,
  format = "latex",
  booktabs = TRUE,
  centering = TRUE,
  escape = FALSE,
  linesep = "\\addlinespace"
) |>
  collapse_rows(
    columns = 1,
    latex_hline = "major"
  ) |>
  column_spec(1, width = "4.5cm") |>
  kable_styling(
    latex_options = c("scale_down")
  )
```

For their simulation study evaluating the unstudentized permutation test @horiguchi2020a regenerated the data whenever the RMST was inestimable for at least one of the two groups and @ditzhaus2023 adopted this procedure for their study.
Accordingly, we follow the same procedure.

*Estimands and other targets*\newline
Although all of the methods involve the point estimation of a particular parameter, namely the two-sample RMST difference $\muhat_1(t^*) - \muhat_0^{t^*}$, they are all based on the Kaplan-Meier estimator of the survival function.
Because of this, the point estimates of the different methods will be either exactly or nearly identical.
The main distinction is how the statistical test of the null hypothesis in (@eq-test-problem) is conducted.
Therefore, that null hypothesis is the target of the simulation study.

*Methods*\newline
All methods that will be assessed in this study have been presented and described in detail in @sec-inference.
Of primary interest are the two methods based on using pseudo-observations, i.e. one employing an asymptotic test and the other one using a bootstrap test.
Since for the latter we use IJ pseudo-observations, we also include an asymptotic test based on these IJ pseudo-observations such that we are better able to gain an understanding of how they would potentially affect the performance of the bootstrap test.
In this simulation study, the standard asymptotic test has the role of a reference method that is employed in practice most often.
On the other hand, we consider the studentized permutation method by @ditzhaus2023 as the current gold standard for conducting a two-sample RMST test.
Other methods that we could have included are the unstudentized permutation test by @horiguchi2020a and the approach based on empirical likelihood ratios by @zhou2021.
As, in our opinion, these two methods cannot be assigned to any of the above categories, we do not cover them in our simulation study.

*Performance measures*\newline
Given the null hypothesis in (@eq-test-problem) as the target of the simulation study and the original problem that the asymptotic test (@eq-test-asy) has an inflated type I error rate, the type I error rate is also the primary performance measure.
Corresponding secondary performance measures are the power and the coverage probability of the respective test and its associated confidence interval.


## Computational Details {#sec-sim-comp}

For each of the scenarios described in @sec-sim-design we generated $N_{\text{sim}} = 5000$ data sets as it has been done by @ditzhaus2023.
Similarly, we used $N_{\text{res}} = 2000$ resampling iterations, for both, the studentized permutation and the pseudo-observations bootstrap test. 
The nominal significance level $\alpha$ was set to $5\%$

All computations were carried out using the `R` programming language in version 4.3.0 [@R-base] on the high performance computing cluster of the GWDG in Goettingen.
The asymptotic test as well as the studentized permutation test were implemented by ourselves based on code supplied by Marc Ditzhaus.
For the approaches using pseudo-observations we used the `rmeanglm()` function from the `{eventglm}` package [@sachs2022], though the functions for calculating the pseudo-observations were programmed by ourselves as well.
For the implementation of the IJ pseudo-observations we used the `pseudo()` function from the `{survival}` package [@R-survival].
All package dependencies, as the ones depicted above, were tracked and managed using the `{renv}` package [@R-renv], contributing to the reproducibility of the results.

Another consideration for reproducibility that needs to be made is how to generate random numbers.
Here, we generated all random data sequentially using the Mersenne-Twister algorithm, therefore avoiding the need to worry about correlated data sets or using RNG streams [@morris2019a].
Moreover, we used the `with_seed()` function from the `{withr}` package [@R-withr] for handling the state of the RNG state within each simulation scenario.

On the other hand, the application of the different statistical methods on the simulated data sets was done in parallel using the `{parallel}`, which gets shipped with base R [@R-base].
While we kept using the `with_seed()` function, we now used the L'Ecuyer-CMRG algorithm for generating pseudo random numbers.
For the standard asymptotic test and those based on pseudo-observations this has no implications.
Conversely, the studentized permutation method and the boostrap test using pseudo-observations both rely on stochastic resampling, which is why a proper handling of (parallel) random number generation is required.


## Results {#sec-sim-results}

For the type-I error rates the detailed results are given in @tbl-type1err whereas @fig-type1err provides an aggregated view of the results allowing to grasp the general patterns faster.
Note that, due to limited space, the asymptotic test based on IJ pseudo-observations is only represented in the plot but not in the table since its inclusion shall rather provide additional contextual information only.
In both, the table and the figure, we have depicted the 95% binomial confidence interval $[4.4\%, 5.6\%]$ around the nominal level $\alpha = 5\%$ derived from the fact that we have run  $N_{\text{sim}} = 5000$ simulations for each scenario.
Using this information, the first thing we can observe is that, in line with the findings by @horiguchi2020a and @ditzhaus2023, the standard asymptotic test indeed leads to too liberal test decisions.
This is particularly pronounced for small samples sizes with $K = 1, 2$ but is even apparent for moderate to large sample sizes ($K = 4, 6$).
Regarding the studentized permutation test by @ditzhaus2023, overall we arrive at similar conclusions as the authors of that test.
In fact, among all tests included in our simulation study, it performed best with respect to controlling the type-I error rate, namely in 93 out of 108 scenarios.
The results suggest that, in particular, situations with very small *and* unbalanced sample sizes pose difficulties to the studentized permutation test.
Another qualitative feature of this test, illustrated by @fig-type1err, is that there seems to be no systematic trend that the test is either too conservative or too liberal in such scenarios.
Moving to the methods based on pseudo-observations, their performance is comparable to though not quite as good as that of the studentized permutation test.
Speaking of the asymptotic test based on pseudo-observations, in total 88 of the 108 null scenarios could be handled in terms of achieving a type-I error rate close enough to the nominal level of $5\%$.
Across all scenarios this performance could be slightly improved by considering the method that uses a bootstrap hypothesis test instead of relying on a normal approximation.
Here, the type-I error rate could be controlled in 90 scenarios.
From @fig-type1err we can conclude that while the ordinary pseudo-observations method has a slight tendency to make too liberal test decisions, especially for very small sample sizes, the opposite is true for the bootstrap method, leaning towards a too conservative behaviour.
These observations particularly apply in situations with very small sample sizes ($K = 1$) but are less pronounced for moderate sample sizes ($K = 2$) already.
Moreover, we can conclude that, if possible, using the ordinary pseudo-observations should be preferred over the IJ pseudo-observations.
Nonetheless, if the usage of ordinary pseudo-observations poses computational challenges, e.g. due to large sample sizes or the application of resampling methods, it can still be recommended.
In the former case the resulting differences even seem to be negligible.

```{r}
#| label: tbl-type1err
#| tbl-cap: "Type-I error rates in % (nominal level $\\alpha = 5\\%$)"

# Calculate rejection rates
# For sake of illustration:
# - ignore pseudo_ij (no bootstrap)
# - ignore k = 6
dt_err = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 0, scenario.id] &
        algo.id != 4],
  stats_NA = FALSE
)
dt_err = merge(dt_err, dts[rmst_diff == 0], by = "scenario.id")

# For latex, directly starting off with a table in wide format might be easier
dt_err = dcast(dt_err, scenario.id ~ algo.id, value.var = "reject")
setnames(dt_err, old = as.character(c(1:3, 5)), new = paste0("algo", c(1:3, 5)))

# Append scenario information...
dt_err = merge(dt_err, dts, by = "scenario.id")
setj_percent(dt_err, paste0("algo", c(1:3, 5)))
for (j in paste0("algo", c(1:3, 5))) {
  set(dt_err, j = j, value = round(dt_err[[j]], 1))
}
dt_err[, samples_alloc := factor(
  sprintf("n_%d_%d", n0 / samples_k, n1 / samples_k),
  levels = sprintf("n_%d_%d", c(12, 15, 18), c(18, 15, 12))
)]
# ... but some columns are or have become redundant
for (j in c("scenario.id", "rmst_diff", "n0", "n1")) {
  set(dt_err, j = j, value = NULL)
}

dt_err = dcast(
  dt_err,
  surv_model + cens_model + samples_k ~ samples_alloc, value.var = paste0("algo", c(1:3, 5))
)

# Order columns and rows
setcolorder(
  dt_err, neworder = c(
    "surv_model", "cens_model", "samples_k",
    sprintf("algo%d_n_12_18", c(1:3, 5)),
    sprintf("algo%d_n_15_15", c(1:3, 5)),
    sprintf("algo%d_n_18_12", c(1:3, 5))
  )
)
dt_err[, `:=`(
  surv_model = factor(surv_model, levels = c("ph_exp", "crossing_pwexp", "crossing_wb")),
  cens_model = factor(cens_model, levels = c("uneq_wb", "eq_unif", "eq_wb"))
)]
setorder(dt_err, surv_model, cens_model, samples_k)

# Make some cells of column `cens_model` empty
na_idx = setdiff(1:nrow(dt_err), seq(1, nrow(dt_err), by = 4))
dt_err[na_idx, cens_model := NA]

# Also need to relabel this
dt_err[, cens_model := as.character(cens_model)]
dt_err[, cens_model := fcase(
  cens_model == "uneq_wb", "un. W.",
  cens_model == "eq_unif", "eq. U.",
  cens_model == "eq_wb", "eq. W."
)]


# NAs as empty cells
options(knitr.kable.NA = '')

# Conditional formatting
for (j in colnames(dt_err)[-(1:3)]) {
  set(
    dt_err, j = j, value = cell_spec(
      format(dt_err[[j]], digits = 1, nsmall = 1), bold = between(dt_err[[j]], 4.4, 5.6)
    )
  )
}

# latex table
kbl(
  copy(dt_err)[, surv_model := NULL],
  format = "latex",
  digits = 1,
  booktabs = TRUE,
  centering = TRUE,
  escape = FALSE,
  align = c("l", rep("c", 13)),
  col.names = c("Censoring", "K", rep(c("Asy", "Perm", "PO1", "PO2"), 3)),
  linesep = c(rep("", 11), "\\hline")
) |>
  # Group the columns by their sample allocation
  add_header_above(
    setNames(
      c(1, 1, 4, 4, 4),
      c(" ", " ", sprintf(r"($N = K \\cdot (%d, %d)$)", c(12, 15, 18), c(18, 15, 12)))
    ),
    escape = FALSE, bold = TRUE
  ) |>
  # Borders between the groups make it more accessible
  column_spec(2, border_right = TRUE) |>
  column_spec(6, border_right = TRUE) |>
  column_spec(10, border_right = TRUE) |>
  # Group table (rows) by survival distributions
  pack_rows(
    "S1: Exponential distributions", 1, 12,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
  pack_rows(
    "S7: Exponential and piecewise exponential distributions with crossing curves", 13, 24,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
    pack_rows(
      "S8: Weibull distributions with crossing curves and shape alternatives", 25, 36,
      bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
    ) |>
  # Make it more accessible using stripes option
  kable_styling(
    latex_options = c("striped", "scale_down"),
    stripe_index = c(5:8, 17:20, 29:32),
    stripe_color = "#e9ecef"
  ) |>
  # Explain binomial confidence interval and abbreviations
  footnote(
    general = c(
      paste0(
        r"(\\textit{Note:} )",
        "The values inside the binomial confidence interval [4.4\\\\%, 5.6\\\\%] are printed bold."
      ),
      paste0(
        r"(\\textit{Abbreviations:} )",
        "Asy, asymptotic test; ", "Perm, studentized permutation test; ",
        "PO1, pseudo-observations; ",
        "PO2, pseudo-observations + bootstrap test; ",
        "un. W., unequal Weibull censoring; ",
        "eq. U., equal uniform censoring; ",
        "eq. W., equal Weibull censoring."
      )
    ),
    general_title = "",
    escape = FALSE,
    threeparttable = TRUE
  )
```

```{r}
#| label: fig-type1err
#| fig-cap: "Type-I error rates in % (nominal level $\\alpha = 5\\%$)"
#| fig-width: 10
#| fig-height: 6

dt1 = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 0, scenario.id]],
  stats_NA = FALSE
)
dt1 = merge(
  dt1, dts[rmst_diff == 0, .(scenario.id, num_samples = n0 + n1)],
  by = "scenario.id"
)
setj_percent(dt1, "reject")

ggplot(dt1, aes(factor(algo.id), reject)) +
  geom_boxplot() +
  theme_bw() +
  # Binomial confidence interval
  geom_hline(yintercept = 4.4, linetype = "dashed") +
  geom_hline(yintercept = 5.6, linetype = "dashed") +
  # Labels etc.
  scale_y_continuous(
    name = "Type I error in %\n"
  ) +
  scale_x_discrete(
    name = "\nMethods",
    labels = c("Asy", "Stud. Perm.", "PO", "PO IJ", "PO IJ Boot")
  ) +
  # Differentiate between total sample sizes
  facet_wrap(
    ~ num_samples,
    labeller = labeller(num_samples = \(x) paste0("N = ", x))
  )
```

In a similar fashion, the power values for the different methods and scenarios are displayed in @tbl-power and @fig-power, respectively.
Here, we can see that the conservative behaviour of the bootstrap method, especially in settings with very small sample sizes ($K = 1$) is also reflected in a lower power, e.g. compared to the ordinary pseudo-observations approach.
The studentized permutation method can be seen as a compromise between the two pseudo-observations methods in this regard, having a power that is usually between those of the two pseudo-observations methods.
Universally, the standard asymptotic test has the highest power among all tests, the differences being most prominent in settings with small sample sizes.
This finding should, however, be interpreted with care as the increase in power comes at the cost of an inflated type-I error rate.
Additionally, while the power differences are still present in settings with larger sample sizes ($K = 4, 6$) they can be considered less relevant.

```{r}
#| label: tbl-power
#| tbl-cap: "Rejection rates (power) in % (nominal level $\\alpha = 5\\%$)"

# Calculate rejection rates (power)
dt_pow = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 1.5, scenario.id] & algo.id != 4],
  stats_NA = FALSE
)
dt_pow = merge(dt_pow, dts[rmst_diff == 1.5], by = "scenario.id")


# For latex, directly starting off with a table in wide format might be easier
dt_pow = dcast(dt_pow, scenario.id ~ algo.id, value.var = "reject")
setnames(dt_pow, old = as.character(c(1:3, 5)), new = paste0("algo", c(1:3, 5)))

# Append scenario information...
dt_pow = merge(dt_pow, dts, by = "scenario.id")
setj_percent(dt_pow, paste0("algo", c(1:3, 5)))
for (j in paste0("algo", c(1:3, 5))) {
  set(dt_pow, j = j, value = round(dt_pow[[j]], 1))
}
dt_pow[, samples_alloc := factor(
  sprintf("n_%d_%d", n0 / samples_k, n1 / samples_k),
  levels = sprintf("n_%d_%d", c(12, 15, 18), c(18, 15, 12))
)]
# ... but some columns are or have become redundant
for (j in c("scenario.id", "rmst_diff", "n0", "n1")) {
  set(dt_pow, j = j, value = NULL)
}

dt_pow = dcast(
  dt_pow,
  surv_model + cens_model + samples_k ~ samples_alloc, value.var = paste0("algo", c(1:3, 5))
)

# Order columns and rows
setcolorder(
  dt_pow, neworder = c(
    "surv_model", "cens_model", "samples_k",
    sprintf("algo%d_n_12_18", c(1:3, 5)),
    sprintf("algo%d_n_15_15", c(1:3, 5)),
    sprintf("algo%d_n_18_12", c(1:3, 5))
  )
)
dt_pow[, `:=`(
  surv_model = factor(surv_model, levels = c("ph_exp", "crossing_pwexp", "crossing_wb")),
  cens_model = factor(cens_model, levels = c("uneq_wb", "eq_unif", "eq_wb"))
)]
setorder(dt_pow, surv_model, cens_model, samples_k)

# Make some cells of column `cens_model` empty
na_idx = setdiff(1:nrow(dt_pow), seq(1, nrow(dt_pow), by = 4))
dt_pow[na_idx, cens_model := NA]

# Also need to relabel this
dt_pow[, cens_model := as.character(cens_model)]
dt_pow[, cens_model := fcase(
  cens_model == "uneq_wb", "un. W.",
  cens_model == "eq_unif", "eq. U.",
  cens_model == "eq_wb", "eq. W."
)]


# NAs as empty cells
options(knitr.kable.NA = '')

# latex table
kbl(
  copy(dt_pow)[, surv_model := NULL],
  format = "latex",
  digits = 1,
  booktabs = TRUE,
  centering = TRUE,
  escape = FALSE,
  align = c("l", rep("c", 13)),
  col.names = c("Censoring", "K", rep(c("Asy", "Perm", "PO1", "PO2"), 3)),
  linesep = c(rep("", 11), "\\hline")
) |>
  # Group the columns by their sample allocation
  add_header_above(
    setNames(
      c(1, 1, 4, 4, 4),
      c(" ", " ", sprintf(r"($N = K \\cdot (%d, %d)$)", c(12, 15, 18), c(18, 15, 12)))
    ),
    escape = FALSE, bold = TRUE
  ) |>
  # Borders between the groups make it more accessible
  column_spec(2, border_right = TRUE) |>
  column_spec(6, border_right = TRUE) |>
  column_spec(10, border_right = TRUE) |>
  # Group table (rows) by survival distributions
  pack_rows(
    "S1: Exponential distributions", 1, 12,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
  pack_rows(
    "S7: Exponential and piecewise exponential distributions with crossing curves", 13, 24,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
    pack_rows(
      "S8: Weibull distributions with crossing curves and shape alternatives", 25, 36,
      bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
    ) |>
  # Make it more accessible using stripes option
  kable_styling(
    latex_options = c("striped", "scale_down"),
    stripe_index = c(5:8, 17:20, 29:32),
    stripe_color = "#e9ecef"
  ) |>
  # Explain binomial confidence interval and abbreviations
  footnote(
    general = c(
      paste0(
        r"(\\textit{Abbreviations:} )",
        "Asy, asymptotic test; ", "Perm, studentized permutation test; ",
        "PO1, pseudo-observations; ",
        "PO2, pseudo-observations + bootstrap test; ",
        "un. W., unequal Weibull censoring; ",
        "eq. U., equal uniform censoring; ",
        "eq. W., equal Weibull censoring."
      )
    ),
    general_title = "",
    escape = FALSE,
    threeparttable = TRUE
  )
```

```{r}
#| label: fig-power
#| fig-cap: "Power values in % ($\\alpha = 5\\%$)"
#| fig-width: 10
#| fig-height: 6

dt2 = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 1.5, scenario.id]],
  stats_NA = FALSE
)
dt2 = merge(
  dt2, dts[rmst_diff == 1.5, .(scenario.id, num_samples = n0 + n1)],
  by = "scenario.id"
)
setj_percent(dt2, "reject")

ggplot(dt2, aes(factor(algo.id), reject)) +
  geom_boxplot() +
  theme_bw() +
  scale_y_continuous(
    name = "Power in %\n"
  ) +
  scale_x_discrete(
    name = "\nMethods",
    labels = c("Asy", "Stud. Perm.", "PO", "PO IJ", "PO IJ Boot")
  ) +
  facet_wrap(
    ~ num_samples,
    labeller = labeller(num_samples = \(x) paste0("N = ", x)),
    scales = "free_y"
  )
```

Finally, @fig-coverage shows the coverage rates of the confidence intervals associated with each method.
Overall, we can derive similar conclusions from this figure as above.
Yet, @fig-coverage nicely highlights that, independent of the chosen method, a study design is deribale in which the allocation of the total sample size to the two groups should be either balanced or slightly concentrated towards the group, for which a treatment effect is expected.

```{r}
#| label: fig-coverage
#| fig-cap: "Coverage in % (nominal level $\\alpha = 5\\%$)"
#| fig-width: 10
#| fig-height: 9

dt4 = calc_ci_metrics(
  merge(dtr, dts[, .(scenario.id, rmst_diff)], by = "scenario.id"),
  stats_NA = FALSE
)
dt4 = merge(dt4, dts[, .(scenario.id, samples_k, n0, n1)], by = "scenario.id")
setj_percent(dt4, "coverage")
setj_samples_alloc(dt4)

ggplot(dt4, aes(factor(algo.id), coverage)) +
  geom_boxplot() +
  facet_grid(
    rows = vars(samples_k), cols = vars(samples_alloc),
    labeller = labeller(samples_k = \(x) sprintf("K = %s", x))
  ) +
  theme_bw() +
  # Binomial confidence interval
  geom_hline(yintercept = 94.4, linetype = "dashed") +
  geom_hline(yintercept = 95.6, linetype = "dashed") +
  # y-axis
  scale_y_continuous(
    name = "Coverage in %\n"
  ) +
  # x-axis
  scale_x_discrete(
    name = "\nMethods",
    labels = c("Asy", "Stud. Perm.", "PO", "PO IJ", "PO IJ Boot")
  )
```

[^1]: In comparison to the parametrization of the Weibull distribution in @eq-wb-hazard, @ditzhaus2023 use a different parametrization where the scale parameter is $\sigma = \lambda^{-1}$.
For the sake of consistency we keep employing the parametrization as it is used in @eq-wb-hazard.
Nonetheless, the distributional assumptions are equivalent.
