---
bibliography: ["../bib/references.bib", "../bib/packages.bib"]
---

# Simulation Study {#sec-simulation}

With the intention of evaluating and comparing the empirical performance of the presented methods, in particular in settings with small to moderate sample sizes, we now set up a simulation study pursuing to cover a satisfactory range of different scenarios.
In order to embed the design of this study into a structured framework we make use of the ADEMP methodology introduced by @morris2019a.
ADEMP is an acronym translating to *Aims*, *Data-generating mechanisms*, *Estimands* (and other targets), *Methods* and *Performance measures*.
The concrete configuration of these aspects is described in the first subsection.
Thereafter, we briefly comment computational aspects impacting the results and their reproducibility.
Lastly, we present the results of the study and discuss our findings.


## Design {#sec-sim-design}

*Aims*\newline
The aim of this simulation study is to investigate the performance of different methods for estimating and testing the RMST difference between two samples, e.g. a control and an experimental treatment group in a clinical trial.
In particular, the focus is on settings with small to moderate sample sizes as all methods under investigation either directly or indirectly rely on large sample approximations based on asymptotic theory.

*Data-generating mechanisms*\newline
The data-generating mechanisms are in principle adopted from the simulation study by @ditzhaus2023, including five factors being varied:
The event time distributions, the censoring time distributions, the base allocation of the sample size, the sample multiplier as well as the effect size, i.e. the true RMST difference.
However, we do not evaluate all of the scenarios constructed by @ditzhaus2023 but instead concentrate only on a subset of them that we consider to be representative in the sense of covering a range of possibly heterogeneous scenarios, especially with respect to the event time distributions of the two populations.
For the effect sizes $\Delta = \mu_1(t^*) - \mu_0(t^*)$ @ditzhaus2023 considered four values in total.
Here, we only look at $\Delta \in \{0, 1.5\}$, reflecting scenarios under the null and under the alternative hypothesis, respectively.
In addition, we investigate the following three of the nine pairs of event time distributions considered by @ditzhaus2023:
\begin{itemize}
  \item[S1] Exponential distributions and proportional hazard alternatives: $T_0 \sim \dexp(0.2)$ and $T_1 \sim \dexp(\theta_{\Delta})$
  \item[S7] Exponential vs piecewise Exponential: $T_0 \sim \dexp(0.2)$ and $T_1$ with piecewise constant hazard function $\lambda_1(t) = 0.5 \cdot \mathbf{1}\{t \leq \theta_{\Delta}\} + 0.05 \cdot \mathbf{1}\{t > \theta_{\Delta}\}$
  \item[S8] Weibull distributions with crossing curves and shape alternatives: $T_0 \sim \dweib(3, 1/8)$ and $T_1 \sim \dweib(\theta_{\Delta}, 1/14)$
\end{itemize}
Here, $\theta_{\Delta}$ denotes a generic parameter whose meaning depends on the respective context.
It then gets calibrated according to the given effect size $\Delta$ and the restriction time $t^*$, the latter being fixed to $t^* = 10$.
For S1 this corresponds to the rate parameter of the exponential distribution for the treatment group, whereas for S7 this is the time point at which the hazard of the treatment group changes.
On the other hand, for S8 $\theta_{\Delta}$ is the scale parameter of the Weibull distribution for the treatment group.[^2]
For the censoring distributions, in contrast, we keep using all levels examined by @ditzhaus2023:
\begin{itemize}
  \item[C1] unequally Weibull distributed censoring (Weib, uneq): $C_0 \sim \dweib(3, 1/18)$ and $C_1 \dweib(0.5, 1/40)$
  \item[C2] equally uniformly distributed censoring (Unif, eq): $C_0 \sim \dunif(0, 25)$ and $C_1 \sim \dunif(0, 25)$
  \item[C3] equally Weibull distributed censoring (Weib, eq): $C_0 \sim \dweib(3, 1/15)$ and $C_1 \sim \dweib(3, 1/15)$
\end{itemize}
A visual representation of the pairs of both, the survival and the censoring distributions, is given in @fig-models.

```{r}
#| label: fig-models
#| fig-cap: "Simulation models for the event (top) and censoring (bottom) times"
#| fig-width: 10
#| fig-height: 8

wrap_plots(
  plot_surv_models(linewidth = 1.1, align = "h"),
  plot_cens_models(linewidth = 1.1, align = "h"),
  ncol = 1
)
```

Moreover, we utilize the same assumptions about the sample sizes and their allocations as @ditzhaus2023.
In particular, we consider three base settings of how the samples are allocated between the two groups, i.e. $(n_0, n_1) \in \{(12, 18); (15, 15); (18, 12)\}$.
To obtain different total sample sizes these base allocations are multiplied with the integer $K$.
In comparison to @ditzhaus2023, who considered $K \in \{1, 2, 4\}$, we add $K = 6$ to this set in order to incorporate scenarios for which we expect the asymptotic theory to take effect.
The combination of all these factors, summarized in @tbl-simdesign, leads to a total of 216 scenarios that are being evaluated, i.e. 108 scenarios for assessing the type-I error control of the different methods ($\Delta = 0$) and 108 scenarios for analyzing the power of the tests ($\Delta = 1.5$).

```{r}
#| label: tbl-simdesign
#| tbl-cap: "Factors and levels for the data-generating mechanisms"

dt = data.table(
  Factor = c(
    rep("Survival models", 3),
    rep("Censoring models", 3),
    r"(RMST difference ($\mu_1(t^*) - \mu_0(t^*)$))",
    "Sample allocations",
    r"(Sample multipliers ($K$))"
  ),
  Levels = c(
    "S1: Exponential distributions",
    "S7: Exponential and piecewise exponential distributions with crossing curves",
    "S8: Weibull distributions with crossing curves and shape alternatives",
    "C1: unequal Weibull",
    "C2: equal uniform",
    "C3: equal Weibull",
    "0; 1.5",
    "(12, 18); (15, 15); (18, 12)",
    "1; 2; 4; 6"
  )
)

kbl(
  dt,
  format = "latex",
  booktabs = TRUE,
  centering = TRUE,
  escape = FALSE,
  linesep = "\\addlinespace"
) |>
  collapse_rows(
    columns = 1,
    latex_hline = "major"
  ) |>
  column_spec(1, width = "4.5cm") |>
  kable_styling(
    latex_options = "scale_down"
  )
```

For their simulation study evaluating the unstudentized permutation test @horiguchi2020a regenerated the simulated data whenever the RMST was inestimable for at least one of the two groups using a Kaplan-Meier-based estimator of the RMST.
As described in @sec-studperm such a situation occurs when the largest event time is smaller than $t^*$ and is censored.
@ditzhaus2023 also adopted this procedure for their simulation study.
Accordingly, we employ this routine as well.

*Estimands and other targets*\newline
Although all of the methods involve the point estimation of a particular parameter, namely the two-sample RMST difference $\Delta = \muhat_1(t^*) - \muhat_0^(t^*)$, they are all based on the Kaplan-Meier estimator of the survival function.
Because of this, the point estimates of the different methods will be either exactly or nearly identical.
The main distinction is how the statistical test of the null hypothesis in (@eq-test-problem) is conducted.
Therefore, that null hypothesis is the target of the simulation study.

*Methods*\newline
All methods that will be assessed in this study have been presented and explained in detail in @sec-inference.
Of primary interest, however, are the two proposed methods based on pseudo-observations, i.e. the one employing an asymptotic test and the other one using a bootstrap test.
For the latter we use IJ pseudo-observations as described in @sec-po-boot.
In order to acquire additional information about how the usage of IJ instead of ordinary pseudo-observations might impact the performance of the bootstrap test, we also include an asymptotic test using these IJ pseudo-observations.
However, corresponding results are devoted to the appendix (TODO: reference appendix.)

In this simulation study, the standard asymptotic test (@eq-test-asy) has the role of the main reference method as it can be expected that among all methods presented in this thesis this is the one most commonly employed in practice, both by applied scientists [e.g. @manner2019] and methodological researchers [e.g. @dormuth2023].
On the other hand, based on the results from their simulation study, we consider the studentized permutation method by @ditzhaus2023 as the current gold standard for conducting two-sample RMST tests.
Other methods that were also investigated by @ditzhaus2023 and could have been included in our simulation study are the unstudentized permutation test by @horiguchi2020a and the approach based on empirical likelihood ratios by @zhou2021.
Based on the results by @ditzhaus2023, in our opinion, these two methods cannot be assigned to any of the two aforementioned categories and therefore we do not cover them in our own study.

*Performance measures*\newline
Given the null hypothesis in (@eq-test-problem) as the target of the simulation study and the original problem that the standard asymptotic test (@eq-test-asy) has an inflated type-I error rate, the type-I error rate is also the primary performance measure.
Corresponding secondary performance measures are the power and the coverage probability of the respective test and its associated confidence interval.


## Computational Details {#sec-sim-comp}

For each of the scenarios described in @sec-sim-design we generated $N_{\text{sim}} = 5000$ data sets as it has been done by @ditzhaus2023.
Similarly, we used $N_{\text{res}} = 2000$ resampling iterations, for both, the studentized permutation and the pseudo-observations bootstrap test. 
The nominal significance level $\alpha$ was set to $5\%$

All computations were carried out using the `R` programming language in version 4.3.0 [@R-base] on the high performance computing cluster of the GWDG in Goettingen.
The asymptotic test as well as the studentized permutation test were implemented by ourselves based on code supplied by Marc Ditzhaus.

For the approaches based on pseudo-observations we used the `rmeanglm()` function from the `{eventglm}` package [@sachs2022].
This function is essentially a wrapper around the `stats::glm()` function adapted to regression models for pseudo-observations.
Using the arguments `model.censoring` and `formula.censoring` of that function it is possible to customize how the pseudo-observations are computed.
While the latter is used to specify which variables should be controlled for when calculating the pseudo-observations, to the former we can pass custom "modules" (R functions) that carry out the actual calculation of the pseudo-observations before estimating the regression model.
Although the `{eventglm}` already provides a range of such modules, we programmed these functions by ourselves as well.
For the IJ pseudo-observations module we used the `pseudo()` function from the `{survival}` package [@R-survival].
Further direct and indirect package dependencies were required for running the simulation study, all of which were tracked and managed using the `{renv}` package [@R-renv], contributing to the reproducibility of the results.

Another important consideration regarding the validity and reproducibility of the results is how we generated pseudo-random numbers.
In our simulation study, we generated all random data sequentially using the Mersenne-Twister algorithm, therefore avoiding the need to worry about correlated data sets or using parallel random number generator (RNG) streams [@morris2019a].
Moreover, we used the `with_seed()` function from the `{withr}` package [@R-withr] for handling the RNG state within each simulation scenario.
On the other hand, the application of the different statistical methods on the simulated data sets was done in parallel using the `{parallel}` package, which gets shipped with base R [@R-base].
While we kept using the `with_seed()` function, we now used the L'Ecuyer-CMRG algorithm and parallel RNG streams for generating pseudo-random numbers.
For the standard asymptotic test and those based on pseudo-observations this has no implications.
Conversely, the studentized permutation method and the boostrap test using pseudo-observations both rely on stochastic resampling, which is why a proper handling of (parallel) random number generation is required.


## Results {#sec-sim-results}

We present the results, both in form of tables and figures.
The former should provide all results in great detail whereas the latter shall serve as a way to grasp the overall patterns and conclusion faster.

### Type-I Error {#sec-sim-type1}

Regarding the type-I error rate @fig-type1err displays the results graphically aggregated by the total sample size across the two treatment groups.
Additionally, @tbl-type1err augments this with detailed numerical results.
In both, the table and the figure, we have depicted the 95% binomial confidence interval $[4.4\%, 5.6\%]$ around the nominal level $\alpha = 5\%$ derived from the fact that we have run  $N_{\text{sim}} = 5000$ simulations for each scenario.
Therefore, we can consider any of the statistical tests to have succeeded in terms of controlling the type-I error rate for a given scenario, whenever the empirical type-I error rate falls within that confidence interval.

```{r}
#| label: tbl-type1err
#| tbl-cap: "Type-I error rates in % (nominal level $\\alpha = 5\\%$)"

# Calculate rejection rates
# For sake of illustration:
# - ignore pseudo_ij (no bootstrap)
# - ignore k = 6
dt_err = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 0, scenario.id] &
        algo.id != 4],
  stats_NA = FALSE
)
dt_err = merge(dt_err, dts[rmst_diff == 0], by = "scenario.id")

# For latex, directly starting off with a table in wide format might be easier
dt_err = dcast(dt_err, scenario.id ~ algo.id, value.var = "reject")
setnames(dt_err, old = as.character(c(1:3, 5)), new = paste0("algo", c(1:3, 5)))

# Append scenario information...
dt_err = merge(dt_err, dts, by = "scenario.id")
setj_percent(dt_err, paste0("algo", c(1:3, 5)))
for (j in paste0("algo", c(1:3, 5))) {
  set(dt_err, j = j, value = round(dt_err[[j]], 1))
}
dt_err[, samples_alloc := factor(
  sprintf("n_%d_%d", n0 / samples_k, n1 / samples_k),
  levels = sprintf("n_%d_%d", c(12, 15, 18), c(18, 15, 12))
)]
# ... but some columns are or have become redundant
for (j in c("scenario.id", "rmst_diff", "n0", "n1")) {
  set(dt_err, j = j, value = NULL)
}

dt_err = dcast(
  dt_err,
  surv_model + cens_model + samples_k ~ samples_alloc, value.var = paste0("algo", c(1:3, 5))
)

# Order columns and rows
setcolorder(
  dt_err, neworder = c(
    "surv_model", "cens_model", "samples_k",
    sprintf("algo%d_n_12_18", c(1:3, 5)),
    sprintf("algo%d_n_15_15", c(1:3, 5)),
    sprintf("algo%d_n_18_12", c(1:3, 5))
  )
)
dt_err[, `:=`(
  surv_model = factor(surv_model, levels = c("ph_exp", "crossing_pwexp", "crossing_wb")),
  cens_model = factor(cens_model, levels = c("uneq_wb", "eq_unif", "eq_wb"))
)]
setorder(dt_err, surv_model, cens_model, samples_k)

# Make some cells of column `cens_model` empty
na_idx = setdiff(1:nrow(dt_err), seq(1, nrow(dt_err), by = 4))
dt_err[na_idx, cens_model := NA]

# Also need to relabel this
dt_err[, cens_model := as.character(cens_model)]
dt_err[, cens_model := fcase(
  cens_model == "uneq_wb", "un. W.",
  cens_model == "eq_unif", "eq. U.",
  cens_model == "eq_wb", "eq. W."
)]


# NAs as empty cells
options(knitr.kable.NA = '')

# Conditional formatting
for (j in colnames(dt_err)[-(1:3)]) {
  set(
    dt_err, j = j, value = cell_spec(
      format(dt_err[[j]], digits = 1, nsmall = 1), bold = between(dt_err[[j]], 4.4, 5.6)
    )
  )
}

# latex table
kbl(
  copy(dt_err)[, surv_model := NULL],
  format = "latex",
  digits = 1,
  booktabs = TRUE,
  centering = TRUE,
  escape = FALSE,
  align = c("l", rep("c", 13)),
  col.names = c("Censoring", "K", rep(c("Asy", "Perm", "PO1", "PO2"), 3)),
  linesep = c(rep("", 11), "\\hline")
) |>
  # Group the columns by their sample allocation
  add_header_above(
    setNames(
      c(1, 1, 4, 4, 4),
      c(" ", " ", sprintf(r"($N = K \\cdot (%d, %d)$)", c(12, 15, 18), c(18, 15, 12)))
    ),
    escape = FALSE, bold = TRUE
  ) |>
  # Borders between the groups make it more accessible
  column_spec(2, border_right = TRUE) |>
  column_spec(6, border_right = TRUE) |>
  column_spec(10, border_right = TRUE) |>
  # Group table (rows) by survival distributions
  pack_rows(
    "S1: Exponential distributions", 1, 12,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
  pack_rows(
    "S7: Exponential and piecewise exponential distributions with crossing curves", 13, 24,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
    pack_rows(
      "S8: Weibull distributions with crossing curves and shape alternatives", 25, 36,
      bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
    ) |>
  # Make it more accessible using stripes option
  kable_styling(
    latex_options = c("striped", "scale_down"),
    stripe_index = c(5:8, 17:20, 29:32),
    stripe_color = "#e9ecef"
  ) |>
  # Explain binomial confidence interval and abbreviations
  footnote(
    general = c(
      paste0(
        r"(\\textit{Note:} )",
        "The values inside the binomial confidence interval [4.4\\\\%, 5.6\\\\%] are printed bold."
      ),
      paste0(
        r"(\\textit{Abbreviations:} )",
        "Asy, asymptotic test; ", "Perm, studentized permutation test; ",
        "PO1, pseudo-observations asymptotic; ",
        "PO2, pseudo-observations bootstrap; ",
        "un. W., unequal Weibull censoring; ",
        "eq. U., equal uniform censoring; ",
        "eq. W., equal Weibull censoring."
      )
    ),
    general_title = "",
    escape = FALSE,
    threeparttable = TRUE
  )
```

```{r}
#| label: fig-type1err
#| fig-cap: "Type-I error rates in % (nominal level $\\alpha = 5\\%$)"
#| fig-width: 10
#| fig-height: 6

dt1 = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 0, scenario.id] & algo.id != 4],
  stats_NA = FALSE
)
dt1 = merge(
  dt1, dts[rmst_diff == 0, .(scenario.id, num_samples = n0 + n1)],
  by = "scenario.id"
)
setj_percent(dt1, "reject")

ggplot(dt1, aes(factor(algo.id), reject)) +
  geom_boxplot() +
  # Binomial confidence interval
  geom_hline(yintercept = 4.4, linetype = "dashed") +
  geom_hline(yintercept = 5.6, linetype = "dashed") +
  # Labels etc.
  scale_y_continuous(
    name = "Type I error in %\n"
  ) +
  scale_x_discrete(
    name = "\nMethods",
    labels = c("Asy", "St Perm", "PO Asy", "PO Boot")
  ) +
  # Differentiate between total sample sizes
  facet_wrap(
    ~ num_samples,
    labeller = labeller(num_samples = \(x) paste0("N = ", x))
  )
```

The first thing we can observe is that, in line with the findings by @horiguchi2020a and @ditzhaus2023, the standard asymptotic test indeed leads to too liberal test decisions, only controlling the type-I error in 31 out of 108 scenarios in total.
This is particularly pronounced for small sample sizes ($K \in \{1, 2\}$).
Noticeably, for $K = 1$ there is not a single instance for which the type-I error was controlled by the asymptotic test.
For $K = 2$ it controlled the type-I error in only 3 out of 27 scenarios.
The performance of the test improves for $K = 3$ but even for this subset the test fails in the majority of cases (10 out of 27 possible successes).
When dealing with a total of 180 samples ($K = 6$) the test enhances further but still performs evidently worse than its competitors.

Regarding the studentized permutation test our own findings largely agree with those by @ditzhaus2023.
Across all scenarios it even showed the best performance of all test, being able to control the type-I error in 90 out of 108 scenarios.
Relative to the other methods its performance particularly stands out for very small sample sizes ($K = 1$), where it controlled the type-I error in 16 out of 27 instances.
For larger sample sizes ($K \in \{2, 4, 6\}$), the performance was also decent.
There are only few scenarios, for which the empirical type-I error rate is not within the 95%-confidence interval around the nominal level $\alpha = 5\%$.

Moving to the proposed methods based on pseudo-observations, overall we can conclude that they provide further valid alternatives to the studentized permutation approach for testing RMST differences and should be preferred over the standard asymptotic test.
In summary, both tests controlled the type-I error in 88 scenarios, i.e. only two cases less than the studentized permutation test.
Their slight inferiority relative to the studentized permutation test mainly results from scenarios with very small sample sizes ($K = 1$), where the two methods controlled the type-I error only in 10 (bootstrap) and 13 (asymptotic) scenarios.
However, when the two tests fail in these situations they do for different reasons.
On the one hand, for the asymptotic approach we can observe a small tendency of making too liberal test decisions.
The bootstrap approach, on the other hand, seems to be too conservative with the empirical type-I error rate being as low as 3.3% .
While for larger sample sizes ($K = 2$) these tendencies can still be percieved, they become much less relevant already.


### Power {#sec-sim-power}

In a similar fashion, the power values for the different methods and scenarios are displayed in @tbl-power and @fig-power, respectively.

The first thing we can take away is that the standard asymptotic test has the highest power in all scenarios among the methods investigated.
However, having the discussion of the results in @sec-sim-type1 in mind, this comes at the cost of an inflated type-I error rate.
Therefore, it is hard to assess the higher power of the asymptotic test as an advantage over the other methods.

Other than that, the remaining methods can broadly be ranked in the following order (highest to lowest power): pseudo-observations asymptotic, studentized permutation, pseudo-observations bootstrap.
Hence, the conservativeness of the bootstrap method can also be recognized in terms of the power of the test.
This conservativeness seems to be more pronounced in scenarios with very small sample sizes ($K = 1$) and the sample allocation being unbalanced with the treatment arm consisting of less samples than the control arm ($(n_0, n_1) = (18, 12)$).
Despite this, even though the methods can be ranked quite consistently, the absolute differences in all other scenarios are rather subtle and of minor practical relevance.

```{r}
#| label: tbl-power
#| tbl-cap: "Rejection rates (power) in % (nominal level $\\alpha = 5\\%$)"

# Calculate rejection rates (power)
dt_pow = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 1.5, scenario.id] & algo.id != 4],
  stats_NA = FALSE
)
dt_pow = merge(dt_pow, dts[rmst_diff == 1.5], by = "scenario.id")


# For latex, directly starting off with a table in wide format might be easier
dt_pow = dcast(dt_pow, scenario.id ~ algo.id, value.var = "reject")
setnames(dt_pow, old = as.character(c(1:3, 5)), new = paste0("algo", c(1:3, 5)))

# Append scenario information...
dt_pow = merge(dt_pow, dts, by = "scenario.id")
setj_percent(dt_pow, paste0("algo", c(1:3, 5)))
for (j in paste0("algo", c(1:3, 5))) {
  set(dt_pow, j = j, value = round(dt_pow[[j]], 1))
}
dt_pow[, samples_alloc := factor(
  sprintf("n_%d_%d", n0 / samples_k, n1 / samples_k),
  levels = sprintf("n_%d_%d", c(12, 15, 18), c(18, 15, 12))
)]
# ... but some columns are or have become redundant
for (j in c("scenario.id", "rmst_diff", "n0", "n1")) {
  set(dt_pow, j = j, value = NULL)
}

dt_pow = dcast(
  dt_pow,
  surv_model + cens_model + samples_k ~ samples_alloc, value.var = paste0("algo", c(1:3, 5))
)

# Order columns and rows
setcolorder(
  dt_pow, neworder = c(
    "surv_model", "cens_model", "samples_k",
    sprintf("algo%d_n_12_18", c(1:3, 5)),
    sprintf("algo%d_n_15_15", c(1:3, 5)),
    sprintf("algo%d_n_18_12", c(1:3, 5))
  )
)
dt_pow[, `:=`(
  surv_model = factor(surv_model, levels = c("ph_exp", "crossing_pwexp", "crossing_wb")),
  cens_model = factor(cens_model, levels = c("uneq_wb", "eq_unif", "eq_wb"))
)]
setorder(dt_pow, surv_model, cens_model, samples_k)

# Make some cells of column `cens_model` empty
na_idx = setdiff(1:nrow(dt_pow), seq(1, nrow(dt_pow), by = 4))
dt_pow[na_idx, cens_model := NA]

# Also need to relabel this
dt_pow[, cens_model := as.character(cens_model)]
dt_pow[, cens_model := fcase(
  cens_model == "uneq_wb", "un. W.",
  cens_model == "eq_unif", "eq. U.",
  cens_model == "eq_wb", "eq. W."
)]


# NAs as empty cells
options(knitr.kable.NA = '')

# latex table
kbl(
  copy(dt_pow)[, surv_model := NULL],
  format = "latex",
  digits = 1,
  booktabs = TRUE,
  centering = TRUE,
  escape = FALSE,
  align = c("l", rep("c", 13)),
  col.names = c("Censoring", "K", rep(c("Asy", "Perm", "PO1", "PO2"), 3)),
  linesep = c(rep("", 11), "\\hline")
) |>
  # Group the columns by their sample allocation
  add_header_above(
    setNames(
      c(1, 1, 4, 4, 4),
      c(" ", " ", sprintf(r"($N = K \\cdot (%d, %d)$)", c(12, 15, 18), c(18, 15, 12)))
    ),
    escape = FALSE, bold = TRUE
  ) |>
  # Borders between the groups make it more accessible
  column_spec(2, border_right = TRUE) |>
  column_spec(6, border_right = TRUE) |>
  column_spec(10, border_right = TRUE) |>
  # Group table (rows) by survival distributions
  pack_rows(
    "S1: Exponential distributions", 1, 12,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
  pack_rows(
    "S7: Exponential and piecewise exponential distributions with crossing curves", 13, 24,
    bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
  ) |>
    pack_rows(
      "S8: Weibull distributions with crossing curves and shape alternatives", 25, 36,
      bold = FALSE, latex_gap_space = "0.5em", hline_after = TRUE
    ) |>
  # Make it more accessible using stripes option
  kable_styling(
    latex_options = c("striped", "scale_down"),
    stripe_index = c(5:8, 17:20, 29:32),
    stripe_color = "#e9ecef"
  ) |>
  # Explain binomial confidence interval and abbreviations
  footnote(
    general = c(
      paste0(
        r"(\\textit{Abbreviations:} )",
        "Asy, asymptotic test; ", "Perm, studentized permutation test; ",
        "PO1, pseudo-observations asymptotic; ",
        "PO2, pseudo-observations bootstrap; ",
        "un. W., unequal Weibull censoring; ",
        "eq. U., equal uniform censoring; ",
        "eq. W., equal Weibull censoring."
      )
    ),
    general_title = "",
    escape = FALSE,
    threeparttable = TRUE
  )
```

```{r}
#| label: fig-power
#| fig-cap: "Power values in % ($\\alpha = 5\\%$)"
#| fig-width: 10
#| fig-height: 6

dt2 = calc_rejection_rates(
  dtr[scenario.id %in% dts[rmst_diff == 1.5, scenario.id] & algo.id != 4],
  stats_NA = FALSE
)
dt2 = merge(
  dt2, dts[rmst_diff == 1.5, .(scenario.id, num_samples = n0 + n1)],
  by = "scenario.id"
)
setj_percent(dt2, "reject")

ggplot(dt2, aes(factor(algo.id), reject)) +
  geom_boxplot() +
  scale_y_continuous(
    name = "Power in %\n"
  ) +
  scale_x_discrete(
    name = "\nMethods",
    labels = c("Asy", "St Perm", "PO Asy", "PO Boot")
  ) +
  facet_wrap(
    ~ num_samples,
    labeller = labeller(num_samples = \(x) paste0("N = ", x)),
    scales = "free_y"
  )
```


### Coverage {#sec-sim-coverage}

Finally, we discuss the empirical coverage rates of the confidence intervals associated with each method.
As these results include both, null and alternative scenarios, this time we do not display the results numerically in a table but illustrate them in a single plot for a better overview (@fig-coverage).
Similarly as for the type-I error rate in @sec-sim-type1, we depict the 95% binomial confidence interval around the nominal coverage rate (95%) based on the 5000 simulation samples obtained.
Moreover, we not only distinguish between the total sample size but also between how the samples are allocated across the treatment groups.

In general, the results underline our previous findings for the type-I error rate (@sec-sim-type1) when it comes to comparing the different methods with each other.
Consequently, the standard asymptotic method again fails in a majority of scenarios as the empirical coverage lies within the 95% binomial confidence interval only in about 25% of all cases.
Again, the studentized permutation method achieves the best performance with 184 out of 216 instances, for which the empirical coverage rate is between 94.4% and 95.6%.
Likewise, the pseudo-observations methods are competitive with 174 (asymptotic) and 175 (bootstrap) simulations for which the empirical coverage is close enough to the targeted one.
The minor superiority of the studentized permutation method can again be contributed to the settings with very small sample sizes ($K = 1$), for which the pseudo-observations approaches cannot entirely keep up with the studentized permutation confidence interval.

Additional findings may be provided by this figure regarding the study design.
It seems that, regardless of the chosen method, it is harder to achieve the nominal coverage rate when there are less subjects in the treatment arm than in the control arm ($(n_0, n_1) = (18, 12)$).
As with many other findings from this study, this is especially the case for very small sample sizes ($K = 1$).
In contrast, it does not seem to make such a big difference whether the samples are balanced between the two groups or if the number of samples predominates in the treatment arm.
Therefore, we may conclude that for an actual study we should seek a sample allocation that is either balanced or is slightly concentrated towards the treatment group.

```{r}
#| label: fig-coverage
#| fig-cap: "Coverage in % (nominal level $\\alpha = 5\\%$)"
#| fig-width: 10
#| fig-height: 10

dt4 = calc_ci_metrics(
  merge(dtr[algo.id != 4], dts[, .(scenario.id, rmst_diff)], by = "scenario.id"),
  stats_NA = FALSE
)
dt4 = merge(dt4, dts[, .(scenario.id, samples_k, n0, n1)], by = "scenario.id")
setj_percent(dt4, "coverage")
setj_samples_alloc(dt4)

ggplot(dt4, aes(factor(algo.id), coverage)) +
  geom_boxplot() +
  facet_grid(
    rows = vars(samples_k), cols = vars(samples_alloc),
    labeller = labeller(samples_k = \(x) sprintf("K = %s", x))
  ) +
  # Binomial confidence interval
  geom_hline(yintercept = 94.4, linetype = "dashed") +
  geom_hline(yintercept = 95.6, linetype = "dashed") +
  # y-axis
  scale_y_continuous(
    name = "Coverage in %\n"
  ) +
  # x-axis
  scale_x_discrete(
    name = "\nMethods",
    labels = c("Asy", "St Perm", "PO Asy", "PO Boot")
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


[^2]: In comparison to the parametrization of the Weibull distribution in @eq-wb-hazard, @ditzhaus2023 use a different parametrization where the scale parameter is $\sigma = \lambda^{-1}$.
For the sake of consistency we keep employing the parametrization as it is used in @eq-wb-hazard.
Nonetheless, the distributional assumptions are equivalent.
